# -*- coding: utf-8 -*-
"""Spatial signature Vol 3 iTAL then Immune Metaanalysis V2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nde0K5MA8wkCBiwrR4323pMxK5l8mLct
"""

!pip --quiet install scanpy
!pip --quiet install leidenalg
#!pip --quiet install squidpy

from google.colab import drive
drive.mount('/content/drive')

import csv
import anndata as ad
import gzip
import os
import scipy.io
import scanpy as sc
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from google.colab import drive
import leidenalg as la
from pathlib import Path
#import squidpy as sq

adata=sc.read_h5ad('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/all_Cosmx_cleaned_neighbored.h5ad')

plasma_score=pd.read_csv('/content/plasma_proteome_score_niches.csv')

plasma_score

plasma_score.set_index('Unnamed: 0', inplace=True)

# Create a dictionary from the DataFrame
score_dict = plasma_score['Score'].to_dict()

# Map the scores back to the adata.obs
adata.obs['proteome_score'] = adata.obs.index.map(score_dict)

for sample in adata.obs['sample'].unique():
  print(sample)

plot_df = adata.obs[['proteome_score', 'cluster_labels_annotated_coarse_coarse']].dropna()

# Plot the boxplot using seaborn
plt.figure(figsize=(10, 6))
ax = sns.boxplot(x='cluster_labels_annotated_coarse_coarse', y='proteome_score', data=plot_df, showfliers=False)
ax.set_title('Plasma Proteome Score by niches')
ax.set_xlabel('iTAL_ME_20um Group')
ax.set_ylabel('Proteome Score')
plt.xticks(rotation=45)
plt.tight_layout()

# Save the plot if needed
#plt.savefig('/path/to/your/proteome_score_boxplot.png', dpi=300, bbox_inches='tight')

# Show the plot
plt.show()

plot_df = adata.obs[['proteome_score', 'type']].dropna()

# Plot the boxplot using seaborn
plt.figure(figsize=(10, 6))
ax = sns.boxplot(x='type', y='proteome_score', data=plot_df, showfliers=False)
ax.set_title('Plasma Proteome Score by niches')
ax.set_xlabel('iTAL_ME_20um Group')
ax.set_ylabel('Proteome Score')
plt.xticks(rotation=45)
plt.tight_layout()

# Save the plot if needed
#plt.savefig('/path/to/your/proteome_score_boxplot.png', dpi=300, bbox_inches='tight')

# Show the plot
plt.show()

# Commented out IPython magic to ensure Python compatibility.
import gc
# %config InlineBackend.figure_format='retina'
plt.style.use('dark_background')

for sample in adata.obs['sample'].unique():
  gc.collect()
  adata_subset=adata[adata.obs['sample']==sample]
  sc.pl.scatter(
    adata_subset,
    x="CenterX_global_px",
    y="CenterY_global_px",
    color="proteome_score",
    size=4, legend_fontsize=6,
    legend_fontoutline=2,
    palette='plasma'
)



gene_list1=['HAVCR1', 'MMP7']
sc.tl.score_genes(adata, gene_list1, score_name='KIM1MMP7')

sc.pp.log1p(adata)

import gc

for sample in adata.obs['sample'].unique():
  gc.collect()
  adata_subset=adata[adata.obs['sample']==sample]
  sc.pl.scatter(
    adata_subset,
    x="CenterX_global_px",
    y="CenterY_global_px",
    color="KIM1MMP7",
    size=4, legend_fontsize=6,
    legend_fontoutline=2,
)

plot_df = adata.obs[['KIM1MMP7', 'cluster_labels_annotated_coarse_coarse']].dropna()

# Plot the boxplot using seaborn
plt.figure(figsize=(10, 6))
ax = sns.boxplot(x='cluster_labels_annotated_coarse_coarse', y='KIM1MMP7', data=plot_df, showfliers=False)
ax.set_title('Plasma Proteome Score by niches')
ax.set_xlabel('iTAL_ME_20um Group')
ax.set_ylabel('Proteome Score')
plt.xticks(rotation=45)
plt.tight_layout()

# Save the plot if needed
#plt.savefig('/path/to/your/proteome_score_boxplot.png', dpi=300, bbox_inches='tight')

# Show the plot
plt.show()

adata

plot_df = adata.obs[['KIM1MMP7', 'iTAL_subcluster_ME']].dropna()

# Plot the boxplot using seaborn
plt.figure(figsize=(10, 6))
ax = sns.boxplot(x='iTAL_subcluster_ME', y='KIM1MMP7', data=plot_df, showfliers=False)
ax.set_title('Plasma Proteome Score by niches')
ax.set_xlabel('iTAL_ME_20um Group')
ax.set_ylabel('Proteome Score')
plt.xticks(rotation=45)
plt.tight_layout()

# Save the plot if needed
#plt.savefig('/path/to/your/proteome_score_boxplot.png', dpi=300, bbox_inches='tight')

# Show the plot
plt.show()

plot_df = adata.obs[['KIM1MMP7', 'iPT_subcluster_ME']].dropna()

# Plot the boxplot using seaborn
plt.figure(figsize=(10, 6))
ax = sns.boxplot(x='iPT_subcluster_ME', y='KIM1MMP7', data=plot_df, showfliers=False)
ax.set_title('Plasma Proteome Score by niches')
ax.set_xlabel('iTAL_ME_20um Group')
ax.set_ylabel('Proteome Score')
plt.xticks(rotation=45)
plt.tight_layout()

# Save the plot if needed
#plt.savefig('/path/to/your/proteome_score_boxplot.png', dpi=300, bbox_inches='tight')

# Show the plot
plt.show()

import gc

for sample in adata.obs['sample'].unique():
  gc.collect()
  adata_subset=adata[adata.obs['sample']==sample]
  sc.pl.scatter(
    adata_subset,
    x="CenterX_global_px",
    y="CenterY_global_px",
    color="HAVCR1",
    size=4, legend_fontsize=6,
    legend_fontoutline=2,
)

import gc

for sample in adata.obs['sample'].unique():
  gc.collect()
  adata_subset=adata[adata.obs['sample']==sample]
  sc.pl.scatter(
    adata_subset,
    x="CenterX_global_px",
    y="CenterY_global_px",
    color="MMP7",
    size=4, legend_fontsize=6,
    legend_fontoutline=2,
)

import gc

for sample in adata.obs['sample'].unique():
  gc.collect()
  adata_subset=adata[adata.obs['sample']==sample]
  sc.pl.scatter(
    adata_subset,
    x="CenterX_global_px",
    y="CenterY_global_px",
    color="iTAL_subcluster_ME_20um",
    size=4, legend_fontsize=6,
    legend_fontoutline=2,
)

import gc

for sample in adata.obs['sample'].unique():
  gc.collect()
  adata_subset=adata[adata.obs['sample']==sample]
  sc.pl.scatter(
    adata_subset,
    x="CenterX_global_px",
    y="CenterY_global_px",
    color="proteome_score",
    size=4, legend_fontsize=6,
    legend_fontoutline=2,
)

"""now we take DEGs between all groups and simply eliminate genes from the DEGs for each microenvironment using our metaanalysis"""

metaanalysis_immune=pd.read_csv('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/spatial_signatures/Immune_subset/Immune_subset_metaanalysis.csv')

import os
import pandas as pd

# Assuming the dataframes are already loaded and processed as per your previous steps

# Initialize the top_genes dictionary
top_genes = {}

# Function to get top 20 genes
def get_top_genes(df, df_name):
    df = df[~df['Gene'].isin(['MHC I', 'MALAT1', 'CCL3/L1/L3','XCL1/2', 'XIST', 'SRY','HLA-DQB1/2','C11orf96', 'HSPA1A/B',
                              'RPL37', 'RPL32', 'RPL34'])]
    top_genes[df_name] = df.head(20)['Gene'].tolist()

# Load and process the dataframes
folder_name = '/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/spatial_signatures/DEGs_for_signature'

for filename in os.listdir(folder_name):
    if 'outlier' in filename:
        continue
    df_name = filename[:-4].replace(' ', '_').replace('+', '_')
    df_path = os.path.join(folder_name, filename)
    df = pd.read_csv(df_path)
    if 'Unnamed: 0' in df.columns:
        df = df.drop('Unnamed: 0', axis=1)
    df = df[df['pValue_adj'] < 0.01]

    # Merge with the metaanalysis_immune dataframe based on the specific conditions
    if 'Immune_ME_3_DEGs' in df_name:
        filtered_metaanalysis = metaanalysis_immune[metaanalysis_immune['Min_Value_Column'] == '_20um_Immune_ME_3']
        merged_df = pd.merge(df, filtered_metaanalysis, on='Gene', how='inner')
        merged_df = merged_df[merged_df['LogFoldChange'] > 0].sort_values(by='pi_score', ascending=False)
        get_top_genes(merged_df, df_name)

    elif 'Immune_ME_4_DEGs' in df_name:
        filtered_metaanalysis = metaanalysis_immune[metaanalysis_immune['Min_Value_Column'] == '_20um_Immune_ME_4']
        merged_df = pd.merge(df, filtered_metaanalysis, on='Gene', how='inner')
        merged_df = merged_df[merged_df['LogFoldChange'] > 0].sort_values(by='pi_score', ascending=False)
        get_top_genes(merged_df, df_name)


# Print the top_genes dictionary
for key, value in top_genes.items():
    print(f"{key}: {value}")

"""now with Tubule RNA"""

expression_matrix = pd.read_csv('/content/drive/MyDrive/Bernhard/BulkSeq/HK.Biobank.Tubule.TPM_n900_230928.csv', sep= ';')

expression_matrix=expression_matrix.drop('Unnamed: 0', axis=1)

expression_matrix=expression_matrix.set_index('Symbol')

expression_matrix.index.name=None

expression_matrix=expression_matrix.T

expression_matrix.index = expression_matrix.index.str.rstrip('T')

expression_matrix = np.log2(expression_matrix + 1)

expression_matrix

row_sums = expression_matrix.sum(axis=1)

# Identify columns where the sum of any row is 0
cols_to_drop = expression_matrix.columns[(expression_matrix == 0).all(axis=0)]

# Drop those columns from the DataFrame
df_cleaned = expression_matrix.drop(columns=cols_to_drop)

# Display the cleaned DataFrame
print(df_cleaned.shape)

counts = (df_cleaned > 0).sum(axis=0)
cols_to_drop = counts[counts == 1].index

# Drop those columns from the DataFrame
df_cleaned = df_cleaned.drop(columns=cols_to_drop)

# Display the cleaned DataFrame
print(df_cleaned.shape)

columns_to_drop = [col for col in df_cleaned.columns if col.startswith('MT') or col.startswith('RP')]

# Drop those columns from the DataFrame
df_cleaned = df_cleaned.drop(columns=columns_to_drop)

# Display the cleaned DataFrame
print(df_cleaned.shape)

columns_to_drop = [col for col in df_cleaned.columns if col.startswith('TRAV') or col.startswith('TRBV')]

# Drop those columns from the DataFrame
df_cleaned = df_cleaned.drop(columns=columns_to_drop)

# Display the cleaned DataFrame
print(df_cleaned.shape)

columns_to_drop = [col for col in df_cleaned.columns if col.startswith('IGHV') or col.startswith('IGLV') or col.startswith('IGKV')]

# Drop those columns from the DataFrame
df_cleaned = df_cleaned.drop(columns=columns_to_drop)

# Display the cleaned DataFrame
print(df_cleaned.shape)

columns = ['SLPI', 'MMP7', 'ITGB6', 'KRT7', 'S100A2', 'ANXA2', 'ANXA1', 'CRIP1', 'CLDN4', 'IFITM3']
subset_df_cleaned_iTAL = df_cleaned[columns]
subset_df_cleaned_iTAL

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from scipy.spatial.distance import pdist
from scipy.cluster.hierarchy import linkage, dendrogram, fcluster
import matplotlib.pyplot as plt

# Assuming subset_df_cleaned_iTAL is already defined and contains the relevant data
scaler = StandardScaler()
scaled_data = scaler.fit_transform(subset_df_cleaned_iTAL)

# Compute the distance matrix
distance_matrix = pdist(scaled_data, metric='euclidean')

# Perform hierarchical clustering
linked = linkage(distance_matrix, method='ward')

# Plot the dendrogram with a threshold line
plt.figure(figsize=(10, 7))
dendro = dendrogram(linked, labels=subset_df_cleaned_iTAL.index, leaf_rotation=90, leaf_font_size=12)
plt.axhline(y=40, color='r', linestyle='--')
plt.title('Dendrogram of tubule bulk seq with iTAL ME1 high')
plt.xlabel('Samples')
plt.ylabel('Euclidean distances')

# Remove x-axis labels
plt.xticks([])

# Save the plot
plt.savefig('dendrogram_plot_tubules_iTAL.png', bbox_inches='tight', dpi=450)

# Show the plot
plt.show()

# Create clusters based on the threshold
distance_threshold = 40  # Adjust this value based on your dendrogram
cluster_labels = fcluster(linked, t=distance_threshold, criterion='distance')

# Add the cluster labels to the DataFrame
subset_df_cleaned_iTAL['Cluster_iTAL'] = cluster_labels

subset_df_cleaned_iTAL['Cluster_iTAL'].value_counts()

mean_scores = subset_df_cleaned_iTAL.groupby('Cluster_iTAL')['MMP7'].mean()
print(mean_scores)

subset_df_cleaned_iTAL['Cluster_iTAL']=subset_df_cleaned_iTAL['Cluster_iTAL'].replace(1,'high')
subset_df_cleaned_iTAL['Cluster_iTAL']=subset_df_cleaned_iTAL['Cluster_iTAL'].replace(2,'low')
subset_df_cleaned_iTAL['Cluster_iTAL']=subset_df_cleaned_iTAL['Cluster_iTAL'].replace(3,'low')

df_cleaned['iTAL_cluster']=subset_df_cleaned_iTAL['Cluster_iTAL'].copy()

columns = ['IGHM', 'IGKC', 'CD74', 'CXCR4', 'CD37', 'IGHA1', 'PTPRCAP', 'LTB', 'PTGDS', 'IGHG2', 'SPOCK2', 'IGHG1', 'CD79A', 'HLA-DRA', 'IGHD', 'CCL19', 'CD69', 'CD52', 'MS4A1', 'CD53']
subset_expression_matrix=df_cleaned[df_cleaned['iTAL_cluster']=='high']
subset_df_cleaned_b = subset_expression_matrix[columns]
subset_df_cleaned_b

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from scipy.spatial.distance import pdist
from scipy.cluster.hierarchy import linkage, dendrogram, fcluster
import matplotlib.pyplot as plt

# Assuming subset_df_cleaned_b is already defined and contains the relevant data
scaler = StandardScaler()
scaled_data = scaler.fit_transform(subset_df_cleaned_b)

# Compute the distance matrix
distance_matrix = pdist(scaled_data, metric='euclidean')

# Perform hierarchical clustering
linked = linkage(distance_matrix, method='ward')

# Plot the dendrogram with a threshold line
plt.figure(figsize=(10, 7))
dendro = dendrogram(linked, labels=subset_df_cleaned_b.index, leaf_rotation=90, leaf_font_size=12)
plt.axhline(y=30, color='r', linestyle='--')
plt.title('Dendrogram of tubule bulk seq with immune ME 3 high')
plt.xlabel('Samples')
plt.ylabel('Euclidean distances')

# Remove x-axis labels
plt.xticks([])

# Save the plot
plt.savefig('dendrogram_plot_tubules_ME3.png', bbox_inches='tight', dpi=450)

# Show the plot
plt.show()

# Create clusters based on the threshold
distance_threshold = 30  # Adjust this value based on your dendrogram
cluster_labels = fcluster(linked, t=distance_threshold, criterion='distance')

# Add the cluster labels to the DataFrame
subset_df_cleaned_b['Cluster'] = cluster_labels

subset_df_cleaned_b['Cluster'].value_counts()

mean_scores = subset_df_cleaned_b.groupby('Cluster')['IGKC'].mean()
print(mean_scores)

subset_df_cleaned_b['Cluster']=subset_df_cleaned_b['Cluster'].replace(1, 'high')
subset_df_cleaned_b['Cluster']=subset_df_cleaned_b['Cluster'].replace(2, 'low')
#subset_df_cleaned_b['Cluster']=subset_df_cleaned_b['Cluster'].replace(3, 'low')
#subset_df_cleaned_b['Cluster']=subset_df_cleaned_b['Cluster'].replace(4, 'low')

df_cleaned['b_cluster']=subset_df_cleaned_b['Cluster'].copy()

df_cleaned['b_cluster']=df_cleaned['b_cluster'].fillna('low')

df_cleaned.to_csv('expressionmatrix_cleaned_tubules_cluster_iTAL_clustering_then_b_cell.csv', index=True)

"""Now the Cortex Dataset"""

expression_matrix = pd.read_csv('/content/drive/MyDrive/Bernhard/BulkSeq/HK.Biobank.Cortex.TPM_n508_230928.csv', sep= ';')

expression_matrix=expression_matrix.drop('Unnamed: 0', axis=1)

expression_matrix=expression_matrix.set_index('Genes')

expression_matrix.index.name=None

expression_matrix=expression_matrix.T

expression_matrix.index = expression_matrix.index.str.rstrip('W')

expression_matrix = np.log2(expression_matrix + 1)

expression_matrix

row_sums = expression_matrix.sum(axis=1)

# Identify columns where the sum of any row is 0
cols_to_drop = expression_matrix.columns[(expression_matrix == 0).all(axis=0)]

# Drop those columns from the DataFrame
df_cleaned = expression_matrix.drop(columns=cols_to_drop)

# Display the cleaned DataFrame
print(df_cleaned.shape)

counts = (df_cleaned > 0).sum(axis=0)
cols_to_drop = counts[counts == 1].index

# Drop those columns from the DataFrame
df_cleaned = df_cleaned.drop(columns=cols_to_drop)

# Display the cleaned DataFrame
print(df_cleaned.shape)

columns_to_drop = [col for col in df_cleaned.columns if col.startswith('MT') or col.startswith('RP')]

# Drop those columns from the DataFrame
df_cleaned = df_cleaned.drop(columns=columns_to_drop)

# Display the cleaned DataFrame
print(df_cleaned.shape)

columns_to_drop = [col for col in df_cleaned.columns if col.startswith('TRAV') or col.startswith('TRBV')]

# Drop those columns from the DataFrame
df_cleaned = df_cleaned.drop(columns=columns_to_drop)

# Display the cleaned DataFrame
print(df_cleaned.shape)

columns_to_drop = [col for col in df_cleaned.columns if col.startswith('IGHV') or col.startswith('IGLV') or col.startswith('IGKV')]

# Drop those columns from the DataFrame
df_cleaned = df_cleaned.drop(columns=columns_to_drop)

# Display the cleaned DataFrame
print(df_cleaned.shape)

columns = ['SLPI', 'MMP7', 'ITGB6', 'KRT7', 'S100A2', 'ANXA2', 'ANXA1', 'CRIP1', 'CLDN4', 'IFITM3']
subset_df_cleaned_iTAL = df_cleaned[columns]
subset_df_cleaned_iTAL

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from scipy.spatial.distance import pdist
from scipy.cluster.hierarchy import linkage, dendrogram, fcluster
import matplotlib.pyplot as plt

# Assuming subset_df_cleaned_iTAL is already defined and contains the relevant data
scaler = StandardScaler()
scaled_data = scaler.fit_transform(subset_df_cleaned_iTAL)

# Compute the distance matrix
distance_matrix = pdist(scaled_data, metric='euclidean')

# Perform hierarchical clustering
linked = linkage(distance_matrix, method='ward')

# Plot the dendrogram with a threshold line
plt.figure(figsize=(10, 7))
dendro = dendrogram(linked, labels=subset_df_cleaned_iTAL.index, leaf_rotation=90, leaf_font_size=12)
plt.axhline(y=30, color='r', linestyle='--')
plt.title('Dendrogram of cortex bulk seq with iTAL ME1 high')
plt.xlabel('Samples')
plt.ylabel('Euclidean distances')

# Remove x-axis labels
plt.xticks([])

# Save the plot
plt.savefig('dendrogram_plot_cortex_iTAL.png', bbox_inches='tight', dpi=450)

# Show the plot
plt.show()

# Create clusters based on the threshold
distance_threshold = 30  # Adjust this value based on your dendrogram
cluster_labels = fcluster(linked, t=distance_threshold, criterion='distance')

# Add the cluster labels to the DataFrame
subset_df_cleaned_iTAL['Cluster_iTAL'] = cluster_labels

subset_df_cleaned_iTAL['Cluster_iTAL'].value_counts()

mean_scores = subset_df_cleaned_iTAL.groupby('Cluster_iTAL')['MMP7'].mean()
print(mean_scores)

subset_df_cleaned_iTAL['Cluster_iTAL']=subset_df_cleaned_iTAL['Cluster_iTAL'].replace(1,'low')
subset_df_cleaned_iTAL['Cluster_iTAL']=subset_df_cleaned_iTAL['Cluster_iTAL'].replace(2,'low')
subset_df_cleaned_iTAL['Cluster_iTAL']=subset_df_cleaned_iTAL['Cluster_iTAL'].replace(3,'high')

df_cleaned['iTAL_cluster']=subset_df_cleaned_iTAL['Cluster_iTAL'].copy()

columns = ['IGHM', 'IGKC', 'CD74', 'CXCR4', 'CD37', 'IGHA1', 'PTPRCAP', 'LTB', 'PTGDS', 'IGHG2', 'SPOCK2', 'IGHG1', 'CD79A', 'HLA-DRA', 'IGHD', 'CCL19', 'CD69', 'CD52', 'MS4A1', 'CD53']
subset_expression_matrix=df_cleaned[df_cleaned['iTAL_cluster']=='high']
subset_df_cleaned_b = subset_expression_matrix[columns]
subset_df_cleaned_b

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from scipy.spatial.distance import pdist
from scipy.cluster.hierarchy import linkage, dendrogram, fcluster
import matplotlib.pyplot as plt

# Assuming subset_df_cleaned_b is already defined and contains the relevant data
scaler = StandardScaler()
scaled_data = scaler.fit_transform(subset_df_cleaned_b)

# Compute the distance matrix
distance_matrix = pdist(scaled_data, metric='euclidean')

# Perform hierarchical clustering
linked = linkage(distance_matrix, method='ward')

# Plot the dendrogram with a threshold line
plt.figure(figsize=(10, 7))
dendro = dendrogram(linked, labels=subset_df_cleaned_b.index, leaf_rotation=90, leaf_font_size=12)
plt.axhline(y=30, color='r', linestyle='--')
plt.title('Dendrogram of cortex bulk seq with immune ME 3 high')
plt.xlabel('Samples')
plt.ylabel('Euclidean distances')

# Remove x-axis labels
plt.xticks([])

# Save the plot
plt.savefig('dendrogram_plot_cortex_ME3.png', bbox_inches='tight', dpi=450)

# Show the plot
plt.show()

# Create clusters based on the threshold
distance_threshold = 30  # Adjust this value based on your dendrogram
cluster_labels = fcluster(linked, t=distance_threshold, criterion='distance')

# Add the cluster labels to the DataFrame
subset_df_cleaned_b['Cluster'] = cluster_labels

subset_df_cleaned_b['Cluster'].value_counts()

mean_scores = subset_df_cleaned_b.groupby('Cluster')['IGKC'].mean()
print(mean_scores)

subset_df_cleaned_b['Cluster']=subset_df_cleaned_b['Cluster'].replace(1, 'low')
subset_df_cleaned_b['Cluster']=subset_df_cleaned_b['Cluster'].replace(2, 'high')
#subset_df_cleaned_b['Cluster']=subset_df_cleaned_b['Cluster'].replace(3, 'low')
#subset_df_cleaned_b['Cluster']=subset_df_cleaned_b['Cluster'].replace(4, 'low')

df_cleaned['b_cluster']=subset_df_cleaned_b['Cluster'].copy()

df_cleaned['b_cluster']=df_cleaned['b_cluster'].fillna('low')

df_cleaned.to_csv('expressionmatrix_cleaned_cortex_cluster_iTAL_clustering_then_b_cell.csv', index=True)

"""Now TRIDENT"""

expression_matrix = pd.read_csv('/content/drive/MyDrive/Bernhard/BulkSeq/Tridentalltpm.csv', sep= ';')

expression_matrix

expression_matrix=expression_matrix.set_index('GeneSymbol')

expression_matrix.index.name=None

expression_matrix=expression_matrix.T

expression_matrix = expression_matrix[expression_matrix.index.str.endswith('T')] ###G for the Glomclustering

expression_matrix.index = expression_matrix.index.str.rstrip('T')

expression_matrix = np.log2(expression_matrix + 1)

expression_matrix

row_sums = expression_matrix.sum(axis=1)

# Identify columns where the sum of any row is 0
cols_to_drop = expression_matrix.columns[(expression_matrix == 0).all(axis=0)]

# Drop those columns from the DataFrame
df_cleaned = expression_matrix.drop(columns=cols_to_drop)

# Display the cleaned DataFrame
print(df_cleaned.shape)

counts = (df_cleaned > 0).sum(axis=0)
cols_to_drop = counts[counts == 1].index

# Drop those columns from the DataFrame
df_cleaned = df_cleaned.drop(columns=cols_to_drop)

# Display the cleaned DataFrame
print(df_cleaned.shape)

columns_to_drop = [col for col in df_cleaned.columns if col.startswith('MT') or col.startswith('RP')]

# Drop those columns from the DataFrame
df_cleaned = df_cleaned.drop(columns=columns_to_drop)

# Display the cleaned DataFrame
print(df_cleaned.shape)

columns_to_drop = [col for col in df_cleaned.columns if col.startswith('TRAV') or col.startswith('TRBV')]

# Drop those columns from the DataFrame
df_cleaned = df_cleaned.drop(columns=columns_to_drop)

# Display the cleaned DataFrame
print(df_cleaned.shape)

columns_to_drop = [col for col in df_cleaned.columns if col.startswith('IGHV') or col.startswith('IGLV') or col.startswith('IGKV')]

# Drop those columns from the DataFrame
df_cleaned = df_cleaned.drop(columns=columns_to_drop)

# Display the cleaned DataFrame
print(df_cleaned.shape)

columns = ['SLPI', 'MMP7', 'ITGB6', 'KRT7', 'S100A2', 'ANXA2', 'ANXA1', 'CRIP1', 'CLDN4', 'IFITM3']
subset_df_cleaned_iTAL = df_cleaned[columns]
subset_df_cleaned_iTAL

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from scipy.spatial.distance import pdist
from scipy.cluster.hierarchy import linkage, dendrogram, fcluster
import matplotlib.pyplot as plt

# Assuming subset_df_cleaned_iTAL is already defined and contains the relevant data
scaler = StandardScaler()
scaled_data = scaler.fit_transform(subset_df_cleaned_iTAL)

# Compute the distance matrix
distance_matrix = pdist(scaled_data, metric='euclidean')

# Perform hierarchical clustering
linked = linkage(distance_matrix, method='ward')

# Plot the dendrogram with a threshold line
plt.figure(figsize=(10, 7))
dendro = dendrogram(linked, labels=subset_df_cleaned_iTAL.index, leaf_rotation=90, leaf_font_size=12)
plt.axhline(y=30, color='r', linestyle='--')
plt.title('Dendrogram of TRIDENT bulk seq with iTAL ME1 high')
plt.xlabel('Samples')
plt.ylabel('Euclidean distances')

# Remove x-axis labels
plt.xticks([])

# Save the plot
plt.savefig('dendrogram_plot_TRIDENT_iTAL.png', bbox_inches='tight', dpi=450)

# Show the plot
plt.show()

# Create clusters based on the threshold
distance_threshold = 30  # Adjust this value based on your dendrogram
cluster_labels = fcluster(linked, t=distance_threshold, criterion='distance')

# Add the cluster labels to the DataFrame
subset_df_cleaned_iTAL['Cluster_iTAL'] = cluster_labels

subset_df_cleaned_iTAL['Cluster_iTAL'].value_counts()

mean_scores = subset_df_cleaned_iTAL.groupby('Cluster_iTAL')['MMP7'].mean()
print(mean_scores)

subset_df_cleaned_iTAL['Cluster_iTAL']=subset_df_cleaned_iTAL['Cluster_iTAL'].replace(2,'high')
subset_df_cleaned_iTAL['Cluster_iTAL']=subset_df_cleaned_iTAL['Cluster_iTAL'].replace(1,'low')

df_cleaned['iTAL_cluster']=subset_df_cleaned_iTAL['Cluster_iTAL'].copy()

columns = ['IGHM', 'IGKC', 'CD74', 'CXCR4', 'CD37', 'IGHA1', 'PTPRCAP', 'LTB', 'PTGDS', 'IGHG2', 'SPOCK2', 'IGHG1', 'CD79A', 'HLA-DRA', 'IGHD', 'CCL19', 'CD69', 'CD52', 'MS4A1', 'CD53']
subset_expression_matrix=df_cleaned[df_cleaned['iTAL_cluster']=='high']
subset_df_cleaned_b = subset_expression_matrix[columns]
subset_df_cleaned_b

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from scipy.spatial.distance import pdist
from scipy.cluster.hierarchy import linkage, dendrogram, fcluster
import matplotlib.pyplot as plt

# Assuming subset_df_cleaned_b is already defined and contains the relevant data
scaler = StandardScaler()
scaled_data = scaler.fit_transform(subset_df_cleaned_b)

# Compute the distance matrix
distance_matrix = pdist(scaled_data, metric='euclidean')

# Perform hierarchical clustering
linked = linkage(distance_matrix, method='ward')

# Plot the dendrogram with a threshold line
plt.figure(figsize=(10, 7))
dendro = dendrogram(linked, labels=subset_df_cleaned_b.index, leaf_rotation=90, leaf_font_size=12)
plt.axhline(y=30, color='r', linestyle='--')
plt.title('Dendrogram of TRIDENT bulk seq with immune ME 3 high')
plt.xlabel('Samples')
plt.ylabel('Euclidean distances')

# Remove x-axis labels
plt.xticks([])

# Save the plot
plt.savefig('dendrogram_plot_TRIDENT_ME3.png', bbox_inches='tight', dpi=450)

# Show the plot
plt.show()

# Create clusters based on the threshold
distance_threshold = 30  # Adjust this value based on your dendrogram
cluster_labels = fcluster(linked, t=distance_threshold, criterion='distance')

# Add the cluster labels to the DataFrame
subset_df_cleaned_b['Cluster'] = cluster_labels

subset_df_cleaned_b['Cluster'].value_counts()

mean_scores = subset_df_cleaned_b.groupby('Cluster')['IGKC'].mean()
print(mean_scores)

subset_df_cleaned_b['Cluster']=subset_df_cleaned_b['Cluster'].replace(1, 'high')
subset_df_cleaned_b['Cluster']=subset_df_cleaned_b['Cluster'].replace(2, 'low')
subset_df_cleaned_b['Cluster']=subset_df_cleaned_b['Cluster'].replace(3, 'low')

df_cleaned['b_cluster']=subset_df_cleaned_b['Cluster'].copy()

df_cleaned['b_cluster']=df_cleaned['b_cluster'].fillna('low')

df_cleaned.to_csv('expressionmatrix_cleaned_TRIDENT_cluster_iTAL_clustering_then_b_cell.csv', index=True)

















"""this is after LIMMA DEGS"""

import os
import pandas as pd

folder_name = '/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/spatial_signatures/Bulk_Seq_Comparison_iTAL_B/'

# List all files in the folder and load only those containing 'DEG' in their filename
for filename in os.listdir(folder_name):
    if filename.endswith('.csv') and 'differential_expression' in filename:
        # Create the dataframe name by stripping off the '.csv' extension and replacing spaces with underscores
        df_name = os.path.splitext(filename)[0].replace(' ', '_').replace('+', '_')
        df_name = os.path.splitext(filename)[0].replace('differential_expression_results_', '')
        # Read the CSV file into a dataframe
        df_path = os.path.join(folder_name, filename)
        globals()[df_name] = pd.read_csv(df_path)
        print(f"Loaded dataframe: {df_name} with shape: {globals()[df_name].shape}")

!pip --quiet install gseapy

import gseapy
from gseapy import barplot, dotplot

populations_to_test = [tubule_ital_rest_vs_high,cortex_ital_high_vs_rest,cortex_ital_rest_vs_high,tubule_b_high_vs_rest,
                       tubule_b_rest_vs_high,tubule_ital_high_vs_rest,TRIDENT_ital_rest_vs_high,cortex_b_high_vs_rest,cortex_b_rest_vs_high,
                       TRIDENT_ital_high_vs_rest,TRIDENT_b_rest_vs_high,TRIDENT_b_high_vs_rest]

for i in populations_to_test:
    i["names"] = i['Unnamed: 0']
    i["pi_score"] = -1 * np.log10(i["adj.P.Val"]) * i["logFC"] # pvalue
    i.dropna(inplace=True)

v_library='MSigDB_Hallmark_2020' ## 'WikiPathway_2023_Human' ###
gset = gseapy.parser.get_library(v_library, min_size=20)

import gseapy
import pandas as pd

# Assuming gset is already defined
populations_to_test = [
    'tubule_ital_rest_vs_high', 'cortex_ital_high_vs_rest', 'cortex_ital_rest_vs_high', 'tubule_b_high_vs_rest',
    'tubule_b_rest_vs_high', 'tubule_ital_high_vs_rest', 'TRIDENT_ital_rest_vs_high', 'cortex_b_high_vs_rest', 'cortex_b_rest_vs_high',
    'TRIDENT_ital_high_vs_rest', 'TRIDENT_b_rest_vs_high', 'TRIDENT_b_high_vs_rest'
]

# Loop through each dataframe name
for df_name in populations_to_test:
    # Access the dataframe using the name
    df = globals()[df_name]

    # Perform the operations
    gene_rank = df[['names', 'pi_score']]
    gene_rank.sort_values(by=['pi_score'], inplace=True, ascending=False)
    gene_rank = gene_rank.reset_index(drop=True)
    background_genes = gene_rank['names'].tolist()
    res = gseapy.prerank(rnk=gene_rank, gene_sets=gset)

    # Filter the results
    res.res2d_plot = res.res2d[res.res2d["FWER p-val"] < 0.05]

    # Create a new dataframe name with '_gset' appended
    new_df_name = f"{df_name}_gset"

    # Assign the dataframe to the new name in the global namespace
    globals()[new_df_name] = res.res2d.copy()

    # Print the shape of the new dataframe
    print(f"Created dataframe: {new_df_name} with shape: {globals()[new_df_name].shape}")

tubule_ital_high_vs_rest_gset

# List of dataframe names with the '_gset' suffix
gset_dfs = [
    'tubule_ital_rest_vs_high_gset', 'cortex_ital_high_vs_rest_gset', 'cortex_ital_rest_vs_high_gset', 'tubule_b_high_vs_rest_gset',
    'tubule_b_rest_vs_high_gset', 'tubule_ital_high_vs_rest_gset', 'TRIDENT_ital_rest_vs_high_gset', 'cortex_b_high_vs_rest_gset', 'cortex_b_rest_vs_high_gset',
    'TRIDENT_ital_high_vs_rest_gset', 'TRIDENT_b_rest_vs_high_gset', 'TRIDENT_b_high_vs_rest_gset'
]

# Dictionary to store the top 25 pathways for each dataframe
top_25_pathways = {}

# Loop through each dataframe name
for df_name in gset_dfs:
    # Access the dataframe using the name
    df = globals()[df_name]

    # Filter the dataframe
    filtered_df = df[df['NES'] > 0]

    # Sort the dataframe by 'FDR q-val' in ascending order
    sorted_df = filtered_df.sort_values(by='FDR q-val', ascending=True)

    # Get the top 25 pathways
    top_25 = sorted_df['Term'].head(25)

    # Store the top 25 pathways in the dictionary
    top_25_pathways[df_name] = top_25

    # Optionally, print the top 25 pathways for each dataframe
    print(f"Top 25 pathways for {df_name}:")
    print(top_25)

# The top 25 pathways for each dataframe are now stored in the `top_25_pathways` dictionary

# Keywords to filter and group the dataframes
keywords = ['ital_rest_vs_high_gset', 'ital_high_vs_rest_gset', 'b_high_vs_rest_gset', 'b_rest_vs_high_gset']

# Dictionary to store the intersected pathways
intersections = {}

# Loop through each keyword
for keyword in keywords:
    # Find series that match the keyword
    matching_series = [series for name, series in top_25_pathways.items() if keyword in name]

    # If there are matching series, find their intersection
    if matching_series:
        # Initialize the intersection with the pathways of the first series
        intersection = set(matching_series[0])

        # Intersect with pathways from the remaining series
        for series in matching_series[1:]:
            intersection &= set(series)

        # Store the intersection in the dictionary
        intersections[keyword] = list(intersection)

# Print the intersecting pathways for each keyword
for keyword, intersection in intersections.items():
    print(f"Intersecting pathways for {keyword}:")
    print(intersection)

subset=tubule_b_high_vs_rest_gset[tubule_b_high_vs_rest_gset['FDR q-val']<0.05]
subset.head(5)

subset2=tubule_ital_high_vs_rest_gset[tubule_b_high_vs_rest_gset['FDR q-val']<0.05]
subset2

pathway_dict = dict(zip(subset['Term'], subset['Lead_genes']))
subset2_pathway_dict = dict(zip(subset2['Term'], subset2['Lead_genes']))

# Update the existing pathway_dict with the new entries from subset2
pathway_dict.update(subset2_pathway_dict)

# Print the dictionary
print(pathway_dict)

all_lead_genes = set()
for genes in pathway_dict.values():
    all_lead_genes.update(genes.split(';'))

# Subset df_cleaned to include only columns that are in the set of lead genes
subset_df_cleaned = df_cleaned.loc[:, df_cleaned.columns.isin(all_lead_genes)]

# Optionally, keep 'iTAL_cluster' if it's needed
subset_df_cleaned = pd.concat([subset_df_cleaned, df_cleaned['iTAL_cluster'],df_cleaned['b_cluster']], axis=1)

# Print the resulting subset dataframe
#print(subset_df_cleaned)

import pandas as pd
from sklearn.preprocessing import StandardScaler

# Define a function to process each dataframe
def process_dataframe(df, pathway_dict):
    all_lead_genes = set()
    for genes in pathway_dict.values():
        all_lead_genes.update(genes.split(';'))

    # Subset df to include only columns that are in the set of lead genes
    subset_df_cleaned = df.loc[:, df.columns.isin(all_lead_genes)]

    # Scale the subsetted dataframe
    scaler = StandardScaler()
    scaled_data = scaler.fit_transform(subset_df_cleaned)

    # Convert scaled data back to a dataframe
    scaled_df = pd.DataFrame(scaled_data, columns=subset_df_cleaned.columns, index=subset_df_cleaned.index)

    # Add the 'iTAL_cluster' and 'b_cluster' columns back
    scaled_df = pd.concat([scaled_df, df[['iTAL_cluster', 'b_cluster']]], axis=1)

    # Calculate the average expression of genes for each pathway and add as new columns
    for pathway, genes in pathway_dict.items():
        gene_list = genes.split(';')
        intersecting_genes = [gene for gene in gene_list if gene in scaled_df.columns]
        if intersecting_genes:
            scaled_df[pathway] = scaled_df[intersecting_genes].mean(axis=1)

    return scaled_df

# Read the dataframes
TRIDENT = pd.read_csv('/content/expressionmatrix_cleaned_TRIDENT_cluster_iTAL_clustering_then_b_cell.csv')
Tubule = pd.read_csv('/content/expressionmatrix_cleaned_tubules_cluster_iTAL_clustering_then_b_cell.csv')
Cortex = pd.read_csv('/content/expressionmatrix_cleaned_cortex_cluster_iTAL_clustering_then_b_cell.csv')

# Process each dataframe
scaled_TRIDENT = process_dataframe(TRIDENT, pathway_dict)
scaled_Tubule = process_dataframe(Tubule, pathway_dict)
scaled_Cortex = process_dataframe(Cortex, pathway_dict)

# Print the resulting dataframes
print("Scaled TRIDENT DataFrame:")
print(scaled_TRIDENT.shape)

print("\nScaled Tubule DataFrame:")
print(scaled_Tubule.shape)

print("\nScaled Cortex DataFrame:")
print(scaled_Cortex.shape)

# Function to sort the dataframe as specified
def sort_dataframe(df):
    # Sort by 'iTAL_cluster' in descending order
    df_sorted = df.sort_values(by='iTAL_cluster', ascending=False)

    # Sort the rows where 'iTAL_cluster' is 'high' by 'b_cluster' in descending order
    high_cluster_sorted = df_sorted[df_sorted['iTAL_cluster'] == 'high'].sort_values(by='b_cluster', ascending=False)

    # Separate rows where 'iTAL_cluster' is not 'high'
    non_high_cluster = df_sorted[df_sorted['iTAL_cluster'] != 'high']

    # Combine the sorted 'high' cluster with the rest of the dataframe
    sorted_combined_df = pd.concat([non_high_cluster, high_cluster_sorted])

    return sorted_combined_df

# Sort each processed dataframe
sorted_TRIDENT = sort_dataframe(scaled_TRIDENT)
sorted_Tubule = sort_dataframe(scaled_Tubule)
sorted_Cortex = sort_dataframe(scaled_Cortex)

# Print the sorted dataframes
print("Sorted TRIDENT DataFrame:")
print(sorted_TRIDENT)

print("\nSorted Tubule DataFrame:")
print(sorted_Tubule)

print("\nSorted Cortex DataFrame:")
print(sorted_Cortex)

print(sorted_Cortex['b_cluster'])

sorted_TRIDENT.to_csv('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/spatial_signatures/Bulk_Seq_Comparison_iTAL_B/scaled_TRIDENT.csv', index=True)
sorted_Cortex.to_csv('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/spatial_signatures/Bulk_Seq_Comparison_iTAL_B/scaled_Cortex.csv', index=True)
sorted_Tubule.to_csv('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/spatial_signatures/Bulk_Seq_Comparison_iTAL_B/scaled_Tubule.csv', index=True)

from sklearn.preprocessing import MinMaxScaler

# Function to scale pathway columns from -1 to 1 and add 'iTAL_cluster' and 'b_cluster' back
def scale_and_add_columns(df):
    # Drop 'iTAL_cluster' and 'b_cluster' before scaling
    pathway_columns_df = df.drop(columns=['iTAL_cluster', 'b_cluster'])

    # Scale the pathway columns from -1 to 1
    scaler = MinMaxScaler(feature_range=(-1, 1))
    scaled_pathway_data = scaler.fit_transform(pathway_columns_df)

    # Convert the scaled data back to a dataframe
    scaled_pathway_df = pd.DataFrame(scaled_pathway_data, columns=pathway_columns_df.columns, index=pathway_columns_df.index)

    # Add the 'iTAL_cluster' and 'b_cluster' columns back
    scaled_pathway_df = pd.concat([scaled_pathway_df, df[['iTAL_cluster', 'b_cluster']]], axis=1)

    return scaled_pathway_df

# Scale and add columns for each sorted dataframe
scaled_sorted_TRIDENT = scale_and_add_columns(sorted_TRIDENT)
scaled_sorted_Tubule = scale_and_add_columns(sorted_Tubule)
scaled_sorted_Cortex = scale_and_add_columns(sorted_Cortex)

# Print the resulting dataframes
print("Scaled and Sorted TRIDENT DataFrame:")
print(scaled_sorted_TRIDENT)

print("\nScaled and Sorted Tubule DataFrame:")
print(scaled_sorted_Tubule)

print("\nScaled and Sorted Cortex DataFrame:")
print(scaled_sorted_Cortex)

# Stack the three dataframes on top of each other
stacked_df = pd.concat([scaled_sorted_TRIDENT, scaled_sorted_Tubule, scaled_sorted_Cortex], axis=0)

# Print the resulting stacked dataframe
print(stacked_df)

stacked_df_sorted = stacked_df.sort_values(by='iTAL_cluster',ascending=False)

# Then, sort the rows where 'iTAL_cluster' is 'high' by 'b_cluster'
high_cluster_sorted = stacked_df_sorted[stacked_df_sorted['iTAL_cluster'] == 'high'].sort_values(by='b_cluster', ascending=False)
non_high_cluster = stacked_df_sorted[stacked_df_sorted['iTAL_cluster'] != 'high']

# Combine the sorted 'high' cluster with the rest of the dataframe
sorted_combined_df = pd.concat([non_high_cluster, high_cluster_sorted])

sorted_combined_df

sorted_combined_df['combined']=sorted_combined_df['iTAL_cluster']+' '+sorted_combined_df['b_cluster']
sorted_combined_df['combined']=sorted_combined_df['combined'].replace('low low', 'low')
sorted_combined_df['combined']=sorted_combined_df['combined'].replace('high low', 'iTAL')
sorted_combined_df['combined']=sorted_combined_df['combined'].replace('high high', 'iTAL+B')

sorted_combined_df = sorted_combined_df.set_index('combined')

sorted_combined_df.index.name = None

pathway_columns = list(pathway_dict.keys())
subset_sorted_combined_df=sorted_combined_df[pathway_columns]

subset_sorted_combined_df

subset_sorted_combined_df.columns

desired_order = [
    'Oxidative Phosphorylation', 'Adipogenesis', 'Fatty Acid Metabolism',
       'Pperoxisome',
       'Xenobiotic Metabolism',  'heme Metabolism','Protein Secretion','PI3K/AKT/mTOR  Signaling', 'Apoptosis', 'Apical Junction',
      'Interferon Gamma Response',
       'Interferon Alpha Response',
       'TNF-alpha Signaling via NF-kB',
       'Complement', 'IL-2/STAT5 Signaling',
       'Angiogenesis', 'UV Response Dn',
       'Wnt-beta Catenin Signaling','IL-6/JAK/STAT3 Signaling','KRAS Signaling Up','Epithelial Mesenchymal Transition','Inflammatory Response','Allograft Rejection',
]

# Reorder the columns of the dataframe
reordered_df = subset_sorted_combined_df[desired_order]

# Create the heatmap
plt.figure(figsize=(8, 8))
ax=sns.heatmap(reordered_df, cmap="viridis", annot=False, vmin=0, vmax=1,cbar_kws={'shrink': 0.2})

# Customize the heatmap
plt.title('Pathway Activity (1200 patients, 1600 bulkseq samples)')
plt.xlabel('')
plt.ylabel('')
ax.tick_params(axis='both', which='both', length=0)

# Save the figure with high resolution
plt.savefig('pathway_activity_bulk.png', dpi=450, bbox_inches='tight')
# Show the heatmap
plt.show()

































import numpy as np

"""Lets try Limma on plasma proteomics"""

cluster_trident = pd.read_csv('/content/expressionmatrix_cleaned_TRIDENT_cluster_iTAL_clustering_then_b_cell.csv')

cluster_trident

cluster_trident=cluster_trident.set_index('Unnamed: 0')

cluster_trident.index = cluster_trident.index.astype(int)

columns=['iTAL_cluster', 'b_cluster']
cluster_trident=cluster_trident[columns]
cluster_trident.index.name=None
cluster_trident

trident_plasma=pd.read_csv('/content/trident_proteomics.csv')

trident_plasma=trident_plasma.set_index('SampleID')

trident_plasma.index.name=None

trident_plasma.index = trident_plasma.index.astype(int)

trident_plasma.head(3)

trident_plasma=trident_plasma.dropna(axis=1)
trident_plasma

combined_df = pd.merge(trident_plasma, cluster_trident, left_index=True, right_index=True, how='inner')

# Display the combined DataFrame
print(combined_df)

combined_df.to_csv('plasma_proteomics_trident_clustered_iTAL_then_B.csv', index=True)

cluster_trident

cluster_trident['combined']=cluster_trident['iTAL_cluster']+' '+cluster_trident['b_cluster']

cluster_trident.to_csv('final_clusters_trident_for_samer.csv', index=True)

plasma_degs=pd.read_csv('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/spatial_signatures/Bulk_Seq_Comparison_iTAL_B/plasma_proteomics/differential_expression_results_TRIDENT_plasma_proteomics_ital_high_vs_rest.csv')

#plasma_degs=plasma_degs[plasma_degs['adj.P.Val']<0.05]
#plasma_degs

plasma_degs["pi_score"] = -1 * np.log10(plasma_degs["adj.P.Val"]) * plasma_degs["logFC"]

plasma_degs['names']=plasma_degs['Unnamed: 0'].copy()

plasma_degs=plasma_degs[plasma_degs['adj.P.Val']<0.05]
plasma_degs=plasma_degs[plasma_degs['logFC']>0]
plasma_degs

plasma_degs=plasma_degs.set_index('names')



"""lets do proteomics now"""

tissue_proteomics=pd.read_csv('/content/7288_human_aptamers_tissue_expr_log2_n332_without_outlier.csv')

tissue_proteomics=tissue_proteomics.drop(['TargetFullName', 'EntrezGeneID', 'SOMAmer_ID', 'UniProt', 'Target'], axis=1)
tissue_proteomics=tissue_proteomics.set_index('EntrezGeneSymbol')
tissue_proteomics.index.name=None
tissue_proteomics=tissue_proteomics.T
tissue_proteomics

cortex_clusters=pd.read_csv('/content/expressionmatrix_cleaned_cortex_cluster_iTAL_clustering_then_b_cell.csv')

cortex_clusters=cortex_clusters.set_index('Unnamed: 0')
cortex_clusters.index.name=None
columns=['iTAL_cluster', 'b_cluster']
cortex_clusters=cortex_clusters[columns].copy()
cortex_clusters

combined_df_2 = pd.merge(tissue_proteomics, cortex_clusters, left_index=True, right_index=True, how='inner')

# Display the combined DataFrame
print(combined_df_2)

combined_df_2.to_csv('tissue_proteomics_clustered_iTAL_then_B.csv', index=True)

"""after Limma"""

tissue_degs=pd.read_csv('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/spatial_signatures/Bulk_Seq_Comparison_iTAL_B/tissue_proteomics/differential_expression_results_cortex_tissue_proteomics_ital_high_vs_rest.csv')

#plasma_degs=plasma_degs[plasma_degs['adj.P.Val']<0.05]
#plasma_degs

tissue_degs["pi_score"] = -1 * np.log10(tissue_degs["adj.P.Val"]) * tissue_degs["logFC"]

tissue_degs

tissue_degs['names']=tissue_degs['Unnamed: 0'].copy()
tissue_degs_singificant=tissue_degs[tissue_degs['adj.P.Val']<0.05]
tissue_degs_singificant=tissue_degs_singificant[tissue_degs_singificant['logFC']>0]
tissue_degs_singificant

tissue_degs_singificant=tissue_degs_singificant.set_index('names')
tissue_degs_singificant.index.name=None
tissue_degs_singificant

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()

# Fit and transform the pi_score column
tissue_degs_singificant['pi_score_scaled_tissue'] = scaler.fit_transform(tissue_degs_singificant[['pi_score']])
plasma_degs['pi_score_scaled_plasma'] = scaler.fit_transform(plasma_degs[['pi_score']])

combined_df_significant = pd.merge(plasma_degs, tissue_degs_singificant, left_index=True, right_index=True, how='inner')
combined_df_significant

combined_df_significant['combined']=combined_df_significant['pi_score_scaled_tissue']+combined_df_significant['pi_score_scaled_plasma']
combined_df_significant

gft60=gft60.sort_values('coef', ascending=False)

gft60 = pd.read_csv('/content/gft60_event_ki.csv')

from statsmodels.stats.multitest import multipletests
idx = ['MMP7', 'WFDC2', 'HAVCR1', 'NEFL','LAYN', 'EFEMP1',  'PTK7', 'ROR2', 'TNFRSF19']

gft60 = pd.read_csv('/content/gft60_event_ki.csv')
col_r = ["i",'EntrezGeneSymbol', "Target", 'coef', 'exp(coef)', 'se(coef)', 'exp(coef) lower 95%', 'exp(coef) upper 95%', 'p', 'AUC']
gft60 = gft60[col_r]
# Drop rows with NaN values in the 'p' column
gft60 = gft60.dropna(subset=['p'])
# Perform p-value correction
p_values = gft60['p'].values
adjusted_p_values = multipletests(p_values, method='fdr_bh')[1]
# Add the adjusted p-values to the DataFrame
gft60['adjusted_p'] = adjusted_p_values
gft60 = gft60[(~gft60['Target'].isna()) & (gft60['EntrezGeneSymbol'].isin(idx))]
gft60 = gft60[gft60['i']!='seq.8475.15']
gft60['group']='GFR<60'
#gft60

HTN = pd.read_csv('/content/HTN_event_ki.csv')
col_r = ["i",'EntrezGeneSymbol', "Target", 'coef', 'exp(coef)', 'se(coef)', 'exp(coef) lower 95%', 'exp(coef) upper 95%', 'p', 'AUC']
HTN = HTN[col_r]
# Drop rows with NaN values in the 'p' column
HTN = HTN.dropna(subset=['p'])
# Perform p-value correction
p_values = HTN['p'].values
adjusted_p_values = multipletests(p_values, method='fdr_bh')[1]
# Add the adjusted p-values to the DataFrame
HTN['adjusted_p'] = adjusted_p_values
HTN = HTN[(~HTN['Target'].isna()) & (HTN['EntrezGeneSymbol'].isin(idx))]
HTN = HTN[HTN['i']!='seq.8475.15']
HTN['group']='HTN'
#HTN

DM = pd.read_csv('/content/DM_event_ki.csv')
col_r = ["i", 'EntrezGeneSymbol', "Target", 'coef', 'exp(coef)', 'se(coef)', 'exp(coef) lower 95%', 'exp(coef) upper 95%', 'p', 'AUC']
DM = DM[col_r]
# Drop rows with NaN values in the 'p' column
DM = DM.dropna(subset=['p'])
# Perform p-value correction
p_values = DM['p'].values
adjusted_p_values = multipletests(p_values, method='fdr_bh')[1]
# Add the adjusted p-values to the DataFrame
DM['adjusted_p'] = adjusted_p_values
DM = DM[(~DM['Target'].isna()) & (DM['EntrezGeneSymbol'].isin(idx))]
DM = DM[DM['i']!='seq.8475.15']
DM['group']='DM'

#DM

df = pd.concat([gft60, HTN, DM])
#df

df.shape

df

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming df is already defined in your environment and contains the relevant data

# Define colors
cm = ["#e41a1c", "#377eb8", "#4daf4a", "#984ea3", "#ff7f00", "#FFFF33", "#A65628", "#F781BF"]

# Sort the DataFrame by 'EntrezGeneSymbol'
df = df.sort_values(by='EntrezGeneSymbol')

# Create a new column that combines EntrezGeneSymbol and group
df['symbol_group'] = df['EntrezGeneSymbol'] + ' (' + df['group'] + ')'

# Create the forest plot
plt.figure(figsize=(10, 8))
sns.set(style="whitegrid")

# Plot the data
ax = sns.pointplot(x="exp(coef)", y="symbol_group", hue="group", data=df, join=False, palette=cm)

# Add error bars
for index, row in df.iterrows():
    group_color = cm[df['group'].unique().tolist().index(row['group'])]
    plt.plot([row['exp(coef) lower 95%'], row['exp(coef) upper 95%']], [row['symbol_group'], row['symbol_group']], color=group_color, lw=1)

# Customize the plot
plt.xscale('linear')
plt.axvline(x=1, color='black', linestyle='--')
plt.xlabel('Hazard Ratio')
plt.ylabel('')
plt.xlim(0, 8)
plt.title('Forest Plot of Hazard Ratios')

# Remove the grid
sns.despine(left=True, bottom=True)
ax.grid(False)

# Add a black frame around the plot
for spine in ax.spines.values():
    spine.set_visible(True)
    spine.set_edgecolor('black')
    spine.set_linewidth(1.5)

# Customize the legend
legend = ax.legend(frameon=False)

# Save the plot
plt.savefig("hr_death.pdf", bbox_inches='tight', dpi=450)
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming df is already defined in your environment and contains the relevant data

# Define colors
cm = ["#e41a1c", "#377eb8", "#4daf4a", "#984ea3", "#ff7f00", "#FFFF33", "#A65628", "#F781BF"]

# Sort the DataFrame by 'EntrezGeneSymbol'
df = df.sort_values(by='EntrezGeneSymbol')

# Create a new column that combines EntrezGeneSymbol and group
df['symbol_group'] = df['EntrezGeneSymbol'] + ' (' + df['group'] + ')'

# Insert blank rows after every three rows, except the last three rows
num_rows = len(df)
spacing_indices = [i for i in range(3, num_rows, 3) if i < num_rows - 3]
for index in sorted(spacing_indices, reverse=True):
    blank_row = pd.DataFrame({col: [None] for col in df.columns})
    df = pd.concat([df.iloc[:index], blank_row, df.iloc[index:]]).reset_index(drop=True)

# Create the forest plot
plt.figure(figsize=(10, 8))
sns.set(style="whitegrid")

# Plot the data
ax = sns.pointplot(x="exp(coef)", y="symbol_group", hue="group", data=df, join=False, palette=cm)

# Add error bars
for index, row in df.iterrows():
    if pd.notna(row['symbol_group']):
        group_color = cm[df['group'].unique().tolist().index(row['group'])]
        plt.plot([row['exp(coef) lower 95%'], row['exp(coef) upper 95%']], [row['symbol_group'], row['symbol_group']], color=group_color, lw=2)

# Customize the plot
plt.xscale('linear')
plt.axvline(x=1, color='black', linestyle='--')
plt.xlabel('Hazard Ratio')
plt.ylabel('')
plt.xlim(0, 8)
plt.title('Forest Plot of Hazard Ratios Kidney Event')

# Remove the grid
sns.despine(left=True, bottom=True)
ax.grid(False)

# Add a black frame around the plot
for spine in ax.spines.values():
    spine.set_visible(True)
    spine.set_edgecolor('black')
    spine.set_linewidth(1.5)

# Customize the legend
legend = ax.legend(frameon=False)

# Save the plot
#plt.savefig("hr_death.pdf", bbox_inches='tight', dpi=450)
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming df is already defined in your environment and contains the relevant data

# Define colors
cm = ["#e41a1c", "#377eb8", "#4daf4a", "#984ea3", "#ff7f00", "#FFFF33", "#A65628", "#F781BF"]

# Sort the DataFrame by 'EntrezGeneSymbol'
df = df.sort_values(by='EntrezGeneSymbol')
# Insert blank rows after every three rows, except the last three rows
num_rows = len(df)
spacing_indices = [i for i in range(3, num_rows, 3) if i < num_rows]
for index in sorted(spacing_indices, reverse=True):
    blank_row = pd.DataFrame({'EntrezGeneSymbol': [f'{index}'], 'group': [f'{index}']})
    df = pd.concat([df.iloc[:index], blank_row, df.iloc[index:]]).reset_index(drop=True)

# Create a new column that combines EntrezGeneSymbol and group
df['symbol_group'] = df['EntrezGeneSymbol'] + ' (' + df['group'] + ')'

# Create the forest plot
plt.figure(figsize=(6, 10))
sns.set(style="whitegrid")

# Plot the data
ax = sns.pointplot(x="exp(coef)", y="symbol_group", hue="group", data=df, join=False, palette=cm)

# Add error bars
for index, row in df.iterrows():
  if pd.notna(row['Target']):
        group_color = cm[df['group'].unique().tolist().index(row['group'])]
        plt.plot([row['exp(coef) lower 95%'], row['exp(coef) upper 95%']], [row['symbol_group'], row['symbol_group']], color=group_color, lw=1.5)


# Customize the plot
plt.xscale('linear')
plt.axvline(x=1, color='black', linestyle='--')
plt.xlabel('Hazard Ratio')
plt.ylabel('')
plt.xlim(0, 8)
plt.title('Forest Plot of Hazard Ratios Composite')

# Remove the grid
sns.despine(left=True, bottom=True)
ax.grid(False)

# Add a black frame around the plot
for spine in ax.spines.values():
    spine.set_visible(True)
    spine.set_edgecolor('black')
    spine.set_linewidth(1.5)

# Customize the legend
legend = ax.legend(frameon=False)

# Save the plot
plt.savefig("hr_ki_event.png", bbox_inches='tight', dpi=450)
plt.show()



"""now the death"""

from statsmodels.stats.multitest import multipletests
idx = ['MMP7', 'WFDC2', 'HAVCR1', 'NEFL','LAYN', 'EFEMP1', 'PTK7', 'ROR2', 'TNFRSF19']

gft60 = pd.read_csv('/content/gft60_death.csv')
col_r = ["i",'EntrezGeneSymbol', "Target", 'coef', 'exp(coef)', 'se(coef)', 'exp(coef) lower 95%', 'exp(coef) upper 95%', 'p', 'AUC']
gft60 = gft60[col_r]
# Drop rows with NaN values in the 'p' column
gft60 = gft60.dropna(subset=['p'])
# Perform p-value correction
p_values = gft60['p'].values
adjusted_p_values = multipletests(p_values, method='fdr_bh')[1]
# Add the adjusted p-values to the DataFrame
gft60['adjusted_p'] = adjusted_p_values
gft60 = gft60[(~gft60['Target'].isna()) & (gft60['EntrezGeneSymbol'].isin(idx))]
gft60 = gft60[gft60['i']!='seq.8475.15']
gft60['group']='GFR<60'
#gft60

HTN = pd.read_csv('/content/HTN_death.csv')
col_r = ["i",'EntrezGeneSymbol', "Target", 'coef', 'exp(coef)', 'se(coef)', 'exp(coef) lower 95%', 'exp(coef) upper 95%', 'p', 'AUC']
HTN = HTN[col_r]
# Drop rows with NaN values in the 'p' column
HTN = HTN.dropna(subset=['p'])
# Perform p-value correction
p_values = HTN['p'].values
adjusted_p_values = multipletests(p_values, method='fdr_bh')[1]
# Add the adjusted p-values to the DataFrame
HTN['adjusted_p'] = adjusted_p_values
HTN = HTN[(~HTN['Target'].isna()) & (HTN['EntrezGeneSymbol'].isin(idx))]
HTN = HTN[HTN['i']!='seq.8475.15']
HTN['group']='HTN'
#HTN

DM = pd.read_csv('/content/DM_death.csv')
col_r = ["i", 'EntrezGeneSymbol', "Target", 'coef', 'exp(coef)', 'se(coef)', 'exp(coef) lower 95%', 'exp(coef) upper 95%', 'p', 'AUC']
DM = DM[col_r]
# Drop rows with NaN values in the 'p' column
DM = DM.dropna(subset=['p'])
# Perform p-value correction
p_values = DM['p'].values
adjusted_p_values = multipletests(p_values, method='fdr_bh')[1]
# Add the adjusted p-values to the DataFrame
DM['adjusted_p'] = adjusted_p_values
DM = DM[(~DM['Target'].isna()) & (DM['EntrezGeneSymbol'].isin(idx))]
DM = DM[DM['i']!='seq.8475.15']
DM['group']='DM'

#DM

gft60

df = pd.concat([gft60, HTN, DM])
#df

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming df is already defined in your environment and contains the relevant data

# Define colors
cm = ["#e41a1c", "#377eb8", "#4daf4a", "#984ea3", "#ff7f00", "#FFFF33", "#A65628", "#F781BF"]

# Sort the DataFrame by 'EntrezGeneSymbol'
df = df.sort_values(by='EntrezGeneSymbol')

# Create a new column that combines EntrezGeneSymbol and group
df['symbol_group'] = df['EntrezGeneSymbol'] + ' (' + df['group'] + ')'

# Insert blank rows after every three rows, except the last three rows
num_rows = len(df)
spacing_indices = [i for i in range(3, num_rows, 3) if i < num_rows - 3]
for index in sorted(spacing_indices, reverse=True):
    blank_row = pd.DataFrame({'symbol_group': [''], 'group': ['']})
    df = pd.concat([df.iloc[:index], blank_row, df.iloc[index:]]).reset_index(drop=True)

# Create the forest plot
plt.figure(figsize=(10, 8))
sns.set(style="whitegrid")

# Plot the data
ax = sns.pointplot(x="exp(coef)", y="symbol_group", hue="group", data=df, join=False, palette=cm)

# Add error bars
for index, row in df.iterrows():
    if row['symbol_group'] != '':
        group_color = cm[df['group'].unique().tolist().index(row['group'])]
        plt.plot([row['exp(coef) lower 95%'], row['exp(coef) upper 95%']], [row['symbol_group'], row['symbol_group']], color=group_color, lw=1.5)

# Customize the plot
plt.xscale('linear')
plt.axvline(x=1, color='black', linestyle='--')
plt.xlabel('Hazard Ratio')
plt.ylabel('')
plt.xlim(0, 8)
plt.title('Forest Plot of Hazard Ratios Death')

# Remove the grid
sns.despine(left=True, bottom=True)
ax.grid(False)

# Add a black frame around the plot
for spine in ax.spines.values():
    spine.set_visible(True)
    spine.set_edgecolor('black')
    spine.set_linewidth(1.5)

# Customize the legend
legend = ax.legend(frameon=False)

# Save the plot
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming df is already defined in your environment and contains the relevant data

# Define colors
cm = ["#e41a1c", "#377eb8", "#4daf4a", "#984ea3", "#ff7f00", "#FFFF33", "#A65628", "#F781BF"]

# Sort the DataFrame by 'EntrezGeneSymbol'
df = df.sort_values(by='EntrezGeneSymbol')
# Insert blank rows after every three rows, except the last three rows
num_rows = len(df)
spacing_indices = [i for i in range(3, num_rows, 3) if i < num_rows]
for index in sorted(spacing_indices, reverse=True):
    blank_row = pd.DataFrame({'EntrezGeneSymbol': [f'{index}'], 'group': [f'{index}']})
    df = pd.concat([df.iloc[:index], blank_row, df.iloc[index:]]).reset_index(drop=True)

# Create a new column that combines EntrezGeneSymbol and group
df['symbol_group'] = df['EntrezGeneSymbol'] + ' (' + df['group'] + ')'

# Create the forest plot
plt.figure(figsize=(6, 10))
sns.set(style="whitegrid")

# Plot the data
ax = sns.pointplot(x="exp(coef)", y="symbol_group", hue="group", data=df, join=False, palette=cm)

# Add error bars
for index, row in df.iterrows():
  if pd.notna(row['Target']):
        group_color = cm[df['group'].unique().tolist().index(row['group'])]
        plt.plot([row['exp(coef) lower 95%'], row['exp(coef) upper 95%']], [row['symbol_group'], row['symbol_group']], color=group_color, lw=1.5)


# Customize the plot
plt.xscale('linear')
plt.axvline(x=1, color='black', linestyle='--')
plt.xlabel('Hazard Ratio')
plt.ylabel('')
plt.xlim(0, 8)
plt.title('Forest Plot of Hazard Ratios Death')

# Remove the grid
sns.despine(left=True, bottom=True)
ax.grid(False)

# Add a black frame around the plot
for spine in ax.spines.values():
    spine.set_visible(True)
    spine.set_edgecolor('black')
    spine.set_linewidth(1.5)

# Customize the legend
legend = ax.legend(frameon=False, prop={'size': 6})

# Save the plot
plt.savefig("hr_death.png", bbox_inches='tight', dpi=450)
plt.show()





"""now composite"""

from statsmodels.stats.multitest import multipletests
idx = ['MMP7', 'WFDC2', 'HAVCR1', 'NEFL','LAYN', 'EFEMP1',  'PTK7', 'ROR2', 'TNFRSF19']

gft60 = pd.read_csv('/content/gft60_event.csv')
col_r = ["i",'EntrezGeneSymbol', "Target", 'coef', 'exp(coef)', 'se(coef)', 'exp(coef) lower 95%', 'exp(coef) upper 95%', 'p', 'AUC']
gft60 = gft60[col_r]
# Drop rows with NaN values in the 'p' column
gft60 = gft60.dropna(subset=['p'])
# Perform p-value correction
p_values = gft60['p'].values
adjusted_p_values = multipletests(p_values, method='fdr_bh')[1]
# Add the adjusted p-values to the DataFrame
gft60['adjusted_p'] = adjusted_p_values
gft60 = gft60[(~gft60['Target'].isna()) & (gft60['EntrezGeneSymbol'].isin(idx))]
gft60 = gft60[gft60['i']!='seq.8475.15']
gft60['group']='GFR<60'
#gft60

HTN = pd.read_csv('/content/HTN_event.csv')
col_r = ["i",'EntrezGeneSymbol', "Target", 'coef', 'exp(coef)', 'se(coef)', 'exp(coef) lower 95%', 'exp(coef) upper 95%', 'p', 'AUC']
HTN = HTN[col_r]
# Drop rows with NaN values in the 'p' column
HTN = HTN.dropna(subset=['p'])
# Perform p-value correction
p_values = HTN['p'].values
adjusted_p_values = multipletests(p_values, method='fdr_bh')[1]
# Add the adjusted p-values to the DataFrame
HTN['adjusted_p'] = adjusted_p_values
HTN = HTN[(~HTN['Target'].isna()) & (HTN['EntrezGeneSymbol'].isin(idx))]
HTN = HTN[HTN['i']!='seq.8475.15']
HTN['group']='HTN'
#HTN

DM = pd.read_csv('/content/DM_event.csv')
col_r = ["i", 'EntrezGeneSymbol', "Target", 'coef', 'exp(coef)', 'se(coef)', 'exp(coef) lower 95%', 'exp(coef) upper 95%', 'p', 'AUC']
DM = DM[col_r]
# Drop rows with NaN values in the 'p' column
DM = DM.dropna(subset=['p'])
# Perform p-value correction
p_values = DM['p'].values
adjusted_p_values = multipletests(p_values, method='fdr_bh')[1]
# Add the adjusted p-values to the DataFrame
DM['adjusted_p'] = adjusted_p_values
DM = DM[(~DM['Target'].isna()) & (DM['EntrezGeneSymbol'].isin(idx))]
DM = DM[DM['i']!='seq.8475.15']
DM['group']='DM'

#DM

gft60

df = pd.concat([gft60, HTN, DM])
#df

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming df is already defined in your environment and contains the relevant data

# Define colors
cm = ["#e41a1c", "#377eb8", "#4daf4a", "#984ea3", "#ff7f00", "#FFFF33", "#A65628", "#F781BF"]

# Sort the DataFrame by 'EntrezGeneSymbol'
df = df.sort_values(by='EntrezGeneSymbol')

# Create a new column that combines EntrezGeneSymbol and group
df['symbol_group'] = df['EntrezGeneSymbol'] + ' (' + df['group'] + ')'

# Insert blank rows after every three rows, except the last three rows
num_rows = len(df)
spacing_indices = [i for i in range(3, num_rows, 3) if i < num_rows - 3]
for index in sorted(spacing_indices, reverse=True):
    blank_row = pd.DataFrame({'symbol_group': [''], 'group': ['']})
    df = pd.concat([df.iloc[:index], blank_row, df.iloc[index:]]).reset_index(drop=True)

# Create the forest plot
plt.figure(figsize=(6, 10))
sns.set(style="whitegrid")

# Plot the data
ax = sns.pointplot(x="exp(coef)", y="symbol_group", hue="group", data=df, join=False, palette=cm)

# Add error bars
for index, row in df.iterrows():
    if row['symbol_group'] != '':
        group_color = cm[df['group'].unique().tolist().index(row['group'])]
        plt.plot([row['exp(coef) lower 95%'], row['exp(coef) upper 95%']], [row['symbol_group'], row['symbol_group']], color=group_color, lw=1.5)

# Customize the plot
plt.xscale('linear')
plt.axvline(x=1, color='black', linestyle='--')
plt.xlabel('Hazard Ratio')
plt.ylabel('')
plt.xlim(0, 8)
plt.title('Forest Plot of Hazard Ratios Composite')

# Remove the grid
sns.despine(left=True, bottom=True)
ax.grid(False)

# Add a black frame around the plot
for spine in ax.spines.values():
    spine.set_visible(True)
    spine.set_edgecolor('black')
    spine.set_linewidth(1.5)

# Customize the legend
legend = ax.legend(frameon=False)

# Save the plot
plt.savefig("hr_event.pdf", bbox_inches='tight', dpi=450)
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming df is already defined in your environment and contains the relevant data

# Define colors
cm = ["#e41a1c", "#377eb8", "#4daf4a", "#984ea3", "#ff7f00", "#FFFF33", "#A65628", "#F781BF"]

# Sort the DataFrame by 'EntrezGeneSymbol'
df = df.sort_values(by='EntrezGeneSymbol')
# Insert blank rows after every three rows, except the last three rows
num_rows = len(df)
spacing_indices = [i for i in range(3, num_rows, 3) if i < num_rows]
for index in sorted(spacing_indices, reverse=True):
    blank_row = pd.DataFrame({'EntrezGeneSymbol': [f'{index}'], 'group': [f'{index}']})
    df = pd.concat([df.iloc[:index], blank_row, df.iloc[index:]]).reset_index(drop=True)

# Create a new column that combines EntrezGeneSymbol and group
df['symbol_group'] = df['EntrezGeneSymbol'] + ' (' + df['group'] + ')'

# Create the forest plot
plt.figure(figsize=(6, 10))
sns.set(style="whitegrid")

# Plot the data
ax = sns.pointplot(x="exp(coef)", y="symbol_group", hue="group", data=df, join=False, palette=cm)

# Add error bars
for index, row in df.iterrows():
  if pd.notna(row['Target']):
        group_color = cm[df['group'].unique().tolist().index(row['group'])]
        plt.plot([row['exp(coef) lower 95%'], row['exp(coef) upper 95%']], [row['symbol_group'], row['symbol_group']], color=group_color, lw=1.5)


# Customize the plot
plt.xscale('linear')
plt.axvline(x=1, color='black', linestyle='--')
plt.xlabel('Hazard Ratio')
plt.ylabel('')
plt.xlim(0, 8)
plt.title('Forest Plot of Hazard Ratios Composite')

# Remove the grid
sns.despine(left=True, bottom=True)
ax.grid(False)

# Add a black frame around the plot
for spine in ax.spines.values():
    spine.set_visible(True)
    spine.set_edgecolor('black')
    spine.set_linewidth(1.5)

# Customize the legend
legend = ax.legend(frameon=False)

# Save the plot
plt.savefig("hr_composite.png", bbox_inches='tight', dpi=450)
plt.show()

df=df.dropna(axis=0)
df=df.drop('symbol_group', axis=1)

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming df is already defined in your environment and contains the relevant data

# Define colors
cm = ["#e41a1c", "#377eb8", "#4daf4a", "#984ea3", "#ff7f00", "#FFFF33", "#A65628", "#F781BF"]

# Sort the DataFrame by 'EntrezGeneSymbol'
df = df.sort_values(by='EntrezGeneSymbol')
# Insert blank rows after every three rows, except the last three rows
num_rows = len(df)
spacing_indices = [i for i in range(3, num_rows, 3) if i < num_rows]
for index in sorted(spacing_indices, reverse=True):
    blank_row = pd.DataFrame({'EntrezGeneSymbol': [f'{index}'], 'group': [f'{index}']})
    df = pd.concat([df.iloc[:index], blank_row, df.iloc[index:]]).reset_index(drop=True)

# Create a new column that combines EntrezGeneSymbol and group
df['symbol_group'] = df['EntrezGeneSymbol'] + ' (' + df['group'] + ')'

# Create the forest plot
plt.figure(figsize=(6, 10))
sns.set(style="whitegrid")

# Plot the data
ax = sns.pointplot(x="exp(coef)", y="symbol_group", hue="group", data=df, join=False, palette=cm)

# Add error bars
for index, row in df.iterrows():
  if pd.notna(row['Target']):
        group_color = cm[df['group'].unique().tolist().index(row['group'])]
        plt.plot([row['exp(coef) lower 95%'], row['exp(coef) upper 95%']], [row['symbol_group'], row['symbol_group']], color=group_color, lw=1.5)


# Customize the plot
plt.xscale('linear')
plt.axvline(x=1, color='black', linestyle='--')
plt.xlabel('Hazard Ratio')
plt.ylabel('')
plt.xlim(0, 8)
plt.title('Forest Plot of Hazard Ratios Composite')

# Remove the grid
sns.despine(left=True, bottom=True)
ax.grid(False)

# Add a black frame around the plot
for spine in ax.spines.values():
    spine.set_visible(True)
    spine.set_edgecolor('black')
    spine.set_linewidth(1.5)

# Customize the legend
legend = ax.legend(frameon=False)

# Save the plot
plt.savefig("hr_composite.png", bbox_inches='tight', dpi=450)
plt.show()

df

spacing_indices

df









import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the data


#gft60 = gft60[(~gft60['Target'].isna()) & (gft60['i'].isin(idx))]
#gft60['coef'] = -gft60['coef']
#gft60 = gft60.sort_values(by='coef', ascending=False)


#idx = HA['i'].tolist()[:12]

#HA = pd.read_csv("res/coef_4/HR_death.csv.gz")[col_r]
#HA["apol1"] = 'HA'

#LA = pd.read_csv("res/coef_4/LA_death.csv.gz")[col_r]
#LA["apol1"] = 'LA'

#LE = pd.read_csv("res/coef_4/LE_death.csv.gz")[col_r]
#LE["apol1"] = 'LE'

#df = pd.concat([HA, LA, LE])
#df = df[(~df['Target'].isna()) & (df['i'].isin(idx))]
#df['coef'] = -df['coef']
#df = df.sort_values(by='coef', ascending=False)

gft60.columns = ["i_seq", "i", 'b', 'hr', 'se', 'low95', 'up95', 'p', 'AUC', "apol1"]

# Define colors
cm = ["#e41a1c", "#377eb8", "#4daf4a", "#984ea3", "#ff7f00", "#FFFF33", "#A65628", "#F781BF"]

# Create the forest plot
plt.figure(figsize=(10, 8))
sns.set(style="whitegrid")

# Plot the data
ax = sns.pointplot(x="hr", y="i", hue="apol1", data=df, dodge=0.5, join=False, palette=cm)

# Add error bars
for index, row in df.iterrows():
    plt.plot([row['low95'], row['up95']], [row['i'], row['i']], color='black', lw=1)

# Customize the plot
plt.xscale('log')
plt.axvline(x=1, color='black', linestyle='--')
plt.xlabel('Hazard Ratio (log scale)')
plt.ylabel('Features')
plt.title('Forest Plot of Hazard Ratios')

# Save the plot
plt.savefig("hr_death.pdf", bbox_inches='tight', dpi=450)
plt.show()

gft60





"""lets plot the combined genes"""

atlas=sc.read_h5ad('/content/drive/MyDrive/Atlas_human_extension_II_shared.h5ad')

combined_df_significant_sorted = combined_df_significant.sort_values(by='combined', ascending=False)

# Extract the top 25 genes based on the sorted DataFrame
top_25_genes = combined_df_significant_sorted.head(25).index.tolist()
missing_genes = [gene for gene in top_25_genes if gene not in atlas.var_names]
print(f"Missing genes: {missing_genes}")

# Filter out the missing genes
top_25_genes_filtered = [gene for gene in top_25_genes if gene in atlas.var_names]

# Display the top 25 genes
print(top_25_genes_filtered)

sc.pl.dotplot(atlas, top_25_genes_filtered, groupby='C_scANVI', cmap='Blues', log = False)





v_library='MSigDB_Hallmark_2020' ## 'WikiPathway_2023_Human' ###
gset = gseapy.parser.get_library(v_library, min_size=20)

gene_rank = tissue_degs[['names', 'pi_score']]
gene_rank.sort_values(by=['pi_score'], inplace=True, ascending=False)
gene_rank = gene_rank.reset_index(drop=True)
background_genes = gene_rank['names'].tolist()
res = gseapy.prerank(rnk=gene_rank, gene_sets=gset)
res.res2d_plot = res.res2d[res.res2d["FWER p-val"] < 0.05]
b_high=res.res2d.copy()

b_high

iTAL_high











cluster_trident=cluster_trident.set_index('Unnamed: 0')

cluster_trident.index = cluster_trident.index.astype(int)

columns=['iTAL_cluster', 'b_cluster']
cluster_trident=cluster_trident[columns]
cluster_trident.index.name=None
cluster_trident











import pandas as pd
from scipy.stats import pearsonr

# Assuming 'cleaned_df' is your DataFrame

correlate_with = 'immune_Immune_ME_3_score'  # Specify the column name for correlation

# Initialize an empty dictionary to store the correlation coefficients and p-values
correlation_dict = {}

# Calculate the Pearson correlation coefficient for each gene
for gene in cleaned_df.columns:
    if gene in ['immune_Immune_ME_3_score', 'immune_Immune_ME_4_score', 'immune_Immune_ME_3', 'immune_Immune_ME_4']:
        continue
    correlation_coef, p_value = pearsonr(cleaned_df[gene], cleaned_df[correlate_with])
    correlation_dict[gene] = (correlation_coef, p_value)

# Create a DataFrame from the correlation dictionary
correlation_df = pd.DataFrame.from_dict(correlation_dict, orient='index', columns=['Correlation Coefficient', 'P-value'])

# Sort the DataFrame by the correlation coefficient in descending order
sorted_correlation_df = correlation_df.sort_values(by='Correlation Coefficient', ascending=False)

# Display the sorted DataFrame
print(sorted_correlation_df)

sorted_correlation_df

wilcoxon_df_sorted = wilcoxon_df.sort_values(by='Adjusted p-value')

# Extract the top 25 genes based on the sorted DataFrame
top_25_genes = wilcoxon_df_sorted.head(28).index.tolist()
missing_genes = [gene for gene in top_25_genes if gene not in atlas.var_names]
print(f"Missing genes: {missing_genes}")

# Filter out the missing genes
top_25_genes_filtered = [gene for gene in top_25_genes if gene in atlas.var_names]

# Display the top 25 genes
print(top_25_genes_filtered)

wilcoxon_df_sorted

atlas=sc.read_h5ad('/content/drive/MyDrive/Atlas_human_extension_II_shared.h5ad')

atlas

sc.pl.dotplot(atlas, top_25_genes_filtered, groupby='C_scANVI', cmap='Blues', log = False)

##Forest plot with upper and lower range







cluster_labels_df.to_csv('10_spatialgenes_clustering_trident_niches_glom.csv', index=True)

!pip install --quiet plotly

cluster_labels_df.columns

import pandas as pd
import plotly.graph_objects as go

# Assuming cluster_labels_df is your DataFrame
#10 genes
# Define the columns of interest in sequential order
columns_of_interest = ['niches_CD_Niche', 'niches_CNT_Niche', 'niches_Fibroblast_Niche', 'niches_Glomerular_Niche', 'niches_Immune_Niche',
                       'niches_PT_Niche','niches_TAL_Niche', 'niches_Vascular_Niche','niches_iPT_Niche','niches_iTAL_Niche']

# Prepare data for the Sankey diagram
flows = []
labels = []
for i in range(len(columns_of_interest) - 1):
    src_col = columns_of_interest[i]
    tgt_col = columns_of_interest[i + 1]

    flow_counts = cluster_labels_df.groupby([src_col, tgt_col]).size().reset_index(name='count')

    for _, row in flow_counts.iterrows():
        src_label = f"{src_col}_{row[src_col]}"
        tgt_label = f"{tgt_col}_{row[tgt_col]}"

        if src_label not in labels:
            labels.append(src_label)
        if tgt_label not in labels:
            labels.append(tgt_label)

        flows.append({
            'source': labels.index(src_label),
            'target': labels.index(tgt_label),
            'value': row['count']
        })

# Create the Sankey plot
fig = go.Figure(go.Sankey(
    node=dict(
        pad=15,
        thickness=20,
        line=dict(color="black", width=0.5),
        label=labels
    ),
    link=dict(
        source=[flow['source'] for flow in flows],
        target=[flow['target'] for flow in flows],
        value=[flow['value'] for flow in flows]
    )
))

fig.update_layout(title_text="Flow from Subclusters to Status", font_size=10)
fig.show()

import pandas as pd
import plotly.graph_objects as go

# Assuming cluster_labels_df is your DataFrame
#5 genes
# Define the columns of interest in sequential order
columns_of_interest = ['niches_CD_Niche', 'niches_CNT_Niche', 'niches_Fibroblast_Niche', 'niches_Glomerular_Niche', 'niches_Immune_Niche',
                       'niches_PT_Niche','niches_TAL_Niche', 'niches_Vascular_Niche','niches_iPT_Niche','niches_iTAL_Niche']

# Prepare data for the Sankey diagram
flows = []
labels = []
for i in range(len(columns_of_interest) - 1):
    src_col = columns_of_interest[i]
    tgt_col = columns_of_interest[i + 1]

    flow_counts = cluster_labels_df.groupby([src_col, tgt_col]).size().reset_index(name='count')

    for _, row in flow_counts.iterrows():
        src_label = f"{src_col}_{row[src_col]}"
        tgt_label = f"{tgt_col}_{row[tgt_col]}"

        if src_label not in labels:
            labels.append(src_label)
        if tgt_label not in labels:
            labels.append(tgt_label)

        flows.append({
            'source': labels.index(src_label),
            'target': labels.index(tgt_label),
            'value': row['count']
        })

# Create the Sankey plot
fig = go.Figure(go.Sankey(
    node=dict(
        pad=15,
        thickness=20,
        line=dict(color="black", width=0.5),
        label=labels
    ),
    link=dict(
        source=[flow['source'] for flow in flows],
        target=[flow['target'] for flow in flows],
        value=[flow['value'] for flow in flows]
    )
))

fig.update_layout(title_text="Flow from Subclusters to Status", font_size=10)
fig.show()

import pandas as pd
import plotly.graph_objects as go

# Assuming cluster_labels_df is your DataFrame
#15 genes
# Define the columns of interest in sequential order
columns_of_interest = ['niches_CD_Niche', 'niches_CNT_Niche', 'niches_Fibroblast_Niche', 'niches_Glomerular_Niche', 'niches_Immune_Niche',
                       'niches_PT_Niche','niches_TAL_Niche', 'niches_Vascular_Niche','niches_iPT_Niche','niches_iTAL_Niche']

# Prepare data for the Sankey diagram
flows = []
labels = []
for i in range(len(columns_of_interest) - 1):
    src_col = columns_of_interest[i]
    tgt_col = columns_of_interest[i + 1]

    flow_counts = cluster_labels_df.groupby([src_col, tgt_col]).size().reset_index(name='count')

    for _, row in flow_counts.iterrows():
        src_label = f"{src_col}_{row[src_col]}"
        tgt_label = f"{tgt_col}_{row[tgt_col]}"

        if src_label not in labels:
            labels.append(src_label)
        if tgt_label not in labels:
            labels.append(tgt_label)

        flows.append({
            'source': labels.index(src_label),
            'target': labels.index(tgt_label),
            'value': row['count']
        })

# Create the Sankey plot
fig = go.Figure(go.Sankey(
    node=dict(
        pad=15,
        thickness=20,
        line=dict(color="black", width=0.5),
        label=labels
    ),
    link=dict(
        source=[flow['source'] for flow in flows],
        target=[flow['target'] for flow in flows],
        value=[flow['value'] for flow in flows]
    )
))

fig.update_layout(title_text="Flow from Subclusters to Status", font_size=10)
fig.show()

import pandas as pd
import plotly.graph_objects as go

# Assuming cluster_labels_df is your DataFrame
#15 genes
# Define the columns of interest in sequential order
columns_of_interest = ['niches_CD_Niche', 'niches_CNT_Niche', 'niches_Fibroblast_Niche', 'niches_Glomerular_Niche', 'niches_Immune_Niche',
                       'niches_PT_Niche','niches_TAL_Niche', 'niches_Vascular_Niche','niches_iPT_Niche','niches_iTAL_Niche']

# Prepare data for the Sankey diagram
flows = []
labels = []
for i in range(len(columns_of_interest) - 1):
    src_col = columns_of_interest[i]
    tgt_col = columns_of_interest[i + 1]

    flow_counts = cluster_labels_df.groupby([src_col, tgt_col]).size().reset_index(name='count')

    for _, row in flow_counts.iterrows():
        src_label = f"{src_col}_{row[src_col]}"
        tgt_label = f"{tgt_col}_{row[tgt_col]}"

        if src_label not in labels:
            labels.append(src_label)
        if tgt_label not in labels:
            labels.append(tgt_label)

        flows.append({
            'source': labels.index(src_label),
            'target': labels.index(tgt_label),
            'value': row['count']
        })

# Create the Sankey plot
fig = go.Figure(go.Sankey(
    node=dict(
        pad=15,
        thickness=20,
        line=dict(color="black", width=0.5),
        label=labels
    ),
    link=dict(
        source=[flow['source'] for flow in flows],
        target=[flow['target'] for flow in flows],
        value=[flow['value'] for flow in flows]
    )
))

fig.update_layout(title_text="Flow from Subclusters to Status", font_size=10)
fig.show()

import pandas as pd
import plotly.graph_objects as go

# Assuming cluster_labels_df is your DataFrame
#5 genes
# Define the columns of interest in sequential order
columns_of_interest = ['niches_CD_Niche', 'niches_CNT_Niche', 'niches_Fibroblast_Niche', 'niches_Glomerular_Niche', 'niches_Immune_Niche',
                       'niches_PT_Niche','niches_TAL_Niche', 'niches_Vascular_Niche','niches_iPT_Niche','niches_iTAL_Niche']

# Prepare data for the Sankey diagram
flows = []
labels = []
for i in range(len(columns_of_interest) - 1):
    src_col = columns_of_interest[i]
    tgt_col = columns_of_interest[i + 1]

    flow_counts = cluster_labels_df.groupby([src_col, tgt_col]).size().reset_index(name='count')

    for _, row in flow_counts.iterrows():
        src_label = f"{src_col}_{row[src_col]}"
        tgt_label = f"{tgt_col}_{row[tgt_col]}"

        if src_label not in labels:
            labels.append(src_label)
        if tgt_label not in labels:
            labels.append(tgt_label)

        flows.append({
            'source': labels.index(src_label),
            'target': labels.index(tgt_label),
            'value': row['count']
        })

# Create the Sankey plot
fig = go.Figure(go.Sankey(
    node=dict(
        pad=15,
        thickness=20,
        line=dict(color="black", width=0.5),
        label=labels
    ),
    link=dict(
        source=[flow['source'] for flow in flows],
        target=[flow['target'] for flow in flows],
        value=[flow['value'] for flow in flows]
    )
))

fig.update_layout(title_text="Flow from Subclusters to Status", font_size=10)
fig.show()

# Select columns that have '_score' at the end
columns_to_keep = [col for col in cluster_labels_df.columns if col.endswith('_score')]

# Keep only the selected columns
cluster_labels_df = cluster_labels_df[columns_to_keep]

# Display the resulting dataframe to verify
print(cluster_labels_df.shape)

# Replace 'Fibroblast_Fibroblast' with 'Fibro' in the column names
cluster_labels_df.columns = cluster_labels_df.columns.str.replace('_score', '')
cluster_labels_df.columns = cluster_labels_df.columns.str.replace('niches_', '')
# Display the updated dataframe to verify the changes
#print(cluster_labels_df)

# Compute the Pearson correlation matrix
correlation_matrix = cluster_labels_df.corr(method='pearson')

# Display the correlation matrix
#print(correlation_matrix)

# 10 genes
plt.figure(figsize=(10, 8))
ax = sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', vmin=0)
ax.xaxis.tick_top()  # Move x-axis labels to the top
plt.xticks(rotation=90)  # Rotate the x-axis labels for better readability
plt.title('')
plt.show()

"""now with Tubule RNA"""

expression_matrix = pd.read_csv('/content/drive/MyDrive/Bernhard/BulkSeq/HK.Biobank.Tubule.TPM_n900_230928.csv', sep= ';')

expression_matrix=expression_matrix.drop('Unnamed: 0', axis=1)

expression_matrix=expression_matrix.set_index('Symbol')

expression_matrix.index.name=None

expression_matrix=expression_matrix.T

expression_matrix.index = expression_matrix.index.str.rstrip('T')

expression_matrix

biobank=pd.read_csv('/content/Susztak_Lab_Biobank_122023_subset_scored.csv')

biobank=biobank.set_index('ID')

biobank=biobank.drop(['pos', 'Disease', 'GFR', 'DM', 'HTN', 'Age', 'Gender', 'Race',
       'Collection_Site', 'Height_cm', 'Weight_kg', 'BMI',
       'pct_glom_sclerosis', 'Tubules_Atrophy ',
       'Glomeruli_Total_num', 'Glomeruli_Globally Schlerotic_num',
       'Glomeruli_globally_Schlerotic_pct',
       'Glomeruli_Segmentally_Schlerotic_num', 'Glomeruli_Wall_Thickening_0_3',
       'Glomeruli_Hypoperfused_0_3', 'Glomeruli_Mesangial_Matrix_0_3',
       'Glomeruli_Mesangial_Cellularity_0_3', 'Glomeruli_KW_Nodules_0_1',
       'Glomeruli_Pericapsular_Fibrosis_0_2', 'Tubules_pct_Atrophy ',
       'Tubules_pct_Acute_Tubular_Injury', 'Tubules_Reabsorption_0_3',
       'Interstitium_Lymphocytic_Infiltrate_0_3',
       'Interstitium_Plasmacytic_Infiltrate_0_3',
       'Interstitium_Eosinophils_0_3', 'Vessels_Medial_Thickening_0_3',
       'Vessels_Intimal_Fibrosis_0_3', 'Vessels_Arteriolar_Hyalinosis_0_3'], axis=1)

biobank=biobank.astype(float)

import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans

# Assuming expression_matrix is your DataFrame containing bulk sequencing data

# Initialize an empty DataFrame to store the cluster labels
cluster_labels_df = pd.DataFrame(index=expression_matrix.index)

# Function to process and plot PCA for a given gene set
def process_and_plot_pca(genes_to_score, expression_matrix, plot_title, output_file):
    # Filter genes that are in the expression_matrix
    valid_genes = [gene for gene in genes_to_score if gene in expression_matrix.columns]
    if not valid_genes:
        print(f"No valid genes found for {plot_title}")
        return

    # Select the genes for PCA
    data = expression_matrix[valid_genes]

    # Center the data by subtracting the mean
    data_centered = data - data.mean()

    # Scale the data to the range (-1, 1)
    scaler = MinMaxScaler(feature_range=(-1, 1))
    data_scaled = scaler.fit_transform(data_centered)

    # Convert scaled data back to DataFrame for consistency
    data_scaled_df = pd.DataFrame(data_scaled, index=data.index, columns=data.columns)

    # Perform PCA
    pca = PCA(n_components=2)  # Reduce to 2 components for 2D visualization
    principal_components = pca.fit_transform(data_scaled_df)

    # Create a DataFrame with the principal components
    pca_df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2'], index=data.index)

    # Perform KMeans clustering
    k = 2  # Adjust based on your specific needs
    kmeans = KMeans(n_clusters=k, random_state=0)
    kmeans.fit(data_scaled_df)
    labels = kmeans.labels_

    # Add cluster labels to the PCA DataFrame
    pca_df['Cluster'] = labels

    # Add the cluster labels to the cluster_labels_df
    cluster_labels_df[plot_title] = labels
    cluster_labels_df[f'{plot_title}_score']=data_scaled_df.sum(axis=1)
    # Plot the PCA
    plt.figure(figsize=(6, 4))
    ax = sns.scatterplot(x='PC1', y='PC2', hue='Cluster', data=pca_df, palette='tab10', s=50, alpha=0.8)
    plt.title(plot_title)
    plt.xlabel('Principal Component 1')
    plt.ylabel('Principal Component 2')
    plt.legend(title='Cluster', frameon=False)
    plt.grid(False)
    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)
    plt.tight_layout()
    # Save the plot
    plt.savefig(output_file, dpi=300, bbox_inches='tight')
    plt.show()

    # Optional: Print explained variance to understand the importance of each component
    print(f"Explained variance ratio for {plot_title}:", pca.explained_variance_ratio_)

# Iterate through each key in the top_genes dictionary
for gene_set_name, genes_to_score in top_genes.items():
    plot_title = f'PCA of Gene Expression Data: {gene_set_name}'
    output_file = f'PCA_plot_{gene_set_name}.png'
    process_and_plot_pca(genes_to_score, expression_matrix, plot_title, output_file)

# Display the cluster labels DataFrame
print(cluster_labels_df)

# Strip 'PCA of Gene Expression Data: ' and '_DEGs' from column names
new_columns = [col.replace('PCA of Gene Expression Data: ', '').replace('_DEGs', '') for col in cluster_labels_df.columns]
cluster_labels_df.columns = new_columns

# Display the updated cluster labels DataFrame
print(cluster_labels_df)

for col in cluster_labels_df.columns:
    # Check if the column name ends with '_score'
    if col.endswith('_score'):
        # Calculate the mean of the score column
        mean_score = cluster_labels_df[col].mean()

        # Find the corresponding label column by removing '_score'
        label_col = col.replace('_score', '')

        # Replace 1 and 0 with 'high' and 'low' based on the mean score
        cluster_labels_df[label_col] = cluster_labels_df.apply(
            lambda row: 'high' if row[col] > mean_score else 'low', axis=1
        )

# Display the updated DataFrame
#print(cluster_labels_df)

# Select columns that have '_score' at the end
columns_to_keep = [col for col in cluster_labels_df.columns if col.endswith('_score')]

# Keep only the selected columns
cluster_labels_df = cluster_labels_df[columns_to_keep]

# Display the resulting dataframe to verify
print(cluster_labels_df.shape)

# Replace 'Fibroblast_Fibroblast' with 'Fibro' in the column names
cluster_labels_df.columns = cluster_labels_df.columns.str.replace('_score', '')
cluster_labels_df.columns = cluster_labels_df.columns.str.replace('niches_', '')
# Display the updated dataframe to verify the changes
#print(cluster_labels_df)

combined_df= pd.merge(biobank, cluster_labels_df,left_index=True, right_index=True, how='inner')
#combined_df

combined_df

# Compute the Pearson correlation matrix
correlation_matrix = combined_df.corr(method='pearson')

# Display the correlation matrix
#print(correlation_matrix)

# 10 genes
plt.figure(figsize=(10, 8))
ax = sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', vmin=0)
ax.xaxis.tick_top()  # Move x-axis labels to the top
plt.xticks(rotation=90)  # Rotate the x-axis labels for better readability
plt.title('')
plt.show()

combined_df.columns

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Assuming 'combined_biobank' is your DataFrame and contains all the mentioned genes

# List of genes to plot
correlation = ['CD_Niche', 'CNT_Niche', 'Fibroblast_Niche',
       'Glomerular_Niche', 'Immune_Niche', 'PT_Niche', 'TAL_Niche',
       'Vascular_Niche', 'iPT_Niche', 'iTAL_Niche']
correlate_with = 'Interstitium_Fibrosis'  # Specify the column name for correlation

# Create scatterplots with regression line and correlation coefficient
for gene in correlation:
    plt.figure(figsize=(4, 4))  # Set the figure size for better readability
    # Create a scatter plot with a linear regression line (regplot)
    ax = sns.regplot(x=gene, y=correlate_with, data=combined_df, scatter_kws={'alpha': 1, 'color': 'black', 's': 10}, line_kws={"color": "black"}, ci=None)

    # Calculate the Pearson correlation coefficient
    correlation_coef = combined_df[gene].corr(combined_df[correlate_with])

    # Title with correlation
    plt.title(f'{gene} vs IF - Corr: {correlation_coef:.2f}')

    # Axis labels
    plt.xlabel(f'{gene}_Score')
    plt.ylabel('Interstitial Fibrosis')
    plt.savefig(f'/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/spatial_signatures/niches/scatterplot_interstitial_fibrosis_tubules_{gene}.png', dpi=450, transparent=False)
    # Show the plot
    plt.show()

"""now with Cortex RNA"""

expression_matrix = pd.read_csv('/content/drive/MyDrive/Bernhard/BulkSeq/HK.Biobank.Cortex.TPM_n508_230928.csv', sep= ';')

expression_matrix=expression_matrix.drop('Unnamed: 0', axis=1)

expression_matrix=expression_matrix.set_index('Genes')

expression_matrix.index.name=None

expression_matrix=expression_matrix.T

expression_matrix.index = expression_matrix.index.str.rstrip('W')

expression_matrix

biobank=pd.read_csv('/content/Susztak_Lab_Biobank_122023_subset_scored.csv')

biobank=biobank.set_index('ID')

biobank=biobank.drop(['pos', 'Disease', 'GFR', 'DM', 'HTN', 'Age', 'Gender', 'Race',
       'Collection_Site', 'Height_cm', 'Weight_kg', 'BMI',
       'pct_glom_sclerosis', 'Tubules_Atrophy ',
       'Glomeruli_Total_num', 'Glomeruli_Globally Schlerotic_num',
       'Glomeruli_globally_Schlerotic_pct',
       'Glomeruli_Segmentally_Schlerotic_num', 'Glomeruli_Wall_Thickening_0_3',
       'Glomeruli_Hypoperfused_0_3', 'Glomeruli_Mesangial_Matrix_0_3',
       'Glomeruli_Mesangial_Cellularity_0_3', 'Glomeruli_KW_Nodules_0_1',
       'Glomeruli_Pericapsular_Fibrosis_0_2', 'Tubules_pct_Atrophy ',
       'Tubules_pct_Acute_Tubular_Injury', 'Tubules_Reabsorption_0_3',
       'Interstitium_Lymphocytic_Infiltrate_0_3',
       'Interstitium_Plasmacytic_Infiltrate_0_3',
       'Interstitium_Eosinophils_0_3', 'Vessels_Medial_Thickening_0_3',
       'Vessels_Intimal_Fibrosis_0_3', 'Vessels_Arteriolar_Hyalinosis_0_3'], axis=1)

biobank=biobank.astype(float)

import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans

# Assuming expression_matrix is your DataFrame containing bulk sequencing data

# Initialize an empty DataFrame to store the cluster labels
cluster_labels_df = pd.DataFrame(index=expression_matrix.index)

# Function to process and plot PCA for a given gene set
def process_and_plot_pca(genes_to_score, expression_matrix, plot_title, output_file):
    # Filter genes that are in the expression_matrix
    valid_genes = [gene for gene in genes_to_score if gene in expression_matrix.columns]
    if not valid_genes:
        print(f"No valid genes found for {plot_title}")
        return

    # Select the genes for PCA
    data = expression_matrix[valid_genes]

    # Center the data by subtracting the mean
    data_centered = data - data.mean()

    # Scale the data to the range (-1, 1)
    scaler = MinMaxScaler(feature_range=(-1, 1))
    data_scaled = scaler.fit_transform(data_centered)

    # Convert scaled data back to DataFrame for consistency
    data_scaled_df = pd.DataFrame(data_scaled, index=data.index, columns=data.columns)

    # Perform PCA
    pca = PCA(n_components=2)  # Reduce to 2 components for 2D visualization
    principal_components = pca.fit_transform(data_scaled_df)

    # Create a DataFrame with the principal components
    pca_df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2'], index=data.index)

    # Perform KMeans clustering
    k = 2  # Adjust based on your specific needs
    kmeans = KMeans(n_clusters=k, random_state=0)
    kmeans.fit(data_scaled_df)
    labels = kmeans.labels_

    # Add cluster labels to the PCA DataFrame
    pca_df['Cluster'] = labels

    # Add the cluster labels to the cluster_labels_df
    cluster_labels_df[plot_title] = labels
    cluster_labels_df[f'{plot_title}_score']=data_scaled_df.sum(axis=1)
    # Plot the PCA
    plt.figure(figsize=(6, 4))
    ax = sns.scatterplot(x='PC1', y='PC2', hue='Cluster', data=pca_df, palette='tab10', s=50, alpha=0.8)
    plt.title(plot_title)
    plt.xlabel('Principal Component 1')
    plt.ylabel('Principal Component 2')
    plt.legend(title='Cluster', frameon=False)
    plt.grid(False)
    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)
    plt.tight_layout()
    # Save the plot
    plt.savefig(output_file, dpi=300, bbox_inches='tight')
    plt.show()

    # Optional: Print explained variance to understand the importance of each component
    print(f"Explained variance ratio for {plot_title}:", pca.explained_variance_ratio_)

# Iterate through each key in the top_genes dictionary
for gene_set_name, genes_to_score in top_genes.items():
    plot_title = f'PCA of Gene Expression Data: {gene_set_name}'
    output_file = f'PCA_plot_{gene_set_name}.png'
    process_and_plot_pca(genes_to_score, expression_matrix, plot_title, output_file)

# Display the cluster labels DataFrame
print(cluster_labels_df)

# Strip 'PCA of Gene Expression Data: ' and '_DEGs' from column names
new_columns = [col.replace('PCA of Gene Expression Data: ', '').replace('_DEGs', '') for col in cluster_labels_df.columns]
cluster_labels_df.columns = new_columns

# Display the updated cluster labels DataFrame
print(cluster_labels_df)

for col in cluster_labels_df.columns:
    # Check if the column name ends with '_score'
    if col.endswith('_score'):
        # Calculate the mean of the score column
        mean_score = cluster_labels_df[col].mean()

        # Find the corresponding label column by removing '_score'
        label_col = col.replace('_score', '')

        # Replace 1 and 0 with 'high' and 'low' based on the mean score
        cluster_labels_df[label_col] = cluster_labels_df.apply(
            lambda row: 'high' if row[col] > mean_score else 'low', axis=1
        )

# Display the updated DataFrame
#print(cluster_labels_df)

# Select columns that have '_score' at the end
columns_to_keep = [col for col in cluster_labels_df.columns if col.endswith('_score')]

# Keep only the selected columns
cluster_labels_df = cluster_labels_df[columns_to_keep]

# Display the resulting dataframe to verify
print(cluster_labels_df.shape)

# Replace 'Fibroblast_Fibroblast' with 'Fibro' in the column names
cluster_labels_df.columns = cluster_labels_df.columns.str.replace('_score', '')
cluster_labels_df.columns = cluster_labels_df.columns.str.replace('niches_', '')
# Display the updated dataframe to verify the changes
#print(cluster_labels_df)

combined_df= pd.merge(biobank, cluster_labels_df,left_index=True, right_index=True, how='inner')
#combined_df

# Compute the Pearson correlation matrix
correlation_matrix = combined_df.corr(method='pearson')

# Display the correlation matrix
#print(correlation_matrix)

# 10 genes
plt.figure(figsize=(10, 8))
ax = sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', vmin=0)
ax.xaxis.tick_top()  # Move x-axis labels to the top
plt.xticks(rotation=90)  # Rotate the x-axis labels for better readability
plt.title('')
plt.show()

combined_df.columns

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Assuming 'combined_biobank' is your DataFrame and contains all the mentioned genes

# List of genes to plot
correlation = ['CD_Niche', 'CNT_Niche', 'Fibroblast_Niche',
       'Glomerular_Niche', 'Immune_Niche', 'PT_Niche', 'TAL_Niche',
       'Vascular_Niche', 'iPT_Niche', 'iTAL_Niche']
correlate_with = 'Interstitium_Fibrosis'  # Specify the column name for correlation

# Create scatterplots with regression line and correlation coefficient
for gene in correlation:
    plt.figure(figsize=(4, 4))  # Set the figure size for better readability
    # Create a scatter plot with a linear regression line (regplot)
    ax = sns.regplot(x=gene, y=correlate_with, data=combined_df, scatter_kws={'alpha': 1, 'color': 'black', 's': 10}, line_kws={"color": "black"}, ci=None)

    # Calculate the Pearson correlation coefficient
    correlation_coef = combined_df[gene].corr(combined_df[correlate_with])

    # Title with correlation
    plt.title(f'{gene} vs IF - Corr: {correlation_coef:.2f}')

    # Axis labels
    plt.xlabel(f'{gene}_Score')
    plt.ylabel('Interstitial Fibrosis')
    plt.savefig(f'/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/spatial_signatures/niches/scatterplot_interstitial_fibrosis_cortex_{gene}.png', dpi=450, transparent=False)
    # Show the plot
    plt.show()



"""now with glomseq"""

import pandas as pd
expression_matrix = pd.read_csv('/content/drive/MyDrive/Bernhard/BulkSeq/Glom_Bulk_Seq_HK.txt', sep= '\t')

gene_information = pd.read_csv('/content/drive/MyDrive/Bernhard/BulkSeq/gene_information.txt', sep= '\t')

gene_information['ensmbl']

gene_information

gene_dict = pd.Series(gene_information.symbol.values, index=gene_information.ensmbl_v).to_dict()

# Extract the Gene IDs without the version numbers
expression_matrix['Gene_ID_Base'] = expression_matrix['Gene_ID'].str.split('.').str[0]

# Map the gene names onto the expression matrix
expression_matrix['Gene_Name'] = expression_matrix['Gene_ID'].map(gene_dict)

# Display the first few rows to ensure it was processed correctly
print(expression_matrix.head())

gene_dict = pd.Series(gene_information.symbol.values, index=gene_information.ensmbl).to_dict()

# Map the gene names onto the expression matrix
expression_matrix['Gene_Name_2'] = expression_matrix['Gene_ID_Base'].map(gene_dict)

# Display the first few rows to ensure it was processed correctly
print(expression_matrix.head())

expression_matrix.drop(['Gene_ID_Base', 'Gene_Name', 'Gene_ID'], axis=1)
expression_matrix

expression_matrix=expression_matrix.drop('Gene_ID', axis=1)

expression_matrix

expression_matrix=expression_matrix.set_index('Gene_Name_2')

expression_matrix.index.name=None

expression_matrix=expression_matrix.T

expression_matrix.index = expression_matrix.index.str.rstrip('G')

expression_matrix

biobank=pd.read_csv('/content/Susztak_Lab_Biobank_122023_subset_scored.csv')

biobank=biobank.set_index('ID')

biobank=biobank.drop(['pos', 'Disease', 'GFR', 'DM', 'HTN', 'Age', 'Gender', 'Race',
       'Collection_Site', 'Height_cm', 'Weight_kg', 'BMI',
       'pct_glom_sclerosis', 'Tubules_Atrophy ',
       'Glomeruli_Total_num', 'Glomeruli_Globally Schlerotic_num',
       'Glomeruli_Segmentally_Schlerotic_num', 'Glomeruli_Wall_Thickening_0_3',
       'Glomeruli_Hypoperfused_0_3', 'Glomeruli_Mesangial_Matrix_0_3',
       'Glomeruli_Mesangial_Cellularity_0_3', 'Glomeruli_KW_Nodules_0_1',
       'Glomeruli_Pericapsular_Fibrosis_0_2', 'Tubules_pct_Atrophy ',
       'Tubules_pct_Acute_Tubular_Injury', 'Tubules_Reabsorption_0_3',
       'Interstitium_Lymphocytic_Infiltrate_0_3',
       'Interstitium_Plasmacytic_Infiltrate_0_3',
       'Interstitium_Eosinophils_0_3', 'Vessels_Medial_Thickening_0_3',
       'Vessels_Intimal_Fibrosis_0_3', 'Vessels_Arteriolar_Hyalinosis_0_3'], axis=1)

biobank=biobank.drop(['HK2456', 'HK2770','HK2430', 'HK2739', 'HK2494', 'HK17', 'HK84', 'HK2412', 'HK2441'], axis=0)

#biobank=biobank.drop(['HK344'], axis=0)
biobank=biobank.drop(['HK2379'], axis=0)

biobank

biobank['Glomeruli_globally_Schlerotic_pct'].dtype

biobank=biobank.astype(str)

biobank.shape

biobank=biobank[biobank['Glomeruli_globally_Schlerotic_pct']!='\xa0']
biobank.shape

biobank=biobank.astype(float)
biobank

expression_matrix=expression_matrix.drop(['Gene_ID_Base', 'Gene_Name'], axis=0)

expression_matrix=expression_matrix.astype(float)

import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans

# Assuming expression_matrix is your DataFrame containing bulk sequencing data

# Initialize an empty DataFrame to store the cluster labels
cluster_labels_df = pd.DataFrame(index=expression_matrix.index)

# Function to process and plot PCA for a given gene set
def process_and_plot_pca(genes_to_score, expression_matrix, plot_title, output_file):
    # Filter genes that are in the expression_matrix
    valid_genes = [gene for gene in genes_to_score if gene in expression_matrix.columns]
    if not valid_genes:
        print(f"No valid genes found for {plot_title}")
        return

    # Select the genes for PCA
    data = expression_matrix[valid_genes]

    # Center the data by subtracting the mean
    data_centered = data - data.mean()

    # Scale the data to the range (-1, 1)
    scaler = MinMaxScaler(feature_range=(-1, 1))
    data_scaled = scaler.fit_transform(data_centered)

    # Convert scaled data back to DataFrame for consistency
    data_scaled_df = pd.DataFrame(data_scaled, index=data.index, columns=data.columns)

    # Perform PCA
    pca = PCA(n_components=2)  # Reduce to 2 components for 2D visualization
    principal_components = pca.fit_transform(data_scaled_df)

    # Create a DataFrame with the principal components
    pca_df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2'], index=data.index)

    # Perform KMeans clustering
    k = 2  # Adjust based on your specific needs
    kmeans = KMeans(n_clusters=k, random_state=0)
    kmeans.fit(data_scaled_df)
    labels = kmeans.labels_

    # Add cluster labels to the PCA DataFrame
    pca_df['Cluster'] = labels

    # Add the cluster labels to the cluster_labels_df
    cluster_labels_df[plot_title] = labels
    cluster_labels_df[f'{plot_title}_score']=data_scaled_df.sum(axis=1)
    # Plot the PCA
    plt.figure(figsize=(6, 4))
    ax = sns.scatterplot(x='PC1', y='PC2', hue='Cluster', data=pca_df, palette='tab10', s=50, alpha=0.8)
    plt.title(plot_title)
    plt.xlabel('Principal Component 1')
    plt.ylabel('Principal Component 2')
    plt.legend(title='Cluster', frameon=False)
    plt.grid(False)
    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)
    plt.tight_layout()
    # Save the plot
    plt.savefig(output_file, dpi=300, bbox_inches='tight')
    plt.show()

    # Optional: Print explained variance to understand the importance of each component
    print(f"Explained variance ratio for {plot_title}:", pca.explained_variance_ratio_)

# Iterate through each key in the top_genes dictionary
for gene_set_name, genes_to_score in top_genes.items():
    plot_title = f'PCA of Gene Expression Data: {gene_set_name}'
    output_file = f'PCA_plot_{gene_set_name}.png'
    process_and_plot_pca(genes_to_score, expression_matrix, plot_title, output_file)

# Display the cluster labels DataFrame
print(cluster_labels_df)

# Strip 'PCA of Gene Expression Data: ' and '_DEGs' from column names
new_columns = [col.replace('PCA of Gene Expression Data: ', '').replace('_DEGs', '') for col in cluster_labels_df.columns]
cluster_labels_df.columns = new_columns

# Display the updated cluster labels DataFrame
print(cluster_labels_df)

for col in cluster_labels_df.columns:
    # Check if the column name ends with '_score'
    if col.endswith('_score'):
        # Calculate the mean of the score column
        mean_score = cluster_labels_df[col].mean()

        # Find the corresponding label column by removing '_score'
        label_col = col.replace('_score', '')

        # Replace 1 and 0 with 'high' and 'low' based on the mean score
        cluster_labels_df[label_col] = cluster_labels_df.apply(
            lambda row: 'high' if row[col] > mean_score else 'low', axis=1
        )

# Display the updated DataFrame
#print(cluster_labels_df)

# Select columns that have '_score' at the end
columns_to_keep = [col for col in cluster_labels_df.columns if col.endswith('_score')]

# Keep only the selected columns
cluster_labels_df = cluster_labels_df[columns_to_keep]

# Display the resulting dataframe to verify
print(cluster_labels_df.shape)

# Replace 'Fibroblast_Fibroblast' with 'Fibro' in the column names
cluster_labels_df.columns = cluster_labels_df.columns.str.replace('_score', '')
cluster_labels_df.columns = cluster_labels_df.columns.str.replace('niches_', '')
# Display the updated dataframe to verify the changes
#print(cluster_labels_df)

combined_df= pd.merge(biobank, cluster_labels_df,left_index=True, right_index=True, how='inner')
#combined_df

# Compute the Pearson correlation matrix
correlation_matrix = combined_df.corr(method='pearson')

# Display the correlation matrix
#print(correlation_matrix)

# 10 genes
plt.figure(figsize=(10, 8))
ax = sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', vmin=0)
ax.xaxis.tick_top()  # Move x-axis labels to the top
plt.xticks(rotation=90)  # Rotate the x-axis labels for better readability
plt.title('')
plt.show()

combined_df.columns

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Assuming 'combined_biobank' is your DataFrame and contains all the mentioned genes

# List of genes to plot
correlation = [
       'Glomerular_Niche']
correlate_with = 'Interstitium_Fibrosis'  # Specify the column name for correlation

# Create scatterplots with regression line and correlation coefficient
for gene in correlation:
    plt.figure(figsize=(4, 4))  # Set the figure size for better readability
    # Create a scatter plot with a linear regression line (regplot)
    ax = sns.regplot(x=gene, y=correlate_with, data=combined_df, scatter_kws={'alpha': 1, 'color': 'black', 's': 10}, line_kws={"color": "black"}, ci=None)

    # Calculate the Pearson correlation coefficient
    correlation_coef = combined_df[gene].corr(combined_df[correlate_with])

    # Title with correlation
    plt.title(f'{gene} vs IF - Corr: {correlation_coef:.2f}')

    # Axis labels
    plt.xlabel(f'{gene}_Score')
    plt.ylabel('Interstitial Fibrosis')
    plt.savefig(f'/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/spatial_signatures/niches/scatterplot_interstitial_fibrosis_gloms_{gene}.png', dpi=450, transparent=False)
    # Show the plot
    plt.show()