# -*- coding: utf-8 -*-
"""Prepare Neighborhood SCVI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FzGjQtskqpiPV-ofK0eaUP6ntO_GSKWt
"""

!pip --quiet install scanpy
!pip --quiet install leidenalg
#!pip --quiet install squidpy

from google.colab import drive
drive.mount('/content/drive')

import csv
import anndata as ad
import gzip
import os
import scipy.io
import scanpy as sc
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from google.colab import drive
import leidenalg as la
from pathlib import Path
#import squidpy as sq

adata = sc.read_h5ad('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/all_Cosmx_cleaned_neighbored.h5ad')

adata

adata2 = sc.read_h5ad('/content/drive/MyDrive/Bernhard/Cosmx/SCVI/All_Runs/adult_CosMx_preFilter.h5ad')

print(adata2.obs["sample"])

print(adata2.obs_names)

adata2.obs_names = adata2.obs["sample"].astype(str) + '_' + adata2.obs_names.astype(str)

print(adata2.obs_names)

common_cells = adata.obs.index.intersection(adata2.obs.index)

# Step 2: Filter the second AnnData object
# Create a new AnnData object that only includes the common cells
filtered_adata2 = adata2[common_cells]

filtered_adata2.obs['annotation_post_scanvi70_broad']=adata.obs['annotation_post_scanvi70_broad'].copy()
filtered_adata2.obsm['X_umap']=adata.obsm['X_umap'].copy()
sc.pl.umap(filtered_adata2, color = "annotation_post_scanvi70_broad")

filtered_adata2

adata_list = []

for i, j in enumerate(filtered_adata2.obs["sample"].unique()):
    adata_sample = filtered_adata2[filtered_adata2.obs["sample"] == j]

    sq.gr.spatial_neighbors(
        adata_sample,
        radius=200/0.12,
        coord_type="generic",
        key_added="200_micron",
        spatial_key = "spatial_fov",
    )
    print(j)
    neighbor_check = []
    for k in range(adata_sample.obsp["200_micron_connectivities"].shape[0]):
        neighbor_check.append(np.sum(adata_sample.obsp["200_micron_connectivities"][k]))
        # total number of neighbors
    plt.hist(neighbor_check, bins = np.arange(0, 100, 1))
    plt.show()

    adata_list.append(adata_sample)

adata_cosmx = ad.concat(adata_list, pairwise = True)

# Transfer '20_micron_connectivities'
filtered_adata2.obsp['200_micron_connectivities'] = adata_cosmx.obsp['200_micron_connectivities']

# Transfer '20_micron_distances'
filtered_adata2.obsp['200_micron_distances'] = adata_cosmx.obsp['200_micron_distances']

adata_list = []

for i, j in enumerate(filtered_adata2.obs["sample"].unique()):
    adata_sample = filtered_adata2[filtered_adata2.obs["sample"] == j]

    sq.gr.spatial_neighbors(
        adata_sample,
        radius=180/0.12,
        coord_type="generic",
        key_added="180_micron",
        spatial_key = "spatial_fov",
    )
    print(j)
    neighbor_check = []
    for k in range(adata_sample.obsp["180_micron_connectivities"].shape[0]):
        neighbor_check.append(np.sum(adata_sample.obsp["180_micron_connectivities"][k]))
        # total number of neighbors
    plt.hist(neighbor_check, bins = np.arange(0, 100, 1))
    plt.show()

    adata_list.append(adata_sample)

adata_cosmx = ad.concat(adata_list, pairwise = True)
# Transfer '20_micron_connectivities'
filtered_adata2.obsp['180_micron_connectivities'] = adata_cosmx.obsp['180_micron_connectivities']

# Transfer '20_micron_distances'
filtered_adata2.obsp['180_micron_distances'] = adata_cosmx.obsp['180_micron_distances']

adata_list = []

for i, j in enumerate(filtered_adata2.obs["sample"].unique()):
    adata_sample = filtered_adata2[filtered_adata2.obs["sample"] == j]

    sq.gr.spatial_neighbors(
        adata_sample,
        radius=160/0.12,
        coord_type="generic",
        key_added="160_micron",
        spatial_key = "spatial_fov",
    )
    print(j)
    neighbor_check = []
    for k in range(adata_sample.obsp["160_micron_connectivities"].shape[0]):
        neighbor_check.append(np.sum(adata_sample.obsp["160_micron_connectivities"][k]))
        # total number of neighbors
    plt.hist(neighbor_check, bins = np.arange(0, 100, 1))
    plt.show()

    adata_list.append(adata_sample)

adata_cosmx = ad.concat(adata_list, pairwise = True)
# Transfer '20_micron_connectivities'
filtered_adata2.obsp['160_micron_connectivities'] = adata_cosmx.obsp['160_micron_connectivities']

# Transfer '20_micron_distances'
filtered_adata2.obsp['160_micron_distances'] = adata_cosmx.obsp['160_micron_distances']

adata_list = []

for i, j in enumerate(filtered_adata2.obs["sample"].unique()):
    adata_sample = filtered_adata2[filtered_adata2.obs["sample"] == j]

    sq.gr.spatial_neighbors(
        adata_sample,
        radius=140/0.12,
        coord_type="generic",
        key_added="140_micron",
        spatial_key = "spatial_fov",
    )
    print(j)
    neighbor_check = []
    for k in range(adata_sample.obsp["140_micron_connectivities"].shape[0]):
        neighbor_check.append(np.sum(adata_sample.obsp["140_micron_connectivities"][k]))
        # total number of neighbors
    plt.hist(neighbor_check, bins = np.arange(0, 100, 1))
    plt.show()

    adata_list.append(adata_sample)

adata_cosmx = ad.concat(adata_list, pairwise = True)
# Transfer '20_micron_connectivities'
filtered_adata2.obsp['140_micron_connectivities'] = adata_cosmx.obsp['140_micron_connectivities']

# Transfer '20_micron_distances'
filtered_adata2.obsp['140_micron_distances'] = adata_cosmx.obsp['140_micron_distances']

adata_list = []

for i, j in enumerate(filtered_adata2.obs["sample"].unique()):
    adata_sample = filtered_adata2[filtered_adata2.obs["sample"] == j]

    sq.gr.spatial_neighbors(
        adata_sample,
        radius=120/0.12,
        coord_type="generic",
        key_added="120_micron",
        spatial_key = "spatial_fov",
    )
    print(j)
    neighbor_check = []
    for k in range(adata_sample.obsp["120_micron_connectivities"].shape[0]):
        neighbor_check.append(np.sum(adata_sample.obsp["120_micron_connectivities"][k]))
        # total number of neighbors
    plt.hist(neighbor_check, bins = np.arange(0, 100, 1))
    plt.show()

    adata_list.append(adata_sample)

adata_cosmx = ad.concat(adata_list, pairwise = True)
# Transfer '20_micron_connectivities'
filtered_adata2.obsp['120_micron_connectivities'] = adata_cosmx.obsp['120_micron_connectivities']

# Transfer '20_micron_distances'
filtered_adata2.obsp['120_micron_distances'] = adata_cosmx.obsp['120_micron_distances']

adata_list = []

for i, j in enumerate(filtered_adata2.obs["sample"].unique()):
    adata_sample = filtered_adata2[filtered_adata2.obs["sample"] == j]

    sq.gr.spatial_neighbors(
        adata_sample,
        radius=100/0.12,
        coord_type="generic",
        key_added="100_micron",
        spatial_key = "spatial_fov",
    )
    print(j)
    neighbor_check = []
    for k in range(adata_sample.obsp["100_micron_connectivities"].shape[0]):
        neighbor_check.append(np.sum(adata_sample.obsp["100_micron_connectivities"][k]))
        # total number of neighbors
    plt.hist(neighbor_check, bins = np.arange(0, 100, 1))
    plt.show()

    adata_list.append(adata_sample)

adata_cosmx = ad.concat(adata_list, pairwise = True)
# Transfer '20_micron_connectivities'
filtered_adata2.obsp['100_micron_connectivities'] = adata_cosmx.obsp['100_micron_connectivities']

# Transfer '20_micron_distances'
filtered_adata2.obsp['100_micron_distances'] = adata_cosmx.obsp['100_micron_distances']

adata_list = []

for i, j in enumerate(filtered_adata2.obs["sample"].unique()):
    adata_sample = filtered_adata2[filtered_adata2.obs["sample"] == j]

    sq.gr.spatial_neighbors(
        adata_sample,
        radius=80/0.12,
        coord_type="generic",
        key_added="80_micron",
        spatial_key = "spatial_fov",
    )
    print(j)
    neighbor_check = []
    for k in range(adata_sample.obsp["80_micron_connectivities"].shape[0]):
        neighbor_check.append(np.sum(adata_sample.obsp["80_micron_connectivities"][k]))
        # total number of neighbors
    plt.hist(neighbor_check, bins = np.arange(0, 100, 1))
    plt.show()

    adata_list.append(adata_sample)

adata_cosmx = ad.concat(adata_list, pairwise = True)
# Transfer '20_micron_connectivities'
filtered_adata2.obsp['80_micron_connectivities'] = adata_cosmx.obsp['80_micron_connectivities']

# Transfer '20_micron_distances'
filtered_adata2.obsp['80_micron_distances'] = adata_cosmx.obsp['80_micron_distances']

adata_list = []

for i, j in enumerate(filtered_adata2.obs["sample"].unique()):
    adata_sample = filtered_adata2[filtered_adata2.obs["sample"] == j]

    sq.gr.spatial_neighbors(
        adata_sample,
        radius=60/0.12,
        coord_type="generic",
        key_added="60_micron",
        spatial_key = "spatial_fov",
    )
    print(j)
    neighbor_check = []
    for k in range(adata_sample.obsp["60_micron_connectivities"].shape[0]):
        neighbor_check.append(np.sum(adata_sample.obsp["60_micron_connectivities"][k]))
        # total number of neighbors
    plt.hist(neighbor_check, bins = np.arange(0, 100, 1))
    plt.show()

    adata_list.append(adata_sample)

adata_cosmx = ad.concat(adata_list, pairwise = True)
# Transfer '20_micron_connectivities'
filtered_adata2.obsp['60_micron_connectivities'] = adata_cosmx.obsp['60_micron_connectivities']

# Transfer '20_micron_distances'
filtered_adata2.obsp['60_micron_distances'] = adata_cosmx.obsp['60_micron_distances']

adata_list = []

for i, j in enumerate(filtered_adata2.obs["sample"].unique()):
    adata_sample = filtered_adata2[filtered_adata2.obs["sample"] == j]

    sq.gr.spatial_neighbors(
        adata_sample,
        radius=40/0.12,
        coord_type="generic",
        key_added="40_micron",
        spatial_key = "spatial_fov",
    )
    print(j)
    neighbor_check = []
    for k in range(adata_sample.obsp["40_micron_connectivities"].shape[0]):
        neighbor_check.append(np.sum(adata_sample.obsp["40_micron_connectivities"][k]))
        # total number of neighbors
    plt.hist(neighbor_check, bins = np.arange(0, 100, 1))
    plt.show()

    adata_list.append(adata_sample)

adata_cosmx = ad.concat(adata_list, pairwise = True)
# Transfer '20_micron_connectivities'
filtered_adata2.obsp['40_micron_connectivities'] = adata_cosmx.obsp['40_micron_connectivities']

# Transfer '20_micron_distances'
filtered_adata2.obsp['40_micron_distances'] = adata_cosmx.obsp['40_micron_distances']

adata_list = []

for i, j in enumerate(filtered_adata2.obs["sample"].unique()):
    adata_sample = filtered_adata2[filtered_adata2.obs["sample"] == j]

    sq.gr.spatial_neighbors(
        adata_sample,
        radius=20/0.12,
        coord_type="generic",
        key_added="20_micron",
        spatial_key = "spatial_fov",
    )
    print(j)
    neighbor_check = []
    for k in range(adata_sample.obsp["20_micron_connectivities"].shape[0]):
        neighbor_check.append(np.sum(adata_sample.obsp["20_micron_connectivities"][k]))
        # total number of neighbors
    plt.hist(neighbor_check, bins = np.arange(0, 100, 1))
    plt.show()

    adata_list.append(adata_sample)

adata_cosmx = ad.concat(adata_list, pairwise = True)
# Transfer '20_micron_connectivities'
filtered_adata2.obsp['20_micron_connectivities'] = adata_cosmx.obsp['20_micron_connectivities']

# Transfer '20_micron_distances'
filtered_adata2.obsp['20_micron_distances'] = adata_cosmx.obsp['20_micron_distances']

import pandas as pd


all_dfs = []  # List to store DataFrames for each AnnData object

for i in filtered_adata2.obs["sample"].unique():
    adata_sample = filtered_adata2[filtered_adata2.obs["sample"] == i]
    # Create a temporary DataFrame for the current AnnData object
    temp_df = pd.DataFrame(index=adata_sample.obs.index)

    for pop in adata_sample.obs['annotation_post_scanvi70_broad'].unique():
        neighbor_type_indices = np.where(adata_sample.obs['annotation_post_scanvi70_broad'] == pop)[0]
        number_neighbors = pd.DataFrame(adata_sample.obsp["200_micron_connectivities"][:, neighbor_type_indices].sum(axis=1), index=adata_sample.obs.index)
        temp_df[pop] = number_neighbors[0]  # Assure correct formatting
    # Append the temporary DataFrame to the list
    all_dfs.append(temp_df)

# Concatenate the DataFrames from each AnnData object
df_for_mapping_200 = pd.concat(all_dfs, axis=0)

# Optionally, sort the index if needed
df_for_mapping_200.sort_index(inplace=True)
df_for_mapping_200=df_for_mapping_200.fillna(0)

filtered_adata2

#del filtered_adata2.obsp['200_micron_connectivities']
#del filtered_adata2.obsp['200_micron_distances']
gc.collect()

import gc
gc.collect()

gc.collect()

all_dfs = []  # List to store DataFrames for each AnnData object

for i in filtered_adata2.obs["sample"].unique():
    adata_sample = filtered_adata2[filtered_adata2.obs["sample"] == i]
    # Create a temporary DataFrame for the current AnnData object
    temp_df = pd.DataFrame(index=adata_sample.obs.index)

    for pop in adata_sample.obs['annotation_post_scanvi70_broad'].unique():
        neighbor_type_indices = np.where(adata_sample.obs['annotation_post_scanvi70_broad'] == pop)[0]
        number_neighbors = pd.DataFrame(adata_sample.obsp["180_micron_connectivities"][:, neighbor_type_indices].sum(axis=1), index=adata_sample.obs.index)
        temp_df[pop] = number_neighbors[0]  # Assure correct formatting
    # Append the temporary DataFrame to the list
    all_dfs.append(temp_df)

# Concatenate the DataFrames from each AnnData object
df_for_mapping_180 = pd.concat(all_dfs, axis=0)

# Optionally, sort the index if needed
df_for_mapping_180.sort_index(inplace=True)
df_for_mapping_180=df_for_mapping_180.fillna(0)

del filtered_adata2.obsp['180_micron_connectivities']
del filtered_adata2.obsp['180_micron_distances']
gc.collect()

import gc
gc.collect()

gc.collect()

gc.collect()

all_dfs = []  # List to store DataFrames for each AnnData object

for i in filtered_adata2.obs["sample"].unique():
    adata_sample = filtered_adata2[filtered_adata2.obs["sample"] == i]
    # Create a temporary DataFrame for the current AnnData object
    temp_df = pd.DataFrame(index=adata_sample.obs.index)

    for pop in adata_sample.obs['annotation_post_scanvi70_broad'].unique():
        neighbor_type_indices = np.where(adata_sample.obs['annotation_post_scanvi70_broad'] == pop)[0]
        number_neighbors = pd.DataFrame(adata_sample.obsp["160_micron_connectivities"][:, neighbor_type_indices].sum(axis=1), index=adata_sample.obs.index)
        temp_df[pop] = number_neighbors[0]  # Assure correct formatting
    # Append the temporary DataFrame to the list
    all_dfs.append(temp_df)

# Concatenate the DataFrames from each AnnData object
df_for_mapping_160 = pd.concat(all_dfs, axis=0)

# Optionally, sort the index if needed
df_for_mapping_160.sort_index(inplace=True)
df_for_mapping_160=df_for_mapping_160.fillna(0)

del filtered_adata2.obsm['160_micron_connectivities']
del filtered_adata2.obsm['160_micron_distances']
gc.collect()

all_dfs = []  # List to store DataFrames for each AnnData object

for i in filtered_adata2.obs["sample"].unique():
    adata_sample = filtered_adata2[filtered_adata2.obs["sample"] == i]
    # Create a temporary DataFrame for the current AnnData object
    temp_df = pd.DataFrame(index=adata_sample.obs.index)

    for pop in adata_sample.obs['annotation_post_scanvi70_broad'].unique():
        neighbor_type_indices = np.where(adata_sample.obs['annotation_post_scanvi70_broad'] == pop)[0]
        number_neighbors = pd.DataFrame(adata_sample.obsp["140_micron_connectivities"][:, neighbor_type_indices].sum(axis=1), index=adata_sample.obs.index)
        temp_df[pop] = number_neighbors[0]  # Assure correct formatting
    # Append the temporary DataFrame to the list
    all_dfs.append(temp_df)

# Concatenate the DataFrames from each AnnData object
df_for_mapping_140 = pd.concat(all_dfs, axis=0)

# Optionally, sort the index if needed
df_for_mapping_140.sort_index(inplace=True)
df_for_mapping_140=df_for_mapping_140.fillna(0)

del filtered_adata2.obsm['140_micron_connectivities']
del filtered_adata2.obsm['140_micron_distances']
gc.collect()

all_dfs = []  # List to store DataFrames for each AnnData object

for i in filtered_adata2.obs["sample"].unique():
    adata_sample = filtered_adata2[filtered_adata2.obs["sample"] == i]
    # Create a temporary DataFrame for the current AnnData object
    temp_df = pd.DataFrame(index=adata_sample.obs.index)

    for pop in adata_sample.obs['annotation_post_scanvi70_broad'].unique():
        neighbor_type_indices = np.where(adata_sample.obs['annotation_post_scanvi70_broad'] == pop)[0]
        number_neighbors = pd.DataFrame(adata_sample.obsp["120_micron_connectivities"][:, neighbor_type_indices].sum(axis=1), index=adata_sample.obs.index)
        temp_df[pop] = number_neighbors[0]  # Assure correct formatting
    # Append the temporary DataFrame to the list
    all_dfs.append(temp_df)

# Concatenate the DataFrames from each AnnData object
df_for_mapping_120 = pd.concat(all_dfs, axis=0)

# Optionally, sort the index if needed
df_for_mapping_120.sort_index(inplace=True)
df_for_mapping_120=df_for_mapping_120.fillna(0)

gc.collect()

all_dfs = []  # List to store DataFrames for each AnnData object

for i in filtered_adata2.obs["sample"].unique():
    adata_sample = filtered_adata2[filtered_adata2.obs["sample"] == i]
    # Create a temporary DataFrame for the current AnnData object
    temp_df = pd.DataFrame(index=adata_sample.obs.index)

    for pop in adata_sample.obs['annotation_post_scanvi70_broad'].unique():
        neighbor_type_indices = np.where(adata_sample.obs['annotation_post_scanvi70_broad'] == pop)[0]
        number_neighbors = pd.DataFrame(adata_sample.obsp["100_micron_connectivities"][:, neighbor_type_indices].sum(axis=1), index=adata_sample.obs.index)
        temp_df[pop] = number_neighbors[0]  # Assure correct formatting
    # Append the temporary DataFrame to the list
    all_dfs.append(temp_df)

# Concatenate the DataFrames from each AnnData object
df_for_mapping_100 = pd.concat(all_dfs, axis=0)

# Optionally, sort the index if needed
df_for_mapping_100.sort_index(inplace=True)
df_for_mapping_100=df_for_mapping_100.fillna(0)

del filtered_adata2.obsm['100_micron_connectivities']
del filtered_adata2.obsm['100_micron_distances']
gc.collect()

all_dfs = []  # List to store DataFrames for each AnnData object

for i in filtered_adata2.obs["sample"].unique():
    adata_sample = filtered_adata2[filtered_adata2.obs["sample"] == i]
    # Create a temporary DataFrame for the current AnnData object
    temp_df = pd.DataFrame(index=adata_sample.obs.index)

    for pop in adata_sample.obs['annotation_post_scanvi70_broad'].unique():
        neighbor_type_indices = np.where(adata_sample.obs['annotation_post_scanvi70_broad'] == pop)[0]
        number_neighbors = pd.DataFrame(adata_sample.obsp["80_micron_connectivities"][:, neighbor_type_indices].sum(axis=1), index=adata_sample.obs.index)
        temp_df[pop] = number_neighbors[0]  # Assure correct formatting
    # Append the temporary DataFrame to the list
    all_dfs.append(temp_df)

# Concatenate the DataFrames from each AnnData object
df_for_mapping_80 = pd.concat(all_dfs, axis=0)

# Optionally, sort the index if needed
df_for_mapping_80.sort_index(inplace=True)
df_for_mapping_80=df_for_mapping_80.fillna(0)

all_dfs = []  # List to store DataFrames for each AnnData object

for i in filtered_adata2.obs["sample"].unique():
    adata_sample = filtered_adata2[filtered_adata2.obs["sample"] == i]
    # Create a temporary DataFrame for the current AnnData object
    temp_df = pd.DataFrame(index=adata_sample.obs.index)

    for pop in adata_sample.obs['annotation_post_scanvi70_broad'].unique():
        neighbor_type_indices = np.where(adata_sample.obs['annotation_post_scanvi70_broad'] == pop)[0]
        number_neighbors = pd.DataFrame(adata_sample.obsp["60_micron_connectivities"][:, neighbor_type_indices].sum(axis=1), index=adata_sample.obs.index)
        temp_df[pop] = number_neighbors[0]  # Assure correct formatting
    # Append the temporary DataFrame to the list
    all_dfs.append(temp_df)

# Concatenate the DataFrames from each AnnData object
df_for_mapping_60 = pd.concat(all_dfs, axis=0)

# Optionally, sort the index if needed
df_for_mapping_60.sort_index(inplace=True)
df_for_mapping_60=df_for_mapping_60.fillna(0)

del filtered_adata2.obsm['60_micron_connectivities']
del filtered_adata2.obsm['60_micron_distances']
gc.collect()

gc.collect()

all_dfs = []  # List to store DataFrames for each AnnData object

for i in filtered_adata2.obs["sample"].unique():
    adata_sample = filtered_adata2[filtered_adata2.obs["sample"] == i]
    # Create a temporary DataFrame for the current AnnData object
    temp_df = pd.DataFrame(index=adata_sample.obs.index)

    for pop in adata_sample.obs['annotation_post_scanvi70_broad'].unique():
        neighbor_type_indices = np.where(adata_sample.obs['annotation_post_scanvi70_broad'] == pop)[0]
        number_neighbors = pd.DataFrame(adata_sample.obsp["40_micron_connectivities"][:, neighbor_type_indices].sum(axis=1), index=adata_sample.obs.index)
        temp_df[pop] = number_neighbors[0]  # Assure correct formatting
    # Append the temporary DataFrame to the list
    all_dfs.append(temp_df)

# Concatenate the DataFrames from each AnnData object
df_for_mapping_40 = pd.concat(all_dfs, axis=0)

# Optionally, sort the index if needed
df_for_mapping_40.sort_index(inplace=True)
df_for_mapping_40=df_for_mapping_40.fillna(0)

all_dfs = []  # List to store DataFrames for each AnnData object

for i in filtered_adata2.obs["sample"].unique():
    adata_sample = filtered_adata2[filtered_adata2.obs["sample"] == i]
    # Create a temporary DataFrame for the current AnnData object
    temp_df = pd.DataFrame(index=adata_sample.obs.index)

    for pop in adata_sample.obs['annotation_post_scanvi70_broad'].unique():
        neighbor_type_indices = np.where(adata_sample.obs['annotation_post_scanvi70_broad'] == pop)[0]
        number_neighbors = pd.DataFrame(adata_sample.obsp["20_micron_connectivities"][:, neighbor_type_indices].sum(axis=1), index=adata_sample.obs.index)
        temp_df[pop] = number_neighbors[0]  # Assure correct formatting
    # Append the temporary DataFrame to the list
    all_dfs.append(temp_df)

# Concatenate the DataFrames from each AnnData object
df_for_mapping_20 = pd.concat(all_dfs, axis=0)

# Optionally, sort the index if needed
df_for_mapping_20.sort_index(inplace=True)
df_for_mapping_20=df_for_mapping_20.fillna(0)

df_for_mapping_200=df_for_mapping_200-df_for_mapping_180
df_for_mapping_180=df_for_mapping_180-df_for_mapping_160
df_for_mapping_160=df_for_mapping_160-df_for_mapping_140
df_for_mapping_140=df_for_mapping_140-df_for_mapping_120
df_for_mapping_120=df_for_mapping_120-df_for_mapping_100
df_for_mapping_100=df_for_mapping_100-df_for_mapping_80
df_for_mapping_80=df_for_mapping_80-df_for_mapping_60
df_for_mapping_60=df_for_mapping_60-df_for_mapping_40
df_for_mapping_40=df_for_mapping_40-df_for_mapping_20

df_for_mapping_200.columns = df_for_mapping_200.columns.map(lambda x: f"{x}_200")
df_for_mapping_200

#df_for_mapping_200.columns = df_for_mapping_200.columns.map(lambda x: f"{x}_200")
df_for_mapping_180.columns = df_for_mapping_180.columns.map(lambda x: f"{x}_180")
df_for_mapping_160.columns = df_for_mapping_160.columns.map(lambda x: f"{x}_160")
df_for_mapping_140.columns = df_for_mapping_140.columns.map(lambda x: f"{x}_140")
df_for_mapping_120.columns = df_for_mapping_120.columns.map(lambda x: f"{x}_120")
df_for_mapping_100.columns = df_for_mapping_100.columns.map(lambda x: f"{x}_100")
df_for_mapping_80.columns = df_for_mapping_80.columns.map(lambda x: f"{x}_80")
df_for_mapping_60.columns = df_for_mapping_60.columns.map(lambda x: f"{x}_60")
df_for_mapping_40.columns = df_for_mapping_40.columns.map(lambda x: f"{x}_40")
df_for_mapping_20.columns = df_for_mapping_20.columns.map(lambda x: f"{x}_20")

merged_df = pd.concat([df_for_mapping_20, df_for_mapping_40], axis=1)
merged_df = pd.concat([merged_df, df_for_mapping_60], axis=1)
merged_df = pd.concat([merged_df, df_for_mapping_80], axis=1)
merged_df = pd.concat([merged_df, df_for_mapping_100], axis=1)
merged_df = pd.concat([merged_df, df_for_mapping_120], axis=1)
merged_df = pd.concat([merged_df, df_for_mapping_140], axis=1)
merged_df = pd.concat([merged_df, df_for_mapping_160], axis=1)
merged_df = pd.concat([merged_df, df_for_mapping_180], axis=1)
merged_df = pd.concat([merged_df, df_for_mapping_200], axis=1)

merged_df.shape

from scipy import sparse

filtered_adata2

columns = ['160_micron_connectivities', '160_micron_distances', '140_micron_connectivities', '140_micron_distances', '120_micron_connectivities', '120_micron_distances', '100_micron_connectivities', '100_micron_distances', '80_micron_connectivities', '80_micron_distances', '60_micron_connectivities', '60_micron_distances', '40_micron_connectivities', '40_micron_distances', '20_micron_connectivities', '20_micron_distances']
for column in columns:
  del filtered_adata2.obsp[column]



import anndata
# Convert merged_df to a sparse matrix
sparse_matrix = scipy.sparse.csr_matrix(merged_df.values)

# Create a new AnnData object with the sparse matrix
new_adata = anndata.AnnData(X=sparse_matrix)

# Assign merged_df columns to .var
new_adata.var = pd.DataFrame(index=merged_df.columns)
new_adata.var['feature_names'] = merged_df.columns

# Assign merged_df index to .obs
new_adata.obs = pd.DataFrame(index=merged_df.index)

# Verify the new AnnData object
print(new_adata)

# Convert merged_df to a sparse matrix
sparse_matrix = scipy.sparse.csr_matrix(merged_df.values)
new_adata.X=sparse_matrix



new_adata.layers['original_neighbor_values']=new_adata.X

print(new_adata.X)

print(new_adata.obs_names)

print(filtered_adata2.obs_names)

filtered_adata2

new_adata.obs['sample']=filtered_adata2.obs['sample']

new_adata.obs['type']=filtered_adata2.obs['type']

new_adata.obs['annotation_post_scanvi70_broad']=filtered_adata2.obs['annotation_post_scanvi70_broad']

sc.pp.normalize_total(new_adata, inplace=True)
sc.pp.log1p(new_adata)
sc.pp.pca(new_adata)
sc.pp.neighbors(new_adata, n_neighbors = 10, n_pcs=30)
sc.tl.umap(new_adata)
sc.tl.leiden(new_adata)

sc.pl.umap(new_adata, color = "sample")

hk3106=new_adata[new_adata.obs['sample']=='HK3106']
sc.pl.umap(hk3106, color = "sample")

sc.pp.normalize_total(hk3106, inplace=True)
sc.pp.log1p(hk3106)
sc.pp.pca(hk3106)
sc.pp.neighbors(hk3106, n_neighbors = 10, n_pcs=30)
sc.tl.umap(hk3106)
sc.tl.leiden(hk3106)

sc.pl.umap(hk3106, color = "leiden")

#df_for_mapping_200['sum']=df_for_mapping_200.sum(axis=1)
sum_dict = df_for_mapping_200['sum'].to_dict()
new_adata.obs['total_neighbors_200'] = new_adata.obs.index.map(sum_dict)

df_for_mapping_180['sum']=df_for_mapping_180.sum(axis=1)
sum_dict = df_for_mapping_180['sum'].to_dict()
new_adata.obs['total_neighbors_180'] = new_adata.obs.index.map(sum_dict)

df_for_mapping_160['sum']=df_for_mapping_160.sum(axis=1)
sum_dict = df_for_mapping_160['sum'].to_dict()
new_adata.obs['total_neighbors_160'] = new_adata.obs.index.map(sum_dict)

df_for_mapping_140['sum']=df_for_mapping_140.sum(axis=1)
sum_dict = df_for_mapping_140['sum'].to_dict()
new_adata.obs['total_neighbors_140'] = new_adata.obs.index.map(sum_dict)

df_for_mapping_120['sum']=df_for_mapping_120.sum(axis=1)
sum_dict = df_for_mapping_120['sum'].to_dict()
new_adata.obs['total_neighbors_120'] = new_adata.obs.index.map(sum_dict)

df_for_mapping_100['sum']=df_for_mapping_100.sum(axis=1)
sum_dict = df_for_mapping_100['sum'].to_dict()
new_adata.obs['total_neighbors_100'] = new_adata.obs.index.map(sum_dict)

df_for_mapping_80['sum']=df_for_mapping_80.sum(axis=1)
sum_dict = df_for_mapping_80['sum'].to_dict()
new_adata.obs['total_neighbors_80'] = new_adata.obs.index.map(sum_dict)

df_for_mapping_60['sum']=df_for_mapping_60.sum(axis=1)
sum_dict = df_for_mapping_60['sum'].to_dict()
new_adata.obs['total_neighbors_60'] = new_adata.obs.index.map(sum_dict)

df_for_mapping_40['sum']=df_for_mapping_40.sum(axis=1)
sum_dict = df_for_mapping_40['sum'].to_dict()
new_adata.obs['total_neighbors_40'] = new_adata.obs.index.map(sum_dict)

df_for_mapping_20['sum']=df_for_mapping_20.sum(axis=1)
sum_dict = df_for_mapping_20['sum'].to_dict()
new_adata.obs['total_neighbors_20'] = new_adata.obs.index.map(sum_dict)

df_for_mapping_200

new_adata.write('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/neighbors_as_genes_all_cosmx.h5ad')

new_adata=sc.read_h5ad('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/neighbors_as_genes_all_cosmx.h5ad')

new_adata_subset=new_adata[new_adata.obs['total_neighbors_200']>50]
new_adata_subset

import pandas as pd

# Convert the sparse matrix to a dense format and create a DataFrame
gene_expression_df = pd.DataFrame(
    new_adata_subset.X.toarray(),
    index=new_adata_subset.obs_names,
    columns=new_adata_subset.var_names
)

# Print the first few rows to verify
print(gene_expression_df.head())

# List of .obs columns you want to add to the DataFrame
obs_columns = [
    'total_neighbors_200', 'total_neighbors_180', 'total_neighbors_160',
    'total_neighbors_140', 'total_neighbors_120', 'total_neighbors_100',
    'total_neighbors_80', 'total_neighbors_60', 'total_neighbors_40', 'total_neighbors_20'
]

# Add the selected .obs columns to the gene expression DataFrame
gene_expression_df[obs_columns] = new_adata_subset.obs[obs_columns]

# Print the first few rows to verify the updated DataFrame
print(gene_expression_df.head())

# Loop over each suffix and corresponding total_neighbors column
for neighbors in [200, 180, 160, 140, 120, 100, 80, 60, 40, 20]:
    # Generate the suffix and obs column name
    suffix = f'_{neighbors}'
    obs_column = f'total_neighbors_{neighbors}'

    # Get all columns that end with the current suffix
    matching_columns = [col for col in gene_expression_df.columns if col.endswith(suffix)]

    # Perform the division for each matching column
    for col in matching_columns:
        gene_expression_df[col] /= gene_expression_df[obs_column]

# Print the first few rows to verify the updated DataFrame
print(gene_expression_df.head())

gene_expression_df = gene_expression_df.drop(obs_columns, axis=1)

# Print the first few rows to verify that columns are removed
print(gene_expression_df.head())

import numpy as np

# Replace infinite values with 0
gene_expression_df.replace([np.inf, -np.inf], 0, inplace=True)

# Verify changes by displaying the first few rows of the DataFrame
print(gene_expression_df.head())

gene_expression_df=gene_expression_df.fillna(0)

from sklearn.cluster import MiniBatchKMeans
from sklearn.metrics import silhouette_score

inertias = []
k = range(1, 25)

# dissimilarity would not be defined for a single cluster, thus, minimum number of clusters should be 2
for K in k:
    # Create a MiniBatchKMeans object, arbitrarily choosing 10 clusters
    kmeans = MiniBatchKMeans(n_clusters=K)
    # Fit the model to the data
    kmeans.fit(gene_expression_df)
    inertias.append(kmeans.inertia_)

plt.plot(k, inertias, 'bx-')
plt.xlabel('Values of K')
plt.ylabel('Inertia')
plt.title('The Elbow Method using Inertia')
plt.show()

# Create a MiniBatchKMeans object, arbitrarily choosing 10 clusters
kmeans = MiniBatchKMeans(n_clusters=10)

# Fit the model to the data
kmeans.fit(gene_expression_df)

# Predict the cluster labels for each data point
labels = kmeans.predict(gene_expression_df)

gene_expression_df['cluster_labels'] = labels
print(gene_expression_df)

gene_expression_df['annotation']=new_adata_subset.obs['annotation_post_scanvi70_broad']

gene_expression_df_save=gene_expression_df.copy()
gene_expression_df['cluster_labels'].value_counts()

gene_expression_df.head(2)

# Group by 'cluster_labels' and then get the count of each 'annotation' within each cluster
cluster_annotation_counts = gene_expression_df.groupby('cluster_labels')['annotation'].value_counts()

# If you want a DataFrame with a reset index
cluster_annotation_counts_df = cluster_annotation_counts.reset_index(name='count')

# Print the results
print(cluster_annotation_counts_df)

cluster_annotation_counts_df

import matplotlib.pyplot as plt
# Create a pivot table from the DataFrame
pivot_table = cluster_annotation_counts_df.pivot_table(index='cluster_labels', columns='annotation', values='count', fill_value=0)

# Plotting the data
pivot_table.plot(kind='bar', stacked=True, figsize=(14, 8), cmap='tab20')
plt.title('Stacked Bar Plot of Annotations per Cluster Label')
plt.xlabel('Cluster Labels')
plt.ylabel('Count of Annotations')
plt.xticks(rotation=45)  # Rotate labels for better readability
plt.legend(title='Annotations', bbox_to_anchor=(1.05, 1), loc='upper left')  # Move the legend out of the plot

# Show the plot
plt.tight_layout()  # Adjust subplot params so that the subplot(s) fits in the figure area
plt.show()

new_adata_subset.obs['cluster_labels']=gene_expression_df['cluster_labels']

new_adata_subset.obs['cluster_labels']

new_adata_subset.obs['cluster_labels']=new_adata_subset.obs['cluster_labels'].astype('category')

sc.tl.rank_genes_groups(new_adata_subset, groupby='cluster_labels', method='wilcoxon', pts = True)
sc.pl.rank_genes_groups(new_adata_subset, n_genes=25, sharey=False)

new_adata_subset.X=new_adata_subset.layers['original_neighbor_values']
sc.pp.log1p(new_adata_subset)
sc.tl.rank_genes_groups(new_adata_subset, groupby='cluster_labels', method='wilcoxon', pts = True)
sc.pl.rank_genes_groups(new_adata_subset, n_genes=25, sharey=False)

new_adata_subset.X=new_adata_subset.layers['original_neighbor_values'].copy()

new_adata_subset_2=new_adata[new_adata.obs['total_neighbors_200']>50]
new_adata_subset_2

print(new_adata_subset_2.X)

new_adata_subset_2.obs["cluster_labels_annotated"]=new_adata_subset.obs["cluster_labels_annotated"].copy()

new_adata_subset_2.obs["cluster_labels_annotated"].value_counts()

new_adata_subset.obs["cluster_labels"].value_counts()
cell_identities = {0: 'TAL Neighborhood 1', 1: 'Fibroblast Neighborhood 1', 2: 'PT Neighborhood 1', 3: 'TAL Neighborhood 2',
                   4: 'Immune Neighborhood 1', 5: 'PT Neighborhood 2', 6: 'TAL Neighborhood 3', 7: 'Glomerular Neighborhood', 8: 'PC Neighborhood', 9: 'DCT Neighborhood'}
new_adata_subset.obs["cluster_labels_annotated"] = new_adata_subset.obs['cluster_labels'].map(cell_identities).astype('category')

new_adata_subset.obs["cluster_labels_annotated"].value_counts()

new_adata_subset_2.write('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/neighbors_as_genes_subset_cosmx_annotated.h5ad')

gene_expression_df





"""Lets repeat mini batch K on SCVI corrected neighbormatrix"""

adata=sc.read_h5ad('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/neighbors_as_genes_subset_cosmx_annotated_scvi_v1.h5ad')

adata

print(adata.X)

adata.X=adata.layers['scvi_normalized'].copy()

print(adata.X)

import pandas as pd

# Convert the sparse matrix to a dense format and create a DataFrame
gene_expression_df = pd.DataFrame(
    adata.X,  # Use the array directly if it's already a dense array
    index=adata.obs_names,  # Use observation names as index
    columns=adata.var_names  # Use variable names as columns
)

# Print the first few rows to verify
print(gene_expression_df.head())

from sklearn.cluster import MiniBatchKMeans
from sklearn.metrics import silhouette_score

inertias = []
k = range(1, 25)

# dissimilarity would not be defined for a single cluster, thus, minimum number of clusters should be 2
for K in k:
    # Create a MiniBatchKMeans object, arbitrarily choosing 10 clusters
    kmeans = MiniBatchKMeans(n_clusters=K)
    # Fit the model to the data
    kmeans.fit(gene_expression_df)
    inertias.append(kmeans.inertia_)

plt.plot(k, inertias, 'bx-')
plt.xlabel('Values of K')
plt.ylabel('Inertia')
plt.title('The Elbow Method using Inertia')
plt.show()

# Create a MiniBatchKMeans object, arbitrarily choosing 10 clusters
kmeans = MiniBatchKMeans(n_clusters=10)

# Fit the model to the data
kmeans.fit(gene_expression_df)

# Predict the cluster labels for each data point
labels = kmeans.predict(gene_expression_df)

gene_expression_df['cluster_labels'] = labels
print(gene_expression_df)

gene_expression_df['annotation']=adata.obs['annotation_post_scanvi70_broad']

gene_expression_df_save=gene_expression_df.copy()
gene_expression_df['cluster_labels'].value_counts()

gene_expression_df.head(2)

# Group by 'cluster_labels' and then get the count of each 'annotation' within each cluster
cluster_annotation_counts = gene_expression_df.groupby('cluster_labels')['annotation'].value_counts()

# If you want a DataFrame with a reset index
cluster_annotation_counts_df = cluster_annotation_counts.reset_index(name='count')

# Print the results
print(cluster_annotation_counts_df)

cluster_annotation_counts_df

import matplotlib.pyplot as plt
# Create a pivot table from the DataFrame
pivot_table = cluster_annotation_counts_df.pivot_table(index='cluster_labels', columns='annotation', values='count', fill_value=0)

# Plotting the data
pivot_table.plot(kind='bar', stacked=True, figsize=(14, 8), cmap='tab20')
plt.title('Stacked Bar Plot of Annotations per Cluster Label')
plt.xlabel('Cluster Labels')
plt.ylabel('Count of Annotations')
plt.xticks(rotation=45)  # Rotate labels for better readability
plt.legend(title='Annotations', bbox_to_anchor=(1.05, 1), loc='upper left')  # Move the legend out of the plot

# Show the plot
plt.tight_layout()  # Adjust subplot params so that the subplot(s) fits in the figure area
plt.show()

adata.obs['cluster_labels_2']=gene_expression_df['cluster_labels']

adata.obs['cluster_labels_2']

adata.obs['cluster_labels_2']=adata.obs['cluster_labels_2'].astype('category')

adata.X=adata.layers['original_neighbor_values'].copy()

sc.tl.rank_genes_groups(adata, groupby='cluster_labels_2', method='wilcoxon', pts = True)
sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False)

adata.obs["cluster_labels_2"].value_counts()
cell_identities = {0: 'TAL Neighborhood 1', 1: 'DCT Neighborhood', 2: 'PC Neighborhood', 3: 'Fibroblast Neighborhood',
                   4: 'TAL Neighborhood 2', 5: 'PT Neighborhood 1', 6: 'Glomerular Neighborhood', 7: 'PT Neighborhood 2',
                   8: 'TAL Neighborhood 3', 9: 'Immune Neighborhood'}
adata.obs["cluster_labels_annotated_scvi"] = adata.obs['cluster_labels_2'].map(cell_identities).astype('category')

sc.pl.umap(adata, color = "cluster_labels_annotated_scvi")

sc.tl.rank_genes_groups(adata, groupby='cluster_labels_annotated_scvi', method='wilcoxon', pts = True)
sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False)

adata.X=adata.layers['scvi_normalized'].copy()

sc.tl.rank_genes_groups(adata, groupby='cluster_labels_annotated_scvi', method='wilcoxon', pts = True)
sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False)

adata.write('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/neighbors_as_genes_subset_cosmx_annotated_scvi_v1.h5ad')

adata=sc.read_h5ad('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/neighbors_as_genes_subset_cosmx_annotated_scvi_v1_scanvi_v1.h5ad')

sc.pl.umap(adata, color = "sample")

sc.pl.umap(adata, color = "cluster_labels_annotated")

adata

sc.pl.umap(adata, color = "annotation_post_scanvi70_broad")

sc.pl.umap(adata, color = "leiden_scanvi_0_1")

print(adata.obs['leiden_scanvi_0_1'].value_counts())



adata

adata_2=sc.read_h5ad('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/neighbors_as_genes_subset_cosmx_annotated_scvi_v1_scanvi_v2.h5ad')

sc.pl.umap(adata_2, color = "sample")

adata_3=sc.read_h5ad('')

"""lets try again with clustering the dataframe with neighborfractions"""

new_adata=sc.read_h5ad('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/neighbors_as_genes_all_cosmx.h5ad')

new_adata_subset=new_adata[new_adata.obs['total_neighbors_200']>50]
new_adata_subset

print(new_adata_subset.obs['annotation_post_scanvi70_broad'].value_counts())

import pandas as pd

# Convert the sparse matrix to a dense format and create a DataFrame
gene_expression_df = pd.DataFrame(
    new_adata_subset.X.toarray(),
    index=new_adata_subset.obs_names,
    columns=new_adata_subset.var_names
)

# Print the first few rows to verify
print(gene_expression_df.head())

# List of .obs columns you want to add to the DataFrame
obs_columns = [
    'total_neighbors_200', 'total_neighbors_180', 'total_neighbors_160',
    'total_neighbors_140', 'total_neighbors_120', 'total_neighbors_100',
    'total_neighbors_80', 'total_neighbors_60', 'total_neighbors_40', 'total_neighbors_20'
]

# Add the selected .obs columns to the gene expression DataFrame
gene_expression_df[obs_columns] = new_adata_subset.obs[obs_columns]

# Print the first few rows to verify the updated DataFrame
print(gene_expression_df.head())

# Loop over each suffix and corresponding total_neighbors column
for neighbors in [200, 180, 160, 140, 120, 100, 80, 60, 40, 20]:
    # Generate the suffix and obs column name
    suffix = f'_{neighbors}'
    obs_column = f'total_neighbors_{neighbors}'

    # Get all columns that end with the current suffix
    matching_columns = [col for col in gene_expression_df.columns if col.endswith(suffix)]

    # Perform the division for each matching column
    for col in matching_columns:
        gene_expression_df[col] /= gene_expression_df[obs_column]

# Print the first few rows to verify the updated DataFrame
print(gene_expression_df.head())

gene_expression_df = gene_expression_df.drop(obs_columns, axis=1)

# Print the first few rows to verify that columns are removed
print(gene_expression_df.head())

import numpy as np

# Replace infinite values with 0
gene_expression_df.replace([np.inf, -np.inf], 0, inplace=True)

# Verify changes by displaying the first few rows of the DataFrame
print(gene_expression_df.head())

gene_expression_df=gene_expression_df.fillna(0)

delete_suffixes = ['200', '180', '160', '140', '120', '100']

# Filter out columns that end with the specified suffixes
columns_to_keep = [col for col in gene_expression_df.columns if not any(col.endswith(suffix) for suffix in delete_suffixes)]
filtered_df = gene_expression_df[columns_to_keep]
print(filtered_df.shape)

from sklearn.cluster import MiniBatchKMeans, AgglomerativeClustering
from scipy.cluster.hierarchy import dendrogram, linkage
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
import umap.umap_ as umap
from sklearn.decomposition import PCA

# Assuming your dataframe is already loaded as gene_expression_df
# Normalize the data
scaler = StandardScaler()
features_normalized = scaler.fit_transform(filtered_df)
features_normalized_df = pd.DataFrame(features_normalized, index=filtered_df.index, columns=filtered_df.columns)

features_normalized_df

import anndata
# Convert merged_df to a sparse matrix
sparse_matrix = scipy.sparse.csr_matrix(features_normalized_df.values)

# Create a new AnnData object with the sparse matrix
new_adata = anndata.AnnData(X=sparse_matrix)

# Assign merged_df columns to .var
new_adata.var = pd.DataFrame(index=features_normalized_df.columns)
new_adata.var['feature_names'] = features_normalized_df.columns

# Assign merged_df index to .obs
new_adata.obs = pd.DataFrame(index=features_normalized_df.index)

# Verify the new AnnData object
print(new_adata)

new_adata.obs['sample']=new_adata_subset.obs['sample'].copy()
new_adata.obs['annotation_post_scanvi70_broad']=new_adata_subset.obs['annotation_post_scanvi70_broad'].copy()

sc.pp.pca(new_adata, n_comps=25)

sc.pp.neighbors(new_adata, n_neighbors = 50, n_pcs=25)

sc.tl.umap(new_adata, min_dist=0.3)

sc.pl.umap(new_adata, color = "sample")

sc.tl.leiden(new_adata, resolution = 0.1)

sc.pl.umap(new_adata, color = "leiden")









import anndata
# Convert merged_df to a sparse matrix
sparse_matrix = scipy.sparse.csr_matrix(merged_df.values)

# Create a new AnnData object with the sparse matrix
new_adata = anndata.AnnData(X=sparse_matrix)

# Assign merged_df columns to .var
new_adata.var = pd.DataFrame(index=merged_df.columns)
new_adata.var['feature_names'] = merged_df.columns

# Assign merged_df index to .obs
new_adata.obs = pd.DataFrame(index=merged_df.index)

# Verify the new AnnData object
print(new_adata)

import pandas as pd
import numpy as np
from sklearn.cluster import MiniBatchKMeans, AgglomerativeClustering
from scipy.cluster.hierarchy import dendrogram, linkage
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
import umap.umap_ as umap
from sklearn.decomposition import PCA

# Assuming your dataframe is already loaded as gene_expression_df
# Normalize the data
scaler = StandardScaler()
features_normalized = scaler.fit_transform(filtered_df)

pca = PCA(n_components=25)  # Reduce to 10 dimensions for UMAP input
principal_components = pca.fit_transform(features_normalized)

from sklearn.cluster import MiniBatchKMeans
from sklearn.metrics import silhouette_score

inertias = []
k = range(1, 20)

# dissimilarity would not be defined for a single cluster, thus, minimum number of clusters should be 2
for K in k:
    # Create a MiniBatchKMeans object, arbitrarily choosing 10 clusters
    kmeans = MiniBatchKMeans(n_clusters=K)
    # Fit the model to the data
    kmeans.fit(features_normalized)
    inertias.append(kmeans.inertia_)

plt.plot(k, inertias, 'bx-')
plt.xlabel('Values of K')
plt.ylabel('Inertia')
plt.title('The Elbow Method using Inertia')
plt.show()

kmeans = MiniBatchKMeans(n_clusters=11, random_state=42)
labels = kmeans.fit_predict(principal_components)
filtered_df['cluster_labels'] = labels

filtered_df['annotation']=new_adata_subset.obs['annotation_post_scanvi70_broad']

filtered_df_save=filtered_df.copy()
filtered_df['cluster_labels'].value_counts()

gene_expression_df.head(2)

# Group by 'cluster_labels' and then get the count of each 'annotation' within each cluster
cluster_annotation_counts = filtered_df.groupby('cluster_labels')['annotation'].value_counts()

# If you want a DataFrame with a reset index
cluster_annotation_counts_df = cluster_annotation_counts.reset_index(name='count')

# Print the results
print(cluster_annotation_counts_df)

cluster_annotation_counts_df

import matplotlib.pyplot as plt
# Create a pivot table from the DataFrame
pivot_table = cluster_annotation_counts_df.pivot_table(index='cluster_labels', columns='annotation', values='count', fill_value=0)

# Plotting the data
pivot_table.plot(kind='bar', stacked=True, figsize=(14, 8), cmap='tab20')
plt.title('Stacked Bar Plot of Annotations per Cluster Label')
plt.xlabel('Cluster Labels')
plt.ylabel('Count of Annotations')
plt.xticks(rotation=45)  # Rotate labels for better readability
plt.legend(title='Annotations', bbox_to_anchor=(1.05, 1), loc='upper left')  # Move the legend out of the plot

# Show the plot
plt.tight_layout()  # Adjust subplot params so that the subplot(s) fits in the figure area
plt.show()



gene_expression_df.columns

filtered_df.columns

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
gene_expression_normalized = scaler.fit_transform(filtered_df)

!pip install --quiet hdbscan

import pandas as pd
import hdbscan
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
import numpy as np

# Normalize the data
scaler = StandardScaler()
gene_expression_normalized = scaler.fit_transform(filtered_df)

# Perform HDBSCAN clustering
clusterer = hdbscan.HDBSCAN(min_cluster_size=10000)  # Adjust parameters as needed
cluster_labels = clusterer.fit_predict(gene_expression_normalized)

# Create a DataFrame from the normalized data for plotting
gene_expression_normalized_df = pd.DataFrame(gene_expression_normalized, index=filtered_df.index, columns=filtered_df.columns)
gene_expression_normalized_df['Cluster'] = cluster_labels

# Sort the DataFrame by cluster labels for better visualization
gene_expression_normalized_df = gene_expression_normalized_df.sort_values('Cluster')
print(gene_expression_normalized_df['Cluster'].unique())

gene_expression_normalized_df.to_csv('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/neighborhood_clusters_hdbscan.csv', index=True)

# Calculate cluster centers
unique_labels = np.unique(cluster_labels)
cluster_centers = np.array([gene_expression_normalized_df[cluster_labels == label].mean(axis=0) for label in unique_labels if label != -1])

# Perform hierarchical clustering on the cluster centers
linked = sch.linkage(cluster_centers, method='ward')

# Sort the DataFrame by cluster labels for better visualization
gene_expression_normalized_df_sorted = gene_expression_normalized_df.sort_values('Cluster')

# Compute distances between each point and cluster centers for visualization
distance_matrix = np.zeros((gene_expression_normalized_df.shape[0], cluster_centers.shape[0]))
for i, center in enumerate(cluster_centers):
    distance_matrix[:, i] = np.linalg.norm(gene_expression_normalized_df - center, axis=1)

# Use the cluster center linkage matrix for the clustermap
g = sns.clustermap(gene_expression_normalized_df_sorted.drop('Cluster', axis=1),
                   row_linkage=linked,
                   col_cluster=False,
                   cmap='magma',
                   standard_scale=1,
                   figsize=(50, 25))

# Remove row labels and ticks
g.ax_heatmap.set_yticklabels([])
g.ax_heatmap.set_yticks([])

# Adjust the dendrogram size
dendrogram_ratio = 0.05  # Set this value to your desired dendrogram size ratio
g.ax_row_dendrogram.set_position([0.1, 0.1, dendrogram_ratio, 0.8])
g.ax_heatmap.set_position([0.1 + dendrogram_ratio, 0.1, 0.8 - dendrogram_ratio, 0.8])

# Adjust the colorbar position and size
g.cax.set_position([0.91, 0.1, 0.01, 0.1])

# Save and show the plot
plt.savefig('clustermap_hdbscan_sampled.png', bbox_inches='tight')
plt.show()

# Print number of clusters
num_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)
print(f"Number of clusters: {num_clusters}")











import pandas as pd
import hdbscan
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
import numpy as np

# Assuming your dataframe is already loaded as gene_expression_df

# Randomly downsample to 5,000 rows
sample_size = 10000
gene_expression_sampled = filtered_df.sample(n=sample_size, random_state=42)

# Normalize the data
scaler = StandardScaler()
gene_expression_normalized_sampled = scaler.fit_transform(gene_expression_sampled)

# Perform HDBSCAN clustering
clusterer = hdbscan.HDBSCAN()  # Adjust parameters as needed
cluster_labels = clusterer.fit_predict(gene_expression_normalized_sampled)

# Create a DataFrame from the normalized data for plotting
gene_expression_normalized_df_sampled = pd.DataFrame(gene_expression_normalized_sampled, index=gene_expression_sampled.index, columns=gene_expression_sampled.columns)
gene_expression_normalized_df_sampled['Cluster'] = cluster_labels

# Sort the DataFrame by cluster labels for better visualization
gene_expression_normalized_df_sampled = gene_expression_normalized_df_sampled.sort_values('Cluster')
print(gene_expression_normalized_df_sampled['Cluster'].unique())

# Create a color palette for the clusters
unique_clusters = np.unique(cluster_labels)

# Plot the clustermap
g = sns.heatmap(gene_expression_normalized_df_sampled.drop('Cluster', axis=1),
                   col_cluster=False,
                   cmap='magma',
                   standard_scale=1,
                   figsize=(30, 25))

# Remove row labels and ticks
g.ax_heatmap.set_yticklabels([])
g.ax_heatmap.set_yticks([])

# Adjust the dendrogram size
dendrogram_ratio = 0.05  # Set this value to your desired dendrogram size ratio
g.ax_row_dendrogram.set_position([0.1, 0.1, dendrogram_ratio, 0.8])
g.ax_heatmap.set_position([0.1 + dendrogram_ratio, 0.1, 0.8 - dendrogram_ratio, 0.8])

# Adjust the colorbar position and size
g.cax.set_position([0.91, 0.1, 0.01, 0.1])

# Save and show the plot
plt.savefig('clustermap_hdbscan_sampled.png', bbox_inches='tight')
plt.show()

# Print number of clusters
num_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)
print(f"Number of clusters: {num_clusters}")





!pip install fastcluster

import scipy.cluster.hierarchy as sch
import matplotlib.pyplot as plt

sample_size = 100000
gene_expression_sampled = gene_expression_df.sample(n=sample_size, random_state=42)
gene_expression_sampled_normalized = scaler.fit_transform(gene_expression_sampled)
# Calculate the linkage matrix
linked = sch.linkage(gene_expression_sampled_normalized, method='ward')
g = sns.clustermap(gene_expression_sampled_normalized, row_linkage=linked, col_cluster=False, cmap='magma', standard_scale=1, figsize=(50, 25))

# Remove row labels and ticks
g.ax_heatmap.set_yticklabels([])
g.ax_heatmap.set_yticks([])
dendrogram_ratio = 0.05  # Set this value to your desired dendrogram size ratio
g.ax_row_dendrogram.set_position([0.1, 0.1, dendrogram_ratio, 0.8])
g.ax_heatmap.set_position([0.1 + dendrogram_ratio, 0.1, 0.8 - dendrogram_ratio, 0.8])
g.cax.set_position([0.91, 0.1, 0.01, 0.1])
# Show the plot
plt.savefig('clustermap_neighbor.png', bbox_inches='tight')
plt.show()

import scipy.cluster.hierarchy as sch
import matplotlib.pyplot as plt

# Calculate the linkage matrix
#linked = sch.linkage(gene_expression_normalized, method='ward')
g = sns.clustermap(gene_expression_sampled_normalized, row_linkage=linked, col_cluster=False, cmap='magma', standard_scale=1, figsize=(50, 25))

# Remove row labels and ticks
g.ax_heatmap.set_yticklabels([])
g.ax_heatmap.set_yticks([])
dendrogram_ratio = 0.05  # Set this value to your desired dendrogram size ratio
g.ax_row_dendrogram.set_position([0.1, 0.1, dendrogram_ratio, 0.8])
g.ax_heatmap.set_position([0.1 + dendrogram_ratio, 0.1, 0.8 - dendrogram_ratio, 0.8])
g.cax.set_position([0.91, 0.1, 0.01, 0.1])
# Show the plot
plt.savefig('clustermap_neighbor_high_resolution.png', dpi=450, bbox_inches='tight')
plt.show()

# Define the distance threshold
distance_threshold = 50  # Set this to your desired threshold

# Use fcluster to get cluster labels based on the distance threshold
cluster_labels = sch.fcluster(linked, t=distance_threshold, criterion='distance')

# Count the number of unique clusters formed
num_clusters = len(np.unique(cluster_labels))
print(f"Number of clusters above distance threshold {distance_threshold}: {num_clusters}")

!pip install --quiet hdbscan

import pandas as pd
import hdbscan
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
import numpy as np
import time

# Assuming your dataframe is already loaded as gene_expression_df

# Benchmarking on a smaller subset
sample_size = 5000  # Adjust this for benchmarking
gene_expression_sampled = gene_expression_df.sample(n=sample_size, random_state=42)

# Normalize the data
scaler = StandardScaler()
gene_expression_normalized_sampled = scaler.fit_transform(gene_expression_sampled)

# Perform HDBSCAN clustering and time it
start_time = time.time()
clusterer = hdbscan.HDBSCAN(min_cluster_size=25000)  # Adjust parameters as needed
cluster_labels = clusterer.fit_predict(gene_expression_normalized_sampled)
end_time = time.time()

# Print the time taken for clustering
print(f"Time taken for HDBSCAN clustering on {sample_size} rows: {end_time - start_time} seconds")

# Extrapolate time for full dataset (1.3 million rows)
full_dataset_time_estimate = (end_time - start_time) * (1.3e6 / sample_size)
print(f"Estimated time for HDBSCAN clustering on 1.3 million rows: {full_dataset_time_estimate / 60} minutes")

import pandas as pd
import hdbscan
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
import numpy as np

# Assuming your dataframe is already loaded as gene_expression_df

# Randomly downsample to 5,000 rows
sample_size = 10000
gene_expression_sampled = gene_expression_df.sample(n=sample_size, random_state=42)

# Normalize the data
scaler = StandardScaler()
gene_expression_normalized_sampled = scaler.fit_transform(gene_expression_sampled)

# Perform HDBSCAN clustering
clusterer = hdbscan.HDBSCAN()  # Adjust parameters as needed
cluster_labels = clusterer.fit_predict(gene_expression_normalized_sampled)

# Create a DataFrame from the normalized data for plotting
gene_expression_normalized_df_sampled = pd.DataFrame(gene_expression_normalized_sampled, index=gene_expression_sampled.index, columns=gene_expression_sampled.columns)
gene_expression_normalized_df_sampled['Cluster'] = cluster_labels

# Sort the DataFrame by cluster labels for better visualization
gene_expression_normalized_df_sampled = gene_expression_normalized_df_sampled.sort_values('Cluster')

# Create a color palette for the clusters
unique_clusters = np.unique(cluster_labels)
palette = sns.color_palette("viridis", len(unique_clusters))
lut = dict(zip(unique_clusters, palette))
row_colors = gene_expression_normalized_df_sampled['Cluster'].map(lut)

# Plot the clustermap
g = sns.clustermap(gene_expression_normalized_df_sampled.drop('Cluster', axis=1),
                   col_cluster=False,
                   cmap='magma',
                   standard_scale=1,
                   figsize=(50, 25))

# Remove row labels and ticks
g.ax_heatmap.set_yticklabels([])
g.ax_heatmap.set_yticks([])

# Adjust the dendrogram size
dendrogram_ratio = 0.05  # Set this value to your desired dendrogram size ratio
g.ax_row_dendrogram.set_position([0.1, 0.1, dendrogram_ratio, 0.8])
g.ax_heatmap.set_position([0.1 + dendrogram_ratio, 0.1, 0.8 - dendrogram_ratio, 0.8])

# Adjust the colorbar position and size
g.cax.set_position([0.91, 0.1, 0.01, 0.1])

# Save and show the plot
plt.savefig('clustermap_hdbscan_sampled.png', bbox_inches='tight')
plt.show()

# Print number of clusters
num_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)
print(f"Number of clusters: {num_clusters}")

import pandas as pd
import hdbscan
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
import numpy as np
import scipy.cluster.hierarchy as sch

# Assuming your dataframe is already loaded as gene_expression_df

# Randomly downsample to 5,000 rows
sample_size = 10000
gene_expression_sampled = gene_expression_df.sample(n=sample_size, random_state=42)

# Normalize the data
scaler = StandardScaler()
gene_expression_normalized_sampled = scaler.fit_transform(gene_expression_sampled)

# Perform HDBSCAN clustering
clusterer = hdbscan.HDBSCAN()  # Adjust parameters as needed
cluster_labels = clusterer.fit_predict(gene_expression_normalized_sampled)

# Create a DataFrame from the normalized data for plotting
gene_expression_normalized_df_sampled = pd.DataFrame(gene_expression_normalized_sampled, index=gene_expression_sampled.index, columns=gene_expression_sampled.columns)
gene_expression_normalized_df_sampled['Cluster'] = cluster_labels
# Calculate cluster centers
unique_labels = np.unique(cluster_labels)
cluster_centers = np.array([gene_expression_normalized_sampled[cluster_labels == label].mean(axis=0) for label in unique_labels if label != -1])

# Perform hierarchical clustering on the cluster centers
linked = sch.linkage(cluster_centers, method='ward')

# Plot the clustermap
g = sns.clustermap(gene_expression_normalized_df_sampled.drop('Cluster', axis=1),
                   row_linkage=linked,
                   col_cluster=False,
                   cmap='magma',
                   standard_scale=1,
                   figsize=(50, 25))

# Remove row labels and ticks
g.ax_heatmap.set_yticklabels([])
g.ax_heatmap.set_yticks([])

# Adjust the dendrogram size
dendrogram_ratio = 0.05  # Set this value to your desired dendrogram size ratio
g.ax_row_dendrogram.set_position([0.1, 0.1, dendrogram_ratio, 0.8])
g.ax_heatmap.set_position([0.1 + dendrogram_ratio, 0.1, 0.8 - dendrogram_ratio, 0.8])

# Adjust the colorbar position and size
g.cax.set_position([0.91, 0.1, 0.01, 0.1])

# Save and show the plot
plt.savefig('clustermap_hdbscan_sampled.png', bbox_inches='tight')
plt.show()

# Print number of clusters
num_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)
print(f"Number of clusters: {num_clusters}")

import pandas as pd
import hdbscan
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
import numpy as np
import scipy.cluster.hierarchy as sch

# Assuming your dataframe is already loaded as gene_expression_df

# Randomly downsample to 5,000 rows
sample_size = 100000
gene_expression_sampled = gene_expression_df.sample(n=sample_size, random_state=42)

# Normalize the data
scaler = StandardScaler()
gene_expression_normalized_sampled = scaler.fit_transform(gene_expression_sampled)

# Perform HDBSCAN clustering
clusterer = hdbscan.HDBSCAN()  # Adjust parameters as needed
cluster_labels = clusterer.fit_predict(gene_expression_normalized_sampled)

# Create a DataFrame from the normalized data for plotting
gene_expression_normalized_df_sampled = pd.DataFrame(gene_expression_normalized_sampled, index=gene_expression_sampled.index, columns=gene_expression_sampled.columns)
gene_expression_normalized_df_sampled['Cluster'] = cluster_labels

# Calculate cluster centers
unique_labels = np.unique(cluster_labels)
cluster_centers = np.array([gene_expression_normalized_sampled[cluster_labels == label].mean(axis=0) for label in unique_labels if label != -1])

# Perform hierarchical clustering on the cluster centers
linked = sch.linkage(cluster_centers, method='ward')



# Sort the DataFrame by cluster labels for better visualization
gene_expression_normalized_df_sampled_sorted = gene_expression_normalized_df_sampled.sort_values('Cluster')

# Compute distances between each point and cluster centers for visualization
distance_matrix = np.zeros((gene_expression_normalized_sampled.shape[0], cluster_centers.shape[0]))
for i, center in enumerate(cluster_centers):
    distance_matrix[:, i] = np.linalg.norm(gene_expression_normalized_sampled - center, axis=1)

# Use the cluster center linkage matrix for the clustermap
g = sns.clustermap(gene_expression_normalized_df_sampled_sorted.drop('Cluster', axis=1),
                   row_linkage=linked,
                   col_cluster=False,
                   cmap='magma',
                   standard_scale=1,
                   figsize=(50, 25))

# Remove row labels and ticks
g.ax_heatmap.set_yticklabels([])
g.ax_heatmap.set_yticks([])

# Adjust the dendrogram size
dendrogram_ratio = 0.05  # Set this value to your desired dendrogram size ratio
g.ax_row_dendrogram.set_position([0.1, 0.1, dendrogram_ratio, 0.8])
g.ax_heatmap.set_position([0.1 + dendrogram_ratio, 0.1, 0.8 - dendrogram_ratio, 0.8])

# Adjust the colorbar position and size
g.cax.set_position([0.91, 0.1, 0.01, 0.1])

# Save and show the plot
plt.savefig('clustermap_hdbscan_sampled.png', bbox_inches='tight')
plt.show()

# Print number of clusters
num_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)
print(f"Number of clusters: {num_clusters}")







import pandas as pd
import hdbscan
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
import numpy as np
import time

# Assuming your dataframe is already loaded as gene_expression_df

# Normalize the data
scaler = StandardScaler()
gene_expression_normalized = scaler.fit_transform(gene_expression_df)

# Perform HDBSCAN clustering and time it
start_time = time.time()
clusterer = hdbscan.HDBSCAN(min_cluster_size=50)  # Adjust parameters as needed
cluster_labels = clusterer.fit_predict(gene_expression_normalized)
end_time = time.time()

# Print the time taken for clustering
print(f"Time taken for HDBSCAN clustering on full dataset: {end_time - start_time} seconds")

# Create a DataFrame from the normalized data for plotting
gene_expression_normalized_df = pd.DataFrame(gene_expression_normalized, index=gene_expression_df.index, columns=gene_expression_df.columns)
gene_expression_normalized_df['Cluster'] = cluster_labels

# Plot the clustermap
sns.clustermap(gene_expression_normalized_df.drop('Cluster', axis=1), row_colors=sns.color_palette("viridis", len(set(cluster_labels))), cmap='magma', figsize=(50, 25))
plt.savefig('clustermap_hdbscan.png', bbox_inches='tight')
plt.show()

# Print number of clusters
num_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)
print(f"Number of clusters: {num_clusters}")













import pandas as pd
import scipy.cluster.hierarchy as sch
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import numpy as np

# Assuming your dataframe is already loaded as gene_expression_df

# Randomly downsample to 10,000 rows
sample_size = 10000
gene_expression_sampled = gene_expression_df.sample(n=sample_size, random_state=42)

# Normalize the data
scaler = StandardScaler()
gene_expression_normalized = scaler.fit_transform(gene_expression_sampled)

# Alternatively, using seaborn's clustermap for better visualization
sns.clustermap(pd.DataFrame(gene_expression_sampled, index=gene_expression_sampled.index),
               method='ward', cmap='magma', standard_scale=1,col_cluster=False)
plt.show()

import pandas as pd
import scipy.cluster.hierarchy as sch
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
import numpy as np

# Assuming your dataframe is already loaded as gene_expression_df

# Randomly downsample to 20,000 rows
sample_size = 5000
gene_expression_sampled = gene_expression_df.sample(n=sample_size, random_state=42)

# Normalize the data
scaler = StandardScaler()
gene_expression_normalized = scaler.fit_transform(gene_expression_sampled)

# Create a DataFrame from the normalized data for plotting
gene_expression_normalized_df = pd.DataFrame(gene_expression_normalized, index=gene_expression_sampled.index, columns=gene_expression_sampled.columns)

# Calculate the linkage matrix
linked = sch.linkage(gene_expression_normalized_df, method='ward')

# Plot the clustermap with custom linkage matrix
g = sns.clustermap(gene_expression_normalized_df, row_linkage=linked, col_cluster=False, cmap='magma', standard_scale=1)

# Customize the dendrogram: Set a distance threshold
dendro = sch.dendrogram(linked, color_threshold=100, ax=g.ax_row_dendrogram, above_threshold_color='grey')

# Show the plot
plt.show()

# Assuming your dataframe is already loaded as gene_expression_df

# Randomly downsample to 20,000 rows
sample_size = 5000
gene_expression_sampled = gene_expression_df.sample(n=sample_size, random_state=42)

# Normalize the data
scaler = StandardScaler()
gene_expression_normalized = scaler.fit_transform(gene_expression_sampled)

# Compute the linkage matrix
linked = sch.linkage(gene_expression_normalized, method='ward')

# Define the distance threshold
distance_threshold = 10000

# Function to truncate dendrogram based on distance threshold
def truncate_dendrogram(linked, distance_threshold):
    # Identify clusters that do not meet the distance threshold
    clusters = sch.fcluster(linked, distance_threshold, criterion='distance')
    unique_clusters = np.unique(clusters)

    # Create a new linkage matrix where all distances above the threshold are set to the threshold
    new_linked = linked.copy()
    for i in range(len(linked)):
        if linked[i, 2] > distance_threshold:
            new_linked[i, 2] = distance_threshold

    return new_linked

# Truncate the linkage matrix
truncated_linked = truncate_dendrogram(linked, distance_threshold)

# Create a DataFrame from the normalized data for plotting
gene_expression_normalized_df = pd.DataFrame(gene_expression_normalized, index=gene_expression_sampled.index, columns=gene_expression_sampled.columns)

# Plot the clustermap with the truncated linkage matrix
sns.clustermap(gene_expression_normalized_df, row_linkage=truncated_linked, col_cluster=False, cmap='magma', standard_scale=1, figsize=(50, 25))

plt.show()

g = sns.clustermap(gene_expression_normalized_df, row_linkage=linked, col_cluster=False, cmap='magma', standard_scale=1, figsize=(50, 25))

# Remove row labels and ticks
g.ax_heatmap.set_yticklabels([])
g.ax_heatmap.set_yticks([])
dendrogram_ratio = 0.05  # Set this value to your desired dendrogram size ratio
g.ax_row_dendrogram.set_position([0.1, 0.1, dendrogram_ratio, 0.8])
g.ax_heatmap.set_position([0.1 + dendrogram_ratio, 0.1, 0.8 - dendrogram_ratio, 0.8])
g.cax.set_position([0.91, 0.1, 0.01, 0.1])
# Show the plot
plt.show()

# Compute the linkage matrix
linked = sch.linkage(gene_expression_normalized, method='ward')

# Define the desired number of clusters
num_clusters = 10

# Use fcluster to get cluster labels
cluster_labels = sch.fcluster(linked, num_clusters, criterion='maxclust')

# Create a DataFrame from the normalized data and add cluster labels
gene_expression_normalized_df = pd.DataFrame(gene_expression_normalized, index=gene_expression_sampled.index, columns=gene_expression_sampled.columns)
gene_expression_normalized_df['Cluster'] = cluster_labels

# Create a color palette for the clusters
cluster_palette = sns.color_palette('tab10', num_clusters)
cluster_colors = [cluster_palette[label - 1] for label in cluster_labels]

# Convert to a pandas Series for use in clustermap
row_colors = pd.Series(cluster_colors, index=gene_expression_normalized_df.index)

# Sort the DataFrame by cluster labels
gene_expression_normalized_df_sorted = gene_expression_normalized_df.sort_values('Cluster')
row_colors_sorted = row_colors.loc[gene_expression_normalized_df_sorted.index]

gene_expression_normalized_df_sorted

# Plot the clustermap
sns.clustermap(gene_expression_normalized_df_sorted.drop('Cluster', axis=1),
               row_cluster=True, col_cluster=False, cmap='viridis',
               row_colors=row_colors, standard_scale=1)

plt.show()



gene_expression_sampled

###this is not going to work
import pandas as pd
import scipy.cluster.hierarchy as sch
import seaborn as sns
import matplotlib.pyplot as plt

# Assuming your dataframe is already loaded as gene_expression_df
# Normalize the data if necessary
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
gene_expression_normalized = scaler.fit_transform(gene_expression_df)

# Perform hierarchical clustering
linked = sch.linkage(gene_expression_normalized, method='ward')

# Plot the dendrogram
plt.figure(figsize=(10, 7))
sch.dendrogram(linked,
               orientation='top',
               labels=gene_expression_df.index,
               distance_sort='descending',
               show_leaf_counts=True)
plt.show()

# Alternatively, using seaborn's clustermap for better visualization
sns.clustermap(gene_expression_df, method='ward', cmap='viridis', standard_scale=1)
plt.show()

import pandas as pd
import scipy.cluster.hierarchy as sch
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

# Assuming your dataframe is already loaded as gene_expression_df

# Normalize the data
scaler = StandardScaler()
gene_expression_normalized = scaler.fit_transform(gene_expression_df)

# Perform PCA for dimensionality reduction
n_components = 50  # Adjust the number of components as necessary
pca = PCA(n_components=n_components)
gene_expression_pca = pca.fit_transform(gene_expression_normalized)

# Perform hierarchical clustering on the PCA-reduced data
linked = sch.linkage(gene_expression_pca, method='ward')

# Plot the dendrogram
plt.figure(figsize=(10, 7))
sch.dendrogram(linked,
               orientation='top',
               labels=gene_expression_df.index,
               distance_sort='descending',
               show_leaf_counts=True)
plt.show()

# Alternatively, using seaborn's clustermap for better visualization
sns.clustermap(pd.DataFrame(gene_expression_pca, index=gene_expression_df.index),
               method='ward', cmap='viridis', standard_scale=1)
plt.show()

import pandas as pd
import scipy.cluster.hierarchy as sch
import seaborn as sns
import matplotlib.pyplot as plt

# Perform hierarchical clustering
linked = sch.linkage(gene_expression_df, method='ward')

# Plot the dendrogram
plt.figure(figsize=(10, 7))
sch.dendrogram(linked,
               orientation='top',
               labels=gene_expression_df.index,
               distance_sort='descending',
               show_leaf_counts=True)
plt.show()

# Alternatively, using seaborn's clustermap for better visualization
sns.clustermap(gene_expression_df, method='ward', cmap='viridis', standard_scale=1)
plt.show()