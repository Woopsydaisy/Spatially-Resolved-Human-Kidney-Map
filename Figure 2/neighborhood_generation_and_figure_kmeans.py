# -*- coding: utf-8 -*-
"""Neighborhood generation and figure kmeans.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bc1Pd1khqQB-YZuNWJc1hdhqtIOQXqPQ
"""

!pip --quiet install scanpy
!pip --quiet install leidenalg
#!pip --quiet install squidpy

from google.colab import drive
drive.mount('/content/drive')

import csv
import anndata as ad
import gzip
import os
import scipy.io
import scanpy as sc
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from google.colab import drive
import leidenalg as la
from pathlib import Path
#import squidpy as sq

adata = sc.read_h5ad('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/all_Cosmx_cleaned_neighbored.h5ad')

subset_mc1=adata[adata.obs['annotation_post_scanvi70_broad']=="MC1"]
subset_mc1.obs['type']=subset_mc1.obs['type'].replace('Pediatric', 'Healthy')
subset_mc1.X = subset_mc1.layers["counts"]
sc.pp.log1p(subset_mc1)
sc.tl.rank_genes_groups(subset_mc1, groupby='type', method='wilcoxon', pts = True, reference='Healthy')
sc.pl.rank_genes_groups(subset_mc1, n_genes=25, sharey=False)

adata

adata.obs['cluster_labels_annotated_coarse_coarse'].value_counts()

df= pd.DataFrame(adata.obs['cluster_labels_annotated_coarse_coarse'])
df

df=df[df['cluster_labels_annotated_coarse_coarse']!='outlier']
df

df.to_csv('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/cluster_niche_cell_ids.csv', index=True)

df.to_csv('cluster_niche_cell_ids.csv', index=True)

df = pd.DataFrame({
    'cluster_labels': adata.obs['cluster_labels_annotated_coarse_coarse'],
    'type': adata.obs['type'],
    'sample':adata.obs['sample']
}, index=adata.obs_names)
df

df=df[df['cluster_labels']!='outlier']

df['type']=df['type'].replace('Pediatric', 'Healthy')

df['type'].value_counts()

df=df[df['type']!='Diabetes']
df['sample'].value_counts()

df=df[df['sample']!='HK2874']

df['sample'].value_counts()

df

df=df.drop('sample', axis=1)

df['type'] = df['type'].cat.remove_unused_categories()

df['cluster_labels'] = df['cluster_labels'].cat.remove_unused_categories()

# Step 1: Group by 'type' and 'cluster_labels' and count the occurrences
grouped_counts = df.groupby(['type', 'cluster_labels']).size().reset_index(name='count')

# Step 2: Calculate the total number of 'cluster_labels' for each 'type'
total_counts = df.groupby('type').size().reset_index(name='total_count')

# Step 3: Merge the total counts back to the grouped counts
merged_counts = pd.merge(grouped_counts, total_counts, on='type')

# Step 4: Normalize the counts by the total counts for each 'type'
merged_counts['normalized_count'] = merged_counts['count'] / merged_counts['total_count']

# Step 5: Display the result
print(merged_counts)

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming 'merged_counts' is your DataFrame
# Split the DataFrame into Healthy and Disease
healthy_df = merged_counts[merged_counts['type'] == 'Healthy']
disease_df = merged_counts[merged_counts['type'] == 'Disease']

# Merge Healthy and Disease dataframes on 'cluster_labels'
merged = pd.merge(healthy_df, disease_df, on='cluster_labels', suffixes=('_healthy', '_disease'))

# Calculate the relative increase
merged['relative_increase'] = (merged['normalized_count_disease'] - merged['normalized_count_healthy']) / merged['normalized_count_healthy']
heatmap_data = merged.set_index('cluster_labels')[['relative_increase']]
heatmap_data_rounded = heatmap_data.round(0).astype(int)
# Plotting the heatmap
plt.figure(figsize=(6, 2.5))
ax = sns.heatmap(heatmap_data_rounded.T, annot=True, cmap='magma',fmt='d',cbar=False, vmax=30)
#ax.set_xticks([])
ax.tick_params(axis='y', which='both', left=False, right=False)
# Add labels and title
plt.xlabel('')
plt.ylabel('')
plt.title('Increase in DKD')

# Save the plot
plt.tight_layout()
plt.savefig('relative_increase_heatmap.png', dpi=450)
plt.show()

adata_subset=adata[adata.obs['cluster_labels_annotated_coarse_coarse']!='outlier']

import matplotlib.pyplot as plt
import seaborn as sns

# Assuming new_adata_subset is your AnnData object
# Extract the relevant columns
samples = adata_subset.obs['sample']
cluster_labels = adata_subset.obs['cluster_labels_annotated_coarse_coarse']

# Create a DataFrame from the relevant columns
data = pd.DataFrame({'sample': samples, 'cluster_labels_annotated_coarse_coarse': cluster_labels})

# Group by 'sample' and 'cluster_labels_annotated' and count the occurrences
grouped_data = data.groupby(['sample', 'cluster_labels_annotated_coarse_coarse']).size().unstack(fill_value=0)
desired_order = [
    "Pediatric1", "HK2753", "HK3039", "HK3106", "HK3531", "HK3531_2",
    "HK3063", "HK3542", "HK2874", "HK2695", "HK3035", "HK3035_2",
     "HK2841", "HK2873","HK2844", "HK2844_2"
]

grouped_data=grouped_data.reindex(desired_order)
sample_totals = data['sample'].value_counts()
sample_totals=sample_totals.reindex(desired_order)
normalized_count_df = grouped_data.div(sample_totals, axis='index')
ax = normalized_count_df.plot(kind='bar', stacked=True, figsize=(6, 7), colormap='tab10')

# Set plot labels and title
plt.xlabel('')
plt.ylabel('Relative Amount')
plt.title('Sample Niche Composition')
plt.legend(title='', bbox_to_anchor=((0.5, -0.2)), loc='upper center',ncol=3, frameon=False)
plt.tick_params(axis='x', which='both', length=0)
ax.spines['top'].set_visible(False)
ax.spines['right'].set_visible(False)
plt.tight_layout()
plt.savefig('Kindey Niches Samples.png', dpi=450, bbox_inches='tight')
plt.show()

heatmap_data_rounded

merged['relative_increase']

merged

"""lets try again with clustering the dataframe with neighborfractions"""

new_adata=sc.read_h5ad('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/neighbors_as_genes_all_cosmx.h5ad')

#new_adata.obs['cluster_labels_annotated_coarse_coarse']=adata.obs['cluster_labels_annotated_coarse_coarse'].copy()

#new_adata.write('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/neighbors_as_genes_all_cosmx.h5ad')

new_adata_subset=new_adata[new_adata.obs['total_neighbors_200']>50]
new_adata_subset

subset_mc=new_adata_subset[new_adata_subset.obs['annotation_post_scanvi70_broad']=="MC1"]
print(subset_mc.obs['sample'].value_counts())

print(new_adata_subset.obs['annotation_post_scanvi70_broad'].value_counts())

import pandas as pd

# Convert the sparse matrix to a dense format and create a DataFrame
gene_expression_df = pd.DataFrame(
    new_adata_subset.X.toarray(),
    index=new_adata_subset.obs_names,
    columns=new_adata_subset.var_names
)

# Print the first few rows to verify
print(gene_expression_df.head())

# List of .obs columns you want to add to the DataFrame
obs_columns = [
    'total_neighbors_200', 'total_neighbors_180', 'total_neighbors_160',
    'total_neighbors_140', 'total_neighbors_120', 'total_neighbors_100',
    'total_neighbors_80', 'total_neighbors_60', 'total_neighbors_40', 'total_neighbors_20'
]

# Add the selected .obs columns to the gene expression DataFrame
gene_expression_df[obs_columns] = new_adata_subset.obs[obs_columns]

# Print the first few rows to verify the updated DataFrame
print(gene_expression_df.head())

# Loop over each suffix and corresponding total_neighbors column
for neighbors in [200, 180, 160, 140, 120, 100, 80, 60, 40, 20]:
    # Generate the suffix and obs column name
    suffix = f'_{neighbors}'
    obs_column = f'total_neighbors_{neighbors}'

    # Get all columns that end with the current suffix
    matching_columns = [col for col in gene_expression_df.columns if col.endswith(suffix)]

    # Perform the division for each matching column
    for col in matching_columns:
        gene_expression_df[col] /= gene_expression_df[obs_column]

# Print the first few rows to verify the updated DataFrame
print(gene_expression_df.head())

gene_expression_df = gene_expression_df.drop(obs_columns, axis=1)

# Print the first few rows to verify that columns are removed
print(gene_expression_df.head())

import numpy as np

# Replace infinite values with 0
gene_expression_df.replace([np.inf, -np.inf], 0, inplace=True)

# Verify changes by displaying the first few rows of the DataFrame
print(gene_expression_df.head())

gene_expression_df=gene_expression_df.fillna(0)

gene_expression_df

from scipy import sparse
sparse_matrix = scipy.sparse.csr_matrix(gene_expression_df.values)

new_adata_subset.X=sparse_matrix

print(new_adata_subset.X)

sc.tl.rank_genes_groups(new_adata_subset, groupby='cluster_labels_annotated_coarse_coarse', method='wilcoxon', pts = True)
sc.pl.rank_genes_groups(new_adata_subset, n_genes=25, sharey=False)

markers = [ "iPT_20", "iPT_40"]

sc.pl.dotplot(new_adata_subset, markers, groupby='cluster_labels_annotated_coarse_coarse',cmap='Blues', log = False)

new_adata_subset.obs['type']=new_adata_subset.obs['type'].replace('Pediatric', 'Healthy')

for niche in new_adata_subset.obs['cluster_labels_annotated_coarse_coarse'].unique():
  subset=new_adata_subset[new_adata_subset.obs['cluster_labels_annotated_coarse_coarse']==niche]
  print(niche)
  sc.tl.rank_genes_groups(subset, groupby='type', method='wilcoxon', pts = True)
  sc.pl.rank_genes_groups(subset, n_genes=25, sharey=False)

glom_subset=new_adata_subset[new_adata_subset.obs['cluster_labels_annotated_coarse_coarse']=='Glomerular Niche']
sc.tl.rank_genes_groups(glom_subset, groupby='type', method='wilcoxon', pts = True)
sc.pl.rank_genes_groups(glom_subset, n_genes=35, sharey=False)

de_results = glom_subset.uns['rank_genes_groups']
for group in de_results['names'].dtype.names:
        gene_names = de_results['names'][group]
        gene_scores = de_results['scores'][group]
        gene_logfoldchanges = de_results['logfoldchanges'][group]
        gene_pvals = de_results['pvals'][group]
        gene_pvals_adj = de_results['pvals_adj'][group]

        # Create a DataFrame
        df = pd.DataFrame({
            'Gene': gene_names,
            'Score': gene_scores,
            'LogFoldChange': gene_logfoldchanges,
            'pValue': gene_pvals,
            'pValue_adj': gene_pvals_adj
        })
        print(df.head(40))

glom_subset=glom_subset[glom_subset.obs['type']!='Diabetes']

glom_subset=glom_subset[glom_subset.obs['sample']!='HK2874']
print(glom_subset.obs['sample'].value_counts())

sc.tl.rank_genes_groups(glom_subset, groupby='type', method='wilcoxon', pts = True)
sc.pl.rank_genes_groups(glom_subset, n_genes=35, sharey=False)

de_results = glom_subset.uns['rank_genes_groups']
for group in de_results['names'].dtype.names:
        gene_names = de_results['names'][group]
        gene_scores = de_results['scores'][group]
        gene_logfoldchanges = de_results['logfoldchanges'][group]
        gene_pvals = de_results['pvals'][group]
        gene_pvals_adj = de_results['pvals_adj'][group]

        # Create a DataFrame
        df = pd.DataFrame({
            'Gene': gene_names,
            'Score': gene_scores,
            'LogFoldChange': gene_logfoldchanges,
            'pValue': gene_pvals,
            'pValue_adj': gene_pvals_adj
        })
        df=df[df['LogFoldChange']>1]
        df = df[~df['Gene'].str.endswith('_200')]
        df = df[~df['Gene'].str.endswith('_180')]
        df = df[~df['Gene'].str.endswith('_160')]
        df = df[~df['Gene'].str.endswith('_140')]
        df = df[~df['Gene'].str.endswith('_120')]
        df = df[~df['Gene'].str.endswith('_100')]
        print(df.head(40))

markers = [ "Immune_20","Fibroblast_20","CNT_20", "IC A_20","PC_20",  "Immune_40","Fibroblast_40", "MC1_40"]

sc.pl.dotplot(glom_subset, markers, groupby='type',cmap='Blues', log = False)

new_adata_subset

delete_suffixes = ['200', '180', '160', '140', '120', '100']

# Filter out columns that end with the specified suffixes
columns_to_keep = [col for col in gene_expression_df.columns if not any(col.endswith(suffix) for suffix in delete_suffixes)]
filtered_df = gene_expression_df[columns_to_keep]
print(filtered_df.shape)

import pandas as pd
import numpy as np
from sklearn.cluster import MiniBatchKMeans, AgglomerativeClustering
from scipy.cluster.hierarchy import dendrogram, linkage
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
import umap.umap_ as umap
from sklearn.decomposition import PCA

# Assuming your dataframe is already loaded as gene_expression_df
# Normalize the data
scaler = StandardScaler()
features_normalized = scaler.fit_transform(filtered_df)

pca = PCA(n_components=25)  # Reduce to 10 dimensions for UMAP input
principal_components = pca.fit_transform(features_normalized)

# Perform MiniBatchKMeans clustering
kmeans = MiniBatchKMeans(n_clusters=15, random_state=42)
labels = kmeans.fit_predict(principal_components)
filtered_df['cluster_labels'] = labels

principal_components

from sklearn.cluster import MiniBatchKMeans
from sklearn.metrics import silhouette_score

inertias = []
k = range(1, 20)

# dissimilarity would not be defined for a single cluster, thus, minimum number of clusters should be 2
for K in k:
    # Create a MiniBatchKMeans object, arbitrarily choosing 10 clusters
    kmeans = MiniBatchKMeans(n_clusters=K)
    # Fit the model to the data
    kmeans.fit(principal_components)
    inertias.append(kmeans.inertia_)

plt.plot(k, inertias, 'bx-')
plt.xlabel('Values of K')
plt.ylabel('Inertia')
plt.title('The Elbow Method using Inertia')
plt.show()

new_adata_subset.obs['cluster_labels']=filtered_df['cluster_labels'].copy()

print(new_adata_subset.obs['cluster_labels'])

new_adata_subset.obs['cluster_labels']=new_adata_subset.obs['cluster_labels'].astype('category')

sc.tl.rank_genes_groups(new_adata_subset, groupby='cluster_labels', method='wilcoxon', pts = True)
sc.pl.rank_genes_groups(new_adata_subset, n_genes=25, sharey=False)

new_adata_subset.obs["cluster_labels"].value_counts()
cell_identities = {0: 'PC Neighborhood', 1: 'PT Neighborhood 1', 2: 'CNT Neighborhood 1', 3: 'Glomerular Neighborhood',
                   4: 'DTL_ATL Neighborhood', 5: 'Immune Neighborhood', 6: 'DCT Neighborhood', 7: 'iPT Neighborhood',
                   8: 'Fibroblast Neighborhood', 9: 'TAL Neighborhood', 10:'Vascular Neighborhood', 11:'Glomerular Neighborhood 2',
                   12:'iTAL Neighborhood', 13:'IC Neighborhood', 14:'Vascular Neighborhood 2'}
new_adata_subset.obs["cluster_labels_annotated"] = new_adata_subset.obs['cluster_labels'].map(cell_identities).astype('category')

import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

# Assuming new_adata_subset is your AnnData object and it has the annotations and cluster labels
# Extract the relevant columns
annotations = new_adata_subset.obs["annotation_post_scanvi70_broad"]
cluster_labels = new_adata_subset.obs["cluster_labels_annotated"]

# Create a contingency table (cross-tabulation)
contingency_table = pd.crosstab(cluster_labels, annotations)

# Normalize the data (optional)
# normalized_table = contingency_table.div(contingency_table.sum(axis=1), axis=0)

# Plot the heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(contingency_table, annot=True, fmt="d", cmap="viridis")
plt.title('Matrix Plot of Cluster Labels and Annotations')
plt.xlabel('Cluster Labels')
plt.ylabel('Annotations')
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

# Assuming new_adata_subset is your AnnData object and it has the annotations and cluster labels
# Extract the relevant columns
annotations = new_adata_subset.obs["annotation_post_scanvi70_broad"]
cluster_labels = new_adata_subset.obs["cluster_labels_annotated"]

# Create a contingency table (cross-tabulation)
contingency_table = pd.crosstab(cluster_labels, annotations)

# Normalize the data across the columns (annotations)
normalized_table = contingency_table.div(contingency_table.sum(axis=0), axis=1)

# Plot the heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(normalized_table, annot=False, fmt=".2f", cmap="plasma")
plt.title('Normalized Matrix Plot of Cluster Labels and Annotations')
plt.xlabel('Cluster Labels')
plt.ylabel('Annotations')
plt.show()

normalized_table

print(scaled_table.T.columns)

import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.preprocessing import MinMaxScaler

# Assuming new_adata_subset is your AnnData object and it has the annotations and cluster labels
# Extract the relevant columns
annotations = new_adata_subset.obs["annotation_post_scanvi70_broad"]
cluster_labels = new_adata_subset.obs["cluster_labels_annotated"]

# Create a contingency table (cross-tabulation)
contingency_table = pd.crosstab(cluster_labels, annotations)

# Normalize the data across the columns (annotations)
normalized_table = contingency_table.div(contingency_table.sum(axis=0), axis=1)

# Scale the data from 0 to 1 across the columns
scaler = MinMaxScaler()
scaled_table = pd.DataFrame(scaler.fit_transform(normalized_table),
                            index=normalized_table.index,
                            columns=normalized_table.columns)
desired_column_order = ['PT','EC_Peritub','iPT','TAL','iTAL','DTL_ATL','CNT', 'DCT','IC A', 'IC B', 'PC', 'EC_DVR', 'VSMC',
                  'Fibroblast',  'Immune',   'EC_glom','PEC',
                    'Podo',  'MC1']  # Replace with actual annotation labels
desired_index_order = ['PT Neighborhood 1','iPT Neighborhood','TAL Neighborhood', 'iTAL Neighborhood', 'DTL_ATL Neighborhood',
                       'CNT Neighborhood 1','DCT Neighborhood','IC Neighborhood','PC Neighborhood', 'Vascular Neighborhood',
                  'Vascular Neighborhood 2','Immune Neighborhood',
                   'Fibroblast Neighborhood','Glomerular Neighborhood','Glomerular Neighborhood 2',



                   ]  # Replace with actual cluster labels
scaled_table = scaled_table.reindex(index=desired_index_order, columns=desired_column_order)
# Plot the heatmap
plt.figure(figsize=(5, 8))
sns.heatmap(scaled_table.T, annot=False, fmt=".2f", cmap="Blues")
plt.title('Kidney Niches')
plt.xlabel('Niche Labels')
plt.ylabel('Cell Types')
plt.show()

new_adata_subset.obs["cluster_labels"].value_counts()
cell_identities = {0: 'PC Neighborhood', 1: 'PT Neighborhood', 2: 'CNT Neighborhood', 3: 'Glomerular Neighborhood',
                   4: 'DTL_ATL Neighborhood', 5: 'Immune Neighborhood', 6: 'DCT Neighborhood', 7: 'iPT Neighborhood',
                   8: 'Fibroblast Neighborhood', 9: 'TAL Neighborhood', 10:'Vascular Neighborhood', 11:'Glomerular Neighborhood',
                   12:'iTAL Neighborhood', 13:'IC Neighborhood', 14:'Vascular Neighborhood'}
new_adata_subset.obs["cluster_labels_annotated_coarse"] = new_adata_subset.obs['cluster_labels'].map(cell_identities).astype('category')

import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.preprocessing import MinMaxScaler

# Assuming new_adata_subset is your AnnData object and it has the annotations and cluster labels
# Extract the relevant columns
annotations = new_adata_subset.obs["annotation_post_scanvi70_broad"]
cluster_labels = new_adata_subset.obs["cluster_labels_annotated_coarse"]

# Create a contingency table (cross-tabulation)
contingency_table = pd.crosstab(cluster_labels, annotations)

# Normalize the data across the columns (annotations)
normalized_table = contingency_table.div(contingency_table.sum(axis=0), axis=1)

# Scale the data from 0 to 1 across the columns
scaler = MinMaxScaler()
scaled_table = pd.DataFrame(scaler.fit_transform(normalized_table),
                            index=normalized_table.index,
                            columns=normalized_table.columns)
desired_column_order = ['PT','EC_Peritub','iPT','TAL','iTAL','DTL_ATL','CNT', 'DCT','IC A', 'IC B', 'PC', 'EC_DVR', 'VSMC',
                  'Fibroblast',  'Immune',   'EC_glom','PEC',
                    'Podo',  'MC1']  # Replace with actual annotation labels
desired_index_order = ['PT Neighborhood','iPT Neighborhood','TAL Neighborhood', 'iTAL Neighborhood', 'DTL_ATL Neighborhood',
                       'CNT Neighborhood','DCT Neighborhood','IC Neighborhood','PC Neighborhood', 'Vascular Neighborhood',
                   'Fibroblast Neighborhood','Immune Neighborhood','Glomerular Neighborhood',]  # Replace with actual cluster labels
scaled_table = scaled_table.reindex(index=desired_index_order, columns=desired_column_order)
# Plot the heatmap
plt.figure(figsize=(5, 8))
sns.heatmap(scaled_table.T, annot=False, fmt=".2f", cmap="Blues")
plt.title('Kidney Niches')
plt.tick_params(axis='x', which='both', length=0)
plt.tick_params(axis='y', which='both', length=0)
plt.xlabel('Niche Labels')
plt.ylabel('Cell Types')
plt.savefig('Kindey Niches.png', dpi=450, bbox_inches='tight')
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# Assuming new_adata_subset is your AnnData object
# Extract the relevant columns
samples = new_adata_subset.obs['sample']
cluster_labels = new_adata_subset.obs['cluster_labels_annotated_coarse']

# Create a DataFrame from the relevant columns
data = pd.DataFrame({'sample': samples, 'cluster_labels_annotated_coarse': cluster_labels})

# Group by 'sample' and 'cluster_labels_annotated' and count the occurrences
grouped_data = data.groupby(['sample', 'cluster_labels_annotated_coarse']).size().unstack(fill_value=0)
desired_order = [
    "Pediatric1", "HK2753", "HK3039", "HK3106", "HK3531", "HK3531_2",
    "HK3063", "HK3542", "HK2874", "HK2695", "HK3035", "HK3035_2",
     "HK2841", "HK2873","HK2844", "HK2844_2"
]

grouped_data=grouped_data.reindex(desired_order)
sample_totals = data['sample'].value_counts()
sample_totals=sample_totals.reindex(desired_order)
normalized_count_df = grouped_data.div(sample_totals, axis='index')
ax = normalized_count_df.plot(kind='bar', stacked=True, figsize=(8, 8), colormap='tab20')

# Set plot labels and title
plt.xlabel('')
plt.ylabel('Relative Amount')
plt.title('Sample Niche Composition')
plt.legend(title='Niche Labels', bbox_to_anchor=(1.05, 1), loc='upper left', frameon=False)
plt.tick_params(axis='x', which='both', length=0)
ax.spines['top'].set_visible(False)
ax.spines['right'].set_visible(False)
plt.tight_layout()
plt.savefig('Kindey Niches Samples.png', dpi=450, bbox_inches='tight')
plt.show()

adata.obs['cluster_labels_annotated_coarse']=new_adata_subset.obs["cluster_labels_annotated_coarse"].copy()

adata.obs['cluster_labels_annotated_coarse'] = adata.obs['cluster_labels_annotated_coarse'].cat.add_categories(['outlier'])
adata.obs["cluster_labels_annotated_coarse"]=adata.obs["cluster_labels_annotated_coarse"].fillna('outlier')

filtered_df['cluster_labels']=new_adata_subset.obs['cluster_labels_annotated_coarse']

new_adata_subset

filtered_df['cluster_labels']=new_adata_subset.obs['cluster_labels_annotated_coarse_coarse']

print(new_adata_subset.obs['cluster_labels_annotated_coarse'])

aggregated_df = filtered_df.groupby('cluster_labels').mean()

# Scale the aggregated data
scaler = StandardScaler()
aggregated_normalized = scaler.fit_transform(aggregated_df)

# Compute the linkage matrix using the Ward method
linkage_matrix = linkage(aggregated_normalized, method='ward')

# Plot the dendrogram
plt.figure(figsize=(15, 10))
dendrogram(linkage_matrix, labels=aggregated_df.index.astype(str).tolist(), leaf_rotation=90, leaf_font_size=10)
plt.title('Hierarchical Clustering Dendrogram (Ward)')
plt.xlabel('Cluster label')
plt.ylabel('Distance')
plt.show()

# Set a random seed for reproducibility
random_seed = 42

# Sample 1000 rows from each cluster
sampled_df = filtered_df.groupby('cluster_labels').apply(lambda x: x.sample(min(len(x), 1000), random_state=random_seed)).reset_index(drop=True)

# Aggregate the sampled dataframe by cluster_labels
aggregated_df = sampled_df.groupby('cluster_labels').mean()

# Scale the aggregated data
scaler = StandardScaler()
aggregated_normalized = scaler.fit_transform(aggregated_df)

# Compute the linkage matrix using the Ward method
linkage_matrix = linkage(aggregated_normalized, method='ward')

# Plot the dendrogram
plt.figure(figsize=(15, 10))
dendrogram(linkage_matrix, labels=aggregated_df.index.astype(str).tolist(), leaf_rotation=90, leaf_font_size=10)
plt.title('Hierarchical Clustering Dendrogram (Ward)')
plt.xlabel('Cluster label')
plt.ylabel('Distance')
plt.show()

adata.obs["cluster_labels_annotated_coarse"].value_counts()
cell_identities = {'PC Neighborhood':'CD Niche', 'PT Neighborhood': 'PT Niche', 'CNT Neighborhood': 'CNT Niche',
                   'DTL_ATL Neighborhood':'TAL Niche', 'Immune Neighborhood': 'Immune Niche', 'DCT Neighborhood': 'PT Niche',
                   'iPT Neighborhood': 'iPT Niche',
                   'Fibroblast Neighborhood': 'Fibroblast Niche', 'TAL Neighborhood': 'TAL Niche', 'Glomerular Neighborhood':'Glomerular Niche',
                   'iTAL Neighborhood':'iTAL Niche', 'IC Neighborhood':'CNT Niche', 'Vascular Neighborhood':'Vascular Niche', 'outlier':'outlier'}
adata.obs["cluster_labels_annotated_coarse_coarse"] = adata.obs['cluster_labels_annotated_coarse'].map(cell_identities).astype('category')

adata.obs['cluster_labels_annotated_coarse_coarse'].unique()

import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.preprocessing import MinMaxScaler

# Assuming new_adata_subset is your AnnData object and it has the annotations and cluster labels
# Extract the relevant columns
annotations = adata.obs["annotation_post_scanvi70_broad"]
cluster_labels = adata.obs["cluster_labels_annotated_coarse_coarse"]

# Create a contingency table (cross-tabulation)
contingency_table = pd.crosstab(cluster_labels, annotations)
contingency_table=contingency_table.drop('outlier', axis=0)

# Normalize the data across the columns (annotations)
normalized_table = contingency_table.div(contingency_table.sum(axis=1), axis=0)

# Scale the data from 0 to 1 across the columns
scaler = MinMaxScaler()
scaled_table = pd.DataFrame(scaler.fit_transform(normalized_table),
                            index=normalized_table.index,
                            columns=normalized_table.columns)
desired_column_order = ['EC_glom','PEC','Podo',  'MC1', 'EC_DVR', 'VSMC','TAL','DTL_ATL','EC_Peritub','Fibroblast','iTAL','Immune',
                    'iPT','PT','DCT', 'PC','IC B','CNT', 'IC A',
                     ]  # Replace with actual annotation labels
desired_index_order = ['Glomerular Niche','Vascular Niche','TAL Niche', 'Fibroblast Niche','iTAL Niche','Immune Niche','iPT Niche', 'PT Niche','CD Niche', 'CNT Niche',    ]  # Replace with actual cluster labels
scaled_table = scaled_table.reindex(index=desired_index_order, columns=desired_column_order)
# Plot the heatmap
plt.figure(figsize=(5, 8))
sns.heatmap(scaled_table.T, annot=False, fmt=".2f", cmap="Blues", cbar_kws={'shrink': 0.2})
plt.title('')
plt.tick_params(axis='x', which='both', length=0)
plt.tick_params(axis='y', which='both', length=0)
plt.xlabel('')
plt.ylabel('')
plt.savefig('Kindey Niches Coarse.png', dpi=450, bbox_inches='tight')
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.preprocessing import MinMaxScaler

# Assuming new_adata_subset is your AnnData object and it has the annotations and cluster labels
# Extract the relevant columns
annotations = adata.obs["annotation_post_scanvi70_broad"]
cluster_labels = adata.obs["cluster_labels_annotated_coarse_coarse"]

# Create a contingency table (cross-tabulation)
contingency_table = pd.crosstab(cluster_labels, annotations)
contingency_table=contingency_table.drop('outlier', axis=0)

# Normalize the data across the columns (annotations)
normalized_table = contingency_table.div(contingency_table.sum(axis=1), axis=0)

# Scale the data from 0 to 1 across the columns
scaler = MinMaxScaler()
scaled_table = pd.DataFrame(scaler.fit_transform(normalized_table),
                            index=normalized_table.index,
                            columns=normalized_table.columns)
desired_column_order = ['EC_glom','PEC','Podo',  'MC1', 'EC_DVR', 'VSMC','TAL','DTL_ATL','EC_Peritub','Fibroblast','iTAL','Immune',
                    'iPT','PT','DCT', 'PC','IC B','CNT', 'IC A',
                     ]  # Replace with actual annotation labels
desired_index_order = ['Glomerular Niche','Vascular Niche','TAL Niche', 'Fibroblast Niche','iTAL Niche','Immune Niche','iPT Niche', 'PT Niche','CD Niche', 'CNT Niche',    ]  # Replace with actual cluster labels
scaled_table = scaled_table.reindex(index=desired_index_order, columns=desired_column_order)
# Plot the heatmap
plt.figure(figsize=(5, 8))
sns.heatmap(scaled_table.T, annot=False, fmt=".2f", cmap="Blues", cbar_kws={'shrink': 0.2}, vmax=0.7, vmin=0)
plt.title('')
plt.tick_params(axis='x', which='both', length=0)
plt.tick_params(axis='y', which='both', length=0)
plt.xlabel('')
plt.ylabel('')
plt.savefig('Kindey Niches Coarse.png', dpi=450, bbox_inches='tight')
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.preprocessing import MinMaxScaler

# Assuming new_adata_subset is your AnnData object and it has the annotations and cluster labels
# Extract the relevant columns
annotations = adata.obs["annotation_post_scanvi70_broad"]
cluster_labels = adata.obs["cluster_labels_annotated_coarse_coarse"]

# Create a contingency table (cross-tabulation)
contingency_table = pd.crosstab(cluster_labels, annotations)
contingency_table = contingency_table.drop('outlier', axis=0)

# Normalize the data across the columns (annotations)
normalized_table = contingency_table.div(contingency_table.sum(axis=1), axis=0)

# Scale the data from 0 to 1 across the columns
scaler = MinMaxScaler()
scaled_table = pd.DataFrame(scaler.fit_transform(normalized_table),
                            index=normalized_table.index,
                            columns=normalized_table.columns)
desired_column_order = ['EC_glom', 'PEC', 'Podo', 'MC1', 'EC_DVR', 'VSMC', 'TAL', 'DTL_ATL', 'EC_Peritub', 'Fibroblast', 'iTAL', 'Immune',
                        'iPT', 'PT', 'DCT', 'PC', 'IC B', 'CNT', 'IC A']  # Replace with actual annotation labels
desired_index_order = ['Glomerular Niche', 'Vascular Niche', 'TAL Niche', 'Fibroblast Niche', 'iTAL Niche', 'Immune Niche', 'iPT Niche', 'PT Niche', 'CD Niche', 'CNT Niche']  # Replace with actual cluster labels
scaled_table = scaled_table.reindex(index=desired_index_order, columns=desired_column_order)

# Plot the heatmap
plt.figure(figsize=(5, 13))
ax = sns.heatmap(scaled_table.T, annot=False, fmt=".2f", cmap="Blues", cbar_kws={'shrink': 0.5, 'orientation': 'horizontal', 'pad': 0.05})

# Move x-axis labels to the top and rotate them 90 degrees
ax.xaxis.set_ticks_position('top')
ax.xaxis.set_label_position('top')
plt.xticks(rotation=90)

# Position colorbar to the bottom left
cbar = ax.collections[0].colorbar
cbar.ax.xaxis.set_ticks_position('bottom')
cbar.ax.xaxis.set_label_position('bottom')

plt.title('')
plt.tick_params(axis='x', which='both', length=0)
plt.tick_params(axis='y', which='both', length=0)
plt.xlabel('')
plt.ylabel('')
plt.savefig('Kidney_Niches_Coarse.png', dpi=450, bbox_inches='tight')
plt.show()

scaled_table.shape

col_linkage.shape

import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.preprocessing import MinMaxScaler

# Assuming new_adata_subset is your AnnData object and it has the annotations and cluster labels
# Extract the relevant columns
annotations = adata.obs["annotation_post_scanvi70_broad"]
cluster_labels = adata.obs["cluster_labels_annotated_coarse"]

# Create a contingency table (cross-tabulation)
contingency_table = pd.crosstab(cluster_labels, annotations)

# Normalize the data across the columns (annotations)
normalized_table = contingency_table.div(contingency_table.sum(axis=1), axis=0)

# Scale the data from 0 to 1 across the columns
scaler = MinMaxScaler()
scaled_table = pd.DataFrame(scaler.fit_transform(normalized_table),
                            index=normalized_table.index,
                            columns=normalized_table.columns)
desired_column_order = ['PT','EC_Peritub','iPT','TAL','iTAL','DTL_ATL','CNT', 'DCT','IC A', 'IC B', 'PC', 'EC_DVR', 'VSMC',
                  'Fibroblast',  'Immune',   'EC_glom','PEC',
                    'Podo',  'MC1']  # Replace with actual annotation labels
desired_index_order = ['PT Neighborhood','iPT Neighborhood','TAL Neighborhood', 'iTAL Neighborhood', 'DTL_ATL Neighborhood',
                       'CNT Neighborhood','DCT Neighborhood','IC Neighborhood','PC Neighborhood', 'Vascular Neighborhood',
                   'Fibroblast Neighborhood','Immune Neighborhood','Glomerular Neighborhood',]  # Replace with actual cluster labels
scaled_table = scaled_table.reindex(index=desired_index_order, columns=desired_column_order)
# Plot the heatmap
plt.figure(figsize=(5, 8))
sns.heatmap(scaled_table.T, annot=False, fmt=".2f", cmap="Blues")
plt.title('Kidney Niches')
plt.tick_params(axis='x', which='both', length=0)
plt.tick_params(axis='y', which='both', length=0)
plt.xlabel('Niche Labels')
plt.ylabel('Cell Types')
plt.savefig('Kindey Niches.png', dpi=450, bbox_inches='tight')
plt.show()

import pandas as pd
import numpy as np
from sklearn.cluster import MiniBatchKMeans, AgglomerativeClustering
from scipy.cluster.hierarchy import dendrogram, linkage
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
import umap.umap_ as umap
from sklearn.decomposition import PCA
aggregated_df = filtered_df.groupby('cluster_labels').mean()

# Scale the aggregated data
scaler = StandardScaler()
aggregated_normalized = scaler.fit_transform(aggregated_df)

# Compute the linkage matrix using the Ward method
linkage_matrix = linkage(aggregated_normalized, method='ward')

# Plot the dendrogram
plt.figure(figsize=(7, 5))
dendrogram(linkage_matrix, labels=aggregated_df.index.astype(str).tolist(), leaf_rotation=90, leaf_font_size=10)
plt.title('Hierarchical Clustering Dendrogram (Ward)')
plt.xlabel('Cluster label')
plt.ylabel('Distance')
plt.show()

plt.figure(figsize=(5, 3))
dendrogram(linkage_matrix, labels=aggregated_df.index.astype(str).tolist(), leaf_rotation=90, leaf_font_size=10)
plt.title('Hierarchical Clustering Dendrogram (Ward)')
plt.xlabel('')
plt.ylabel('Distance')
plt.show()

plt.figure(figsize=(5, 3))
dendrogram(linkage_matrix, labels=aggregated_df.index.astype(str).tolist(), leaf_rotation=90, leaf_font_size=10)
plt.title('')
plt.xlabel('')
plt.ylabel('')

# Get the current axes
ax = plt.gca()

# Remove all spines
ax.spines['top'].set_visible(False)
ax.spines['right'].set_visible(False)
#ax.spines['left'].set_visible(False)
#ax.spines['bottom'].set_visible(False)

# Hide y-axis values
#ax.yaxis.set_ticks([])
plt.savefig('Kindey Niches Hierarchical Clustering.png', dpi=450, bbox_inches='tight')
plt.show()

plt.figure(figsize=(7, 3))
dendrogram(linkage_matrix, labels=aggregated_df.index.astype(str).tolist(), leaf_rotation=90, leaf_font_size=10)
plt.title('Hierarchical Clustering Dendrogram (Ward)')
plt.xlabel('')
plt.ylabel('')

# Get the current axes
ax = plt.gca()

# Remove all spines
ax.spines['top'].set_visible(False)
ax.spines['right'].set_visible(False)
ax.spines['left'].set_visible(False)
ax.spines['bottom'].set_visible(False)

# Hide y-axis values
ax.yaxis.set_ticks([])
ax.xaxis.set_ticks([])
plt.show()

contingency_table.sum(axis=0)

print(contingency_table.sum(axis=1))

normalized_table

adata

adata.write('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/all_Cosmx_cleaned_neighbored.h5ad')

adata.obs['fov']=adata.obs['fov'].astype(int)
adata.obs['fov']=adata.obs['fov'].astype(str)
adata.obs['fov']

# Initialize an empty DataFrame with the required columns
results_df = pd.DataFrame(columns=['sample', 'fov', 'fraction_podo', 'fraction_mc1', 'type'])

for sample in adata.obs['sample'].unique():
    subset = adata[adata.obs['sample'] == sample]
    subset = subset[subset.obs['cluster_labels_annotated_coarse_coarse'] != 'outlier']

    for fov in subset.obs['fov'].unique():
        subset_fov = subset[subset.obs['fov'] == fov]
        subset_fov=subset_fov[subset_fov.obs['cluster_labels_annotated_coarse_coarse']=='Glomerular Niche']
        glom_niche_number = len(subset_fov[subset_fov.obs['cluster_labels_annotated_coarse_coarse'] == 'Glomerular Niche'])

        if glom_niche_number > 0:  # Avoid division by zero
            podo_number = len(subset_fov[subset_fov.obs['annotation_post_scanvi70_broad'] == 'Podo'])
            mc_number = len(subset_fov[subset_fov.obs['annotation_post_scanvi70_broad'] == 'MC1'])
            fraction_podo = podo_number / glom_niche_number
            fraction_mc1 = mc_number / glom_niche_number

            # Extract the condition for the current sample and fov
            condition = subset_fov.obs['type'].unique()[0]

            # Append the results to the DataFrame
            new_row = pd.DataFrame({
                'sample': [sample],
                'fov': [fov],
                'fraction_podo': [fraction_podo],
                'fraction_mc1': [fraction_mc1],
                'type': [condition]
            })
            results_df = pd.concat([results_df, new_row], ignore_index=True)

results_df.head(5)

results_df.shape

results_df['type']=results_df['type'].replace('Pediatric', 'Healthy')

results_df=results_df[results_df['type']!='Diabetes']
results_df.shape

results_df=results_df.drop('sample', axis=1)

results_df=results_df.drop('fov', axis=1)

results_df.shape

import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import mannwhitneyu

# Filter the data for the two types you want to compare
type1 = 'Healthy'
type2 = 'Disease'  # Change this to the other type present in your data

data_type1 = results_df[results_df['type'] == type1]
data_type2 = results_df[results_df['type'] == type2]

# Perform Mann-Whitney U test for fraction_podo
stat_podo, pvalue_podo = mannwhitneyu(data_type1['fraction_podo'], data_type2['fraction_podo'])

# Perform Mann-Whitney U test for fraction_mc1
stat_mc1, pvalue_mc1 = mannwhitneyu(data_type1['fraction_mc1'], data_type2['fraction_mc1'])

print(f'Mann-Whitney U test for fraction_podo: U-statistic = {stat_podo}, p-value = {pvalue_podo}')
print(f'Mann-Whitney U test for fraction_mc1: U-statistic = {stat_mc1}, p-value = {pvalue_mc1}')

import numpy as np

# Custom function to compute the 95% confidence interval around the median
def compute_95_ci(data):
    lower_bound = np.percentile(data, 2.5)
    upper_bound = np.percentile(data, 97.5)
    return lower_bound, upper_bound

# Function to remove outliers based on 95% confidence interval
def remove_outliers(df, column):
    lower_bound, upper_bound = compute_95_ci(df[column])
    filtered_df = df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]
    return filtered_df

# Remove outliers for fraction_podo and fraction_mc1
results_df_filtered = remove_outliers(results_df, 'fraction_podo')
results_df_filtered = remove_outliers(results_df_filtered, 'fraction_mc1')


# Perform Mann-Whitney U test again on the filtered data
data_type1_filtered = results_df_filtered[results_df_filtered['type'] == type1]
data_type2_filtered = results_df_filtered[results_df_filtered['type'] == type2]

stat_podo_filtered, pvalue_podo_filtered = mannwhitneyu(data_type1_filtered['fraction_podo'], data_type2_filtered['fraction_podo'])
stat_mc1_filtered, pvalue_mc1_filtered = mannwhitneyu(data_type1_filtered['fraction_mc1'], data_type2_filtered['fraction_mc1'])

print(f'Mann-Whitney U test for fraction_podo (filtered): U-statistic = {stat_podo_filtered}, p-value = {pvalue_podo_filtered}')
print(f'Mann-Whitney U test for fraction_mc1 (filtered): U-statistic = {stat_mc1_filtered}, p-value = {pvalue_mc1_filtered}')

results_df_filtered['type']=results_df_filtered['type'].replace('Healthy', 'Control')
results_df_filtered['type']=results_df_filtered['type'].replace('Disease', 'DKD')

import seaborn as sns
import matplotlib.pyplot as plt

# Define custom colors for each type
palette = {"Control": "#ADCEE5", "DKD": "#F5ACAC"}  # Adjust the keys to match your types

# Group by sample and drop the fov column
grouped_df = results_df_filtered.groupby('type').mean().reset_index()


plt.figure(figsize=(5, 3))

# Violin plot for fraction_podo
plt.subplot(1, 2, 1)
ax1 = sns.violinplot(x='type', y='fraction_podo', data=results_df_filtered, palette=palette, saturation=1, linecolor='black', linewidth=1)
ax1.set_title('')
ax1.set_xlabel('')
ax1.set_ylabel('Podocyte Fraction')
plt.ylim(0, 0.8)
plt.xticks(rotation=0)

# Violin plot for fraction_mc1
plt.subplot(1, 2, 2)
ax2 = sns.violinplot(x='type', y='fraction_mc1', data=results_df_filtered, palette=palette, saturation=1, linecolor='black', linewidth=1)
ax2.set_title('')
ax2.set_xlabel('')
ax2.set_ylabel('MC Fraction')
plt.ylim(0, 0.2)
plt.xticks(rotation=0)

plt.tight_layout()
plt.savefig('Glomerular Niche Podo MC Density.png', dpi=450, bbox_inches='tight')
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

# Define custom colors for each type
palette = {"Control": "#ADCEE5", "DKD": "#F5ACAC"}  # Adjust the keys to match your types

# Group by sample and drop the fov column
grouped_df = results_df_filtered.groupby('type').mean().reset_index()

plt.figure(figsize=(5, 3.2))

# Violin plot for fraction_podo
plt.subplot(1, 2, 1)
ax1 = sns.violinplot(x='type', y='fraction_podo', data=results_df_filtered, palette=palette, saturation=1, linecolor='black', linewidth=0.75)
ax1.set_title('')
ax1.set_xlabel('')
ax1.set_ylabel('Podocyte Fraction')
ax1.set_ylim(0, 0.8)
ax1.yaxis.set_ticks(np.arange(0, 0.9, 0.1))
ax1.spines['top'].set_visible(False)
ax1.spines['right'].set_visible(False)
plt.xticks(rotation=0)

# Violin plot for fraction_mc1
plt.subplot(1, 2, 2)
ax2 = sns.violinplot(x='type', y='fraction_mc1', data=results_df_filtered, palette=palette, saturation=1, linecolor='black', linewidth=0.75)
ax2.set_title('')
ax2.set_xlabel('')
ax2.set_ylabel('MC Fraction')
ax2.set_ylim(0, 0.2)
ax2.yaxis.set_ticks(np.arange(0, 0.21, 0.05))
ax2.spines['top'].set_visible(False)
ax2.spines['right'].set_visible(False)
plt.xticks(rotation=0)

plt.tight_layout()
plt.savefig('Glomerular_Niche_Podo_MC_Density.png', dpi=450, bbox_inches='tight')
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

# Define custom colors for each type
palette = {"Control": "#ADCEE5", "DKD": "#F5ACAC"}  # Adjust the keys to match your types

# Group by sample and drop the fov column
grouped_df = results_df_filtered.groupby('type').mean().reset_index()

plt.figure(figsize=(2.5, 7))

# Violin plot for fraction_podo
plt.subplot(2, 1, 1)  # Change this to (2, 1, 1) for vertical stacking
ax1 = sns.violinplot(x='type', y='fraction_podo', data=results_df_filtered, palette=palette, saturation=1, linecolor='black', linewidth=1)
ax1.set_title('Podocyte Fraction')
ax1.set_xlabel('')
ax1.set_ylabel('Fraction Podo')
plt.ylim(0, 0.8)
ax1.yaxis.set_ticks(np.arange(0, 0.9, 0.1))
plt.xticks(rotation=0)

# Violin plot for fraction_mc1
plt.subplot(2, 1, 2)  # Change this to (2, 1, 2) for vertical stacking
ax2 = sns.violinplot(x='type', y='fraction_mc1', data=results_df_filtered, palette=palette, saturation=1, linecolor='black', linewidth=1)
ax2.set_title('MC Fraction')
ax2.set_xlabel('Type')
ax2.set_ylabel('Fraction MC1')
plt.ylim(0, 0.2)
plt.xticks(rotation=0)
ax2.yaxis.set_ticks(np.arange(0, 0.21, 0.05))

plt.tight_layout()
plt.savefig('Glomerular_Niche_Podo_MC_Density.png', dpi=450, bbox_inches='tight')
plt.show()

print(adata.obs['cluster_labels_annotated_coarse_coarse'].value_counts())

# Initialize an empty DataFrame with the required columns
results_df_2 = pd.DataFrame(columns=['sample', 'fov', 'fraction_immune', 'type'])

for sample in adata.obs['sample'].unique():
    subset = adata[adata.obs['sample'] == sample]
    subset = subset[subset.obs['cluster_labels_annotated_coarse_coarse'] != 'outlier']

    for fov in subset.obs['fov'].unique():
        subset_fov = subset[subset.obs['fov'] == fov]
        subset_fov=subset_fov[subset_fov.obs['cluster_labels_annotated_coarse_coarse']=='Vascular Niche']
        vasc_niche_number = len(subset_fov[subset_fov.obs['cluster_labels_annotated_coarse_coarse'] == 'Vascular Niche'])

        if vasc_niche_number > 0:  # Avoid division by zero
            immune_number = len(subset_fov[subset_fov.obs['annotation_post_scanvi70_broad'] == 'Immune'])
            fraction_immune = immune_number / vasc_niche_number

            # Extract the condition for the current sample and fov
            condition = subset_fov.obs['type'].unique()[0]

            # Append the results to the DataFrame
            new_row = pd.DataFrame({
                'sample': [sample],
                'fov': [fov],
                'fraction_immune': [fraction_immune],
                'type': [condition]
            })
            results_df_2 = pd.concat([results_df_2, new_row], ignore_index=True)

results_df_2

results_df_2.shape

results_df_2['type']=results_df_2['type'].replace('Pediatric', 'Healthy')

results_df_2=results_df_2[results_df_2['type']!='Diabetes']
results_df_2.shape

results_df_2=results_df_2.drop('sample', axis=1)

results_df_2=results_df_2.drop('fov', axis=1)

results_df_2.shape

results_df_2

import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import mannwhitneyu

# Filter the data for the two types you want to compare
type1 = 'Healthy'
type2 = 'Disease'  # Change this to the other type present in your data

data_type1 = results_df_2[results_df_2['type'] == type1]
data_type2 = results_df_2[results_df_2['type'] == type2]

# Perform Mann-Whitney U test for fraction_podo
stat_podo, pvalue_podo = mannwhitneyu(data_type1['fraction_immune'], data_type2['fraction_immune'])

print(f'Mann-Whitney U test for fraction_immune: U-statistic = {stat_podo}, p-value = {pvalue_podo}')

import numpy as np

# Custom function to compute the 95% confidence interval around the median
def compute_95_ci(data):
    lower_bound = np.percentile(data, 2.5)
    upper_bound = np.percentile(data, 97.5)
    return lower_bound, upper_bound

# Function to remove outliers based on 95% confidence interval
def remove_outliers(df, column):
    lower_bound, upper_bound = compute_95_ci(df[column])
    filtered_df = df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]
    return filtered_df

# Remove outliers for fraction_podo and fraction_mc1
results_df_filtered = remove_outliers(results_df_2, 'fraction_immune')

# Perform Mann-Whitney U test again on the filtered data
data_type1_filtered = results_df_filtered[results_df_filtered['type'] == type1]
data_type2_filtered = results_df_filtered[results_df_filtered['type'] == type2]

stat_podo_filtered, pvalue_podo_filtered = mannwhitneyu(data_type1_filtered['fraction_immune'], data_type2_filtered['fraction_immune'])


print(f'Mann-Whitney U test for fraction_immune (filtered): U-statistic = {stat_podo_filtered}, p-value = {pvalue_podo_filtered}')

results_df_filtered['type']=results_df_filtered['type'].replace('Healthy', 'Control')
results_df_filtered['type']=results_df_filtered['type'].replace('Disease', 'DKD')

import seaborn as sns
import matplotlib.pyplot as plt

# Define custom colors for each type
palette = {"Control": "#ADCEE5", "DKD": "#F5ACAC"}  # Adjust the keys to match your types

# Group by sample and drop the fov column
grouped_df = results_df_filtered.groupby('type').mean().reset_index()

plt.figure(figsize=(2.5, 3.2))

# Violin plot for fraction_podo
#plt.subplot(1, 2, 1)
ax1 = sns.violinplot(x='type', y='fraction_immune', data=results_df_filtered, palette=palette, saturation=1, linecolor='black', linewidth=0.75)
ax1.set_title('')
ax1.set_xlabel('')
ax1.set_ylabel('Immune Fraction')
ax1.set_ylim(0, 0.44)
ax1.yaxis.set_ticks(np.arange(0, 0.44, 0.1))
ax1.spines['top'].set_visible(False)
ax1.spines['right'].set_visible(False)
#ax1.yaxis.set_ticks(np.arange(0, 0.44, 0.1))
plt.xticks(rotation=0)
#plt.ylim(0,0.44)

plt.tight_layout()
plt.savefig('Vascular_Niche_Immune_Density.png', dpi=450, bbox_inches='tight')
plt.show()

"""find a good niche"""

adata

import gc

for niche in adata.obs['cluster_labels_annotated_coarse_coarse'].unique():
  if niche=='outlier':
    continue
  if niche=='PT Niche':
    continue
  if niche=='TAL Niche':
    continue
  if niche =='iTAL Niche':
    continue
  if niche=='Fibroblast Niche':
    continue
  if niche =='Immune Niche':
    continue
  if niche =='CNT Niche':
    continue
  subset=adata[adata.obs['cluster_labels_annotated_coarse_coarse']==niche]
  for sample in subset.obs['sample'].unique():
    if sample != 'HK3063':
      continue
    subset_sample=subset[subset.obs['sample']==sample]
    for fov in subset_sample.obs['fov'].unique():
      subset_fov=subset_sample[subset_sample.obs['fov']==fov]
      sc.pl.scatter(
        subset_fov,
        x="CenterX_global_px",
        y="CenterY_global_px",
        color="annotation_post_scanvi70_broad",
        size=150,
        title=f'{fov} for {niche} for {sample}',
      )
  gc.collect()



"""lets do cortex medulla"""

import pandas as pd
import numpy as np
from sklearn.cluster import MiniBatchKMeans, AgglomerativeClustering
from scipy.cluster.hierarchy import dendrogram, linkage
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
import umap.umap_ as umap
from sklearn.decomposition import PCA

# Assuming your dataframe is already loaded as gene_expression_df
# Normalize the data
scaler = StandardScaler()
features_normalized = scaler.fit_transform(filtered_df)

pca = PCA(n_components=25)  # Reduce to 10 dimensions for UMAP input
principal_components = pca.fit_transform(features_normalized)

# Perform MiniBatchKMeans clustering
kmeans = MiniBatchKMeans(n_clusters=2, random_state=42)
labels = kmeans.fit_predict(principal_components)
filtered_df['cluster_labels'] = labels

new_adata_subset.obs['cluster_labels']=filtered_df['cluster_labels']

new_adata_subset.obs['cluster_labels']=new_adata_subset.obs['cluster_labels'].astype('category')

sc.tl.rank_genes_groups(new_adata_subset, groupby='cluster_labels', method='wilcoxon', pts = True)
sc.pl.rank_genes_groups(new_adata_subset, n_genes=25, sharey=False)

new_adata_subset.obs["cluster_labels"].value_counts()
cell_identities = {0: 'Cortex', 1: 'Medulla'}
new_adata_subset.obs["cluster_labels_annotated"] = new_adata_subset.obs['cluster_labels'].map(cell_identities).astype('category')

import matplotlib.pyplot as plt
import seaborn as sns

# Assuming new_adata_subset is your AnnData object
# Extract the relevant columns
samples = new_adata_subset.obs['sample']
cluster_labels = new_adata_subset.obs['cluster_labels_annotated']

# Create a DataFrame from the relevant columns
data = pd.DataFrame({'sample': samples, 'cluster_labels_annotated': cluster_labels})

# Group by 'sample' and 'cluster_labels_annotated' and count the occurrences
grouped_data = data.groupby(['sample', 'cluster_labels_annotated']).size().unstack(fill_value=0)
desired_order = [
    "Pediatric1", "HK2753", "HK3039", "HK3106", "HK3531", "HK3531_2",
    "HK3063", "HK3542", "HK2874", "HK2695", "HK3035", "HK3035_2",
     "HK2841", "HK2873","HK2844", "HK2844_2"
]

grouped_data=grouped_data.reindex(desired_order)
sample_totals = data['sample'].value_counts()
sample_totals=sample_totals.reindex(desired_order)
normalized_count_df = grouped_data.div(sample_totals, axis='index')
ax = normalized_count_df.plot(kind='bar', stacked=True, figsize=(8, 6), colormap='tab10b')

# Set plot labels and title
plt.xlabel('')
plt.ylabel('Relative Amount')
plt.title('Sample Region Composition')
plt.legend(title='Region Labels', bbox_to_anchor=(1.05, 1), loc='upper left', frameon=False)
plt.tick_params(axis='x', which='both', length=0)
ax.spines['top'].set_visible(False)
ax.spines['right'].set_visible(False)
plt.tight_layout()
plt.savefig('Cortex Medulla Samples.png', dpi=450, bbox_inches='tight')
plt.show()

adata.obs['cluster_labels_annotated_regions']=new_adata_subset.obs["cluster_labels_annotated"].copy()

for sample in adata.obs['sample'].unique():
  subset=adata[adata.obs['sample']==sample]
  sc.pl.scatter(
    subset,
    x="CenterX_global_px",
    y="CenterY_global_px",
    color="cluster_labels_annotated_regions",
    size=3,
    title=f'Niche Composition for {sample}',
  )

adata.obs['cluster_labels_annotated_regions'] = adata.obs['cluster_labels_annotated_regions'].cat.add_categories(['outlier'])
adata.obs["cluster_labels_annotated_regions"]=adata.obs["cluster_labels_annotated_regions"].fillna('outlier')

annotated_regions = adata.obs['cluster_labels_annotated_regions']
coarse_labels = adata.obs['cluster_labels_annotated_coarse']

# Create a dataframe
df = pd.DataFrame({
    'cluster_labels_annotated_regions': annotated_regions,
    'cluster_labels_annotated_coarse': coarse_labels
})
df

df=df[df['cluster_labels_annotated_coarse']!='outlier']

medulla_df = df[df['cluster_labels_annotated_regions'] == 'Medulla']
cortex_df = df[df['cluster_labels_annotated_regions'] == 'Cortex']

# Sort each dataframe by the 'cluster_labels_coarse' column
medulla_df_sorted = medulla_df.sort_values(by='cluster_labels_annotated_coarse')
cortex_df_sorted = cortex_df.sort_values(by='cluster_labels_annotated_coarse')

# Combine the sorted dataframes back into one
sorted_df = pd.concat([medulla_df_sorted, cortex_df_sorted])

print(sorted_df)

!pip install plotly

sorted_df['cluster_labels_annotated_regions'] = sorted_df['cluster_labels_annotated_regions'].cat.remove_unused_categories()
sorted_df['cluster_labels_annotated_coarse'] = sorted_df['cluster_labels_annotated_coarse'].cat.remove_unused_categories()

import pandas as pd
import plotly.graph_objects as go

# Assuming sorted_df is your dataframe created earlier
# Prepare data for the Sankey diagram
source_labels = sorted_df['cluster_labels_annotated_regions'].unique().tolist()
target_labels = sorted_df['cluster_labels_annotated_coarse'].unique().tolist()
labels = source_labels + target_labels

source_indices = sorted_df['cluster_labels_annotated_regions'].apply(lambda x: source_labels.index(x)).tolist()
target_indices = sorted_df['cluster_labels_annotated_coarse'].apply(lambda x: target_labels.index(x) + len(source_labels)).tolist()

# Create a dataframe for the counts of each flow
flow_counts = sorted_df.groupby(['cluster_labels_annotated_regions', 'cluster_labels_annotated_coarse']).size().reset_index(name='count')

# Create the Sankey plot
fig = go.Figure(go.Sankey(
    node=dict(
        pad=15,
        thickness=20,
        line=dict(color="black", width=0.5),
        label=labels
    ),
    link=dict(
        source=flow_counts['cluster_labels_annotated_regions'].apply(lambda x: source_labels.index(x)).tolist(),
        target=flow_counts['cluster_labels_annotated_coarse'].apply(lambda x: target_labels.index(x) + len(source_labels)).tolist(),
        value=flow_counts['count']
    )
))

fig.update_layout(title_text="Flow from Annotated Regions to Coarse Labels", font_size=10)
fig.show()

adata

annotated_regions = adata.obs['cluster_labels_annotated_regions']
coarse_labels = adata.obs['annotation_post_scanvi70_broad']

# Create a dataframe
df = pd.DataFrame({
    'cluster_labels_annotated_regions': annotated_regions,
    'annotation_post_scanvi70_broad': coarse_labels
})
df

df=df[df['cluster_labels_annotated_regions']!='outlier']

medulla_df = df[df['cluster_labels_annotated_regions'] == 'Medulla']
cortex_df = df[df['cluster_labels_annotated_regions'] == 'Cortex']

# Sort each dataframe by the 'cluster_labels_coarse' column
medulla_df_sorted = medulla_df.sort_values(by='annotation_post_scanvi70_broad')
cortex_df_sorted = cortex_df.sort_values(by='annotation_post_scanvi70_broad')

# Combine the sorted dataframes back into one
sorted_df = pd.concat([medulla_df_sorted, cortex_df_sorted])

print(sorted_df)

!pip install plotly

sorted_df['annotation_post_scanvi70_broad'] = sorted_df['annotation_post_scanvi70_broad'].cat.remove_unused_categories()
sorted_df['cluster_labels_annotated_regions'] = sorted_df['cluster_labels_annotated_regions'].cat.remove_unused_categories()

import pandas as pd
import plotly.graph_objects as go

# Assuming sorted_df is your dataframe created earlier
# Prepare data for the Sankey diagram
source_labels = sorted_df['cluster_labels_annotated_regions'].unique().tolist()
target_labels = sorted_df['annotation_post_scanvi70_broad'].unique().tolist()
labels = source_labels + target_labels

source_indices = sorted_df['cluster_labels_annotated_regions'].apply(lambda x: source_labels.index(x)).tolist()
target_indices = sorted_df['annotation_post_scanvi70_broad'].apply(lambda x: target_labels.index(x) + len(source_labels)).tolist()

# Create a dataframe for the counts of each flow
flow_counts = sorted_df.groupby(['cluster_labels_annotated_regions', 'annotation_post_scanvi70_broad']).size().reset_index(name='count')

# Create the Sankey plot
fig = go.Figure(go.Sankey(
    node=dict(
        pad=15,
        thickness=20,
        line=dict(color="black", width=0.5),
        label=labels
    ),
    link=dict(
        source=flow_counts['cluster_labels_annotated_regions'].apply(lambda x: source_labels.index(x)).tolist(),
        target=flow_counts['annotation_post_scanvi70_broad'].apply(lambda x: target_labels.index(x) + len(source_labels)).tolist(),
        value=flow_counts['count']
    )
))

fig.update_layout(title_text="Flow from Annotated Regions to Coarse Labels", font_size=10)
fig.show()

flow_counts.shape

filtered_flow_counts.shape

!pip install -U kaleido

!pip install -U kaleido

import pandas as pd
import plotly.graph_objects as go
import kaleido

# Assuming sorted_df is your dataframe created earlier
# Prepare data for the Sankey diagram
source_labels = sorted_df['cluster_labels_annotated_regions'].unique().tolist()
target_labels = sorted_df['annotation_post_scanvi70_broad'].unique().tolist()
labels = source_labels + target_labels

# Create a dataframe for the counts of each flow
flow_counts = sorted_df.groupby(['cluster_labels_annotated_regions', 'annotation_post_scanvi70_broad']).size().reset_index(name='count')

# Calculate the total count for each target label
target_totals = flow_counts.groupby('annotation_post_scanvi70_broad')['count'].sum().reset_index(name='total_count')

# Merge the total counts back into the flow_counts dataframe
flow_counts = flow_counts.merge(target_totals, on='annotation_post_scanvi70_broad')

# Determine the threshold for 5% of each target label's total count
flow_counts['threshold'] = 0.1 * flow_counts['total_count']

# Filter the flow_counts to include only flows greater than 5% of the total for each target label
filtered_flow_counts = flow_counts[flow_counts['count'] > flow_counts['threshold']]

# Prepare the indices for the Sankey plot
source_indices = filtered_flow_counts['cluster_labels_annotated_regions'].apply(lambda x: source_labels.index(x)).tolist()
target_indices = filtered_flow_counts['annotation_post_scanvi70_broad'].apply(lambda x: target_labels.index(x) + len(source_labels)).tolist()

# Create the Sankey plot
fig = go.Figure(go.Sankey(
    node=dict(
        pad=150,
        thickness=200,
        line=dict(color="black", width=5),
        label=labels
    ),
    link=dict(
        source=source_indices,
        target=target_indices,
        value=filtered_flow_counts['count']
    )
))

#fig.update_layout(title_text="Flow from Annotated Regions to Coarse Labels", font_size=10)
fig.update_layout(
    title_text="",
    font_size=300,
    width=6000,  # Adjust the width as needed
    height=12000   # Adjust the height as needed
)

# Save the figure
#fig.write_image("sankey_plot.png")

# Show the figure
fig.show()

import pandas as pd
import plotly.graph_objects as go
import kaleido

# Assuming sorted_df is your dataframe created earlier
# Prepare data for the Sankey diagram
source_labels = sorted_df['cluster_labels_annotated_regions'].unique().tolist()
target_labels = sorted_df['annotation_post_scanvi70_broad'].unique().tolist()
labels = source_labels + target_labels

# Create a dataframe for the counts of each flow
flow_counts = sorted_df.groupby(['cluster_labels_annotated_regions', 'annotation_post_scanvi70_broad']).size().reset_index(name='count')

# Calculate the total count for each target label
target_totals = flow_counts.groupby('annotation_post_scanvi70_broad')['count'].sum().reset_index(name='total_count')

# Merge the total counts back into the flow_counts dataframe
flow_counts = flow_counts.merge(target_totals, on='annotation_post_scanvi70_broad')

# Determine the threshold for 5% of each target label's total count
flow_counts['threshold'] = 0.1 * flow_counts['total_count']

# Filter the flow_counts to include only flows greater than 5% of the total for each target label
filtered_flow_counts = flow_counts[flow_counts['count'] > flow_counts['threshold']]

# Prepare the indices for the Sankey plot
source_indices = filtered_flow_counts['cluster_labels_annotated_regions'].apply(lambda x: source_labels.index(x)).tolist()
target_indices = filtered_flow_counts['annotation_post_scanvi70_broad'].apply(lambda x: target_labels.index(x) + len(source_labels)).tolist()

x_coords = [0.1]  + [0.1]
y_coords = [0.4]  + [0.1]

# Create the Sankey plot
fig = go.Figure(go.Sankey(
    node=dict(
        pad=15,
        thickness=20,
        line=dict(color="black", width=1),
        label=labels,
        x=x_coords,
        y=y_coords
    ),
    link=dict(
        source=source_indices,
        target=target_indices,
        value=filtered_flow_counts['count']
    )
))

#fig.update_layout(title_text="Flow from Annotated Regions to Coarse Labels", font_size=10)
fig.update_layout(
    title_text="",
    font_size=10,
    width=600,  # Adjust the width as needed
    height=800   # Adjust the height as needed
)

# Save the figure
#fig.write_image("sankey_plot.png")

# Show the figure
fig.show()



import pandas as pd
import plotly.graph_objects as go
import kaleido

# Assuming sorted_df is your dataframe created earlier
# Prepare data for the Sankey diagram
source_labels = sorted_df['cluster_labels_annotated_regions'].unique().tolist()
target_labels = sorted_df['annotation_post_scanvi70_broad'].unique().tolist()
labels = source_labels + target_labels

# Create a dataframe for the counts of each flow
flow_counts = sorted_df.groupby(['cluster_labels_annotated_regions', 'annotation_post_scanvi70_broad']).size().reset_index(name='count')

# Calculate the total count for each target label
target_totals = flow_counts.groupby('annotation_post_scanvi70_broad')['count'].sum().reset_index(name='total_count')

# Merge the total counts back into the flow_counts dataframe
flow_counts = flow_counts.merge(target_totals, on='annotation_post_scanvi70_broad')

# Determine the threshold for 5% of each target label's total count
flow_counts['threshold'] = 0.1 * flow_counts['total_count']

# Filter the flow_counts to include only flows greater than 5% of the total for each target label
filtered_flow_counts = flow_counts[flow_counts['count'] > flow_counts['threshold']]

# Prepare the indices for the Sankey plot
source_indices = filtered_flow_counts['cluster_labels_annotated_regions'].apply(lambda x: source_labels.index(x)).tolist()
target_indices = filtered_flow_counts['annotation_post_scanvi70_broad'].apply(lambda x: target_labels.index(x) + len(source_labels)).tolist()

x_coords = [0.1]  + [0.1]
y_coords = [0.4]  + [0.1]

# Create the Sankey plot
fig = go.Figure(go.Sankey(
    node=dict(
        pad=150,  # Increased padding by 10x
        thickness=200,  # Increased thickness by 10x
        line=dict(color="black", width=10),  # Increased line width by 10x
        label=labels,
        x=x_coords,
        y=y_coords
    ),
    link=dict(
        source=source_indices,
        target=target_indices,
        value=filtered_flow_counts['count']
    )
))

# Adjust the layout of the figure
fig.update_layout(
    title_text="",
    font_size=100,  # Increased font size by 10x
    width=6000,  # Increased width by 10x
    height=8000,  # Increased height by 10x
    margin=dict(l=200, r=200, t=200, b=200)  # Increased margins by 10x
)


# Show the figure
fig.show()

import pandas as pd
import plotly.graph_objects as go
import kaleido

# Assuming sorted_df is your dataframe created earlier
# Prepare data for the Sankey diagram
source_labels = sorted_df['cluster_labels_annotated_regions'].unique().tolist()
target_labels = sorted_df['annotation_post_scanvi70_broad'].unique().tolist()
labels = source_labels + target_labels

# Create a dataframe for the counts of each flow
flow_counts = sorted_df.groupby(['cluster_labels_annotated_regions', 'annotation_post_scanvi70_broad']).size().reset_index(name='count')

# Calculate the total count for each target label
target_totals = flow_counts.groupby('annotation_post_scanvi70_broad')['count'].sum().reset_index(name='total_count')

# Merge the total counts back into the flow_counts dataframe
flow_counts = flow_counts.merge(target_totals, on='annotation_post_scanvi70_broad')

# Determine the threshold for 5% of each target label's total count
flow_counts['threshold'] = 0.1 * flow_counts['total_count']

# Filter the flow_counts to include only flows greater than 5% of the total for each target label
filtered_flow_counts = flow_counts[flow_counts['count'] > flow_counts['threshold']]

# Prepare the indices for the Sankey plot
source_indices = filtered_flow_counts['cluster_labels_annotated_regions'].apply(lambda x: source_labels.index(x)).tolist()
target_indices = filtered_flow_counts['annotation_post_scanvi70_broad'].apply(lambda x: target_labels.index(x) + len(source_labels)).tolist()

x_coords = [0.1]  + [0.1]
y_coords = [0.6]  + [0.1]

# Create the Sankey plot
fig = go.Figure(go.Sankey(
    node=dict(
        pad=60,
        thickness=30,
        line=dict(color="black", width=4),
        label=labels,
        x=x_coords,
        y=y_coords
    ),
    link=dict(
        source=source_indices,
        target=target_indices,
        value=filtered_flow_counts['count']
    )
))

#fig.update_layout(title_text="Flow from Annotated Regions to Coarse Labels", font_size=10)
fig.update_layout(
    title_text="",
    font_size=60,
    width=2400,  # Adjust the width as needed
    height=3600,   # Adjust the height as needed
    margin=dict(l=20, r=20, t=20, b=20)
)

# Save the figure
#fig.write_image("sankey_plot.png")

# Show the figure
fig.show()

import pandas as pd
import plotly.graph_objects as go
import kaleido

# Assuming sorted_df is your dataframe created earlier
# Prepare data for the Sankey diagram
source_labels = sorted_df['cluster_labels_annotated_regions'].unique().tolist()
target_labels = sorted_df['annotation_post_scanvi70_broad'].unique().tolist()
labels = source_labels + target_labels

# Create a dataframe for the counts of each flow
flow_counts = sorted_df.groupby(['cluster_labels_annotated_regions', 'annotation_post_scanvi70_broad']).size().reset_index(name='count')

# Calculate the total count for each target label
target_totals = flow_counts.groupby('annotation_post_scanvi70_broad')['count'].sum().reset_index(name='total_count')

# Merge the total counts back into the flow_counts dataframe
flow_counts = flow_counts.merge(target_totals, on='annotation_post_scanvi70_broad')

# Determine the threshold for 5% of each target label's total count
flow_counts['threshold'] = 0.1 * flow_counts['total_count']

# Filter the flow_counts to include only flows greater than 5% of the total for each target label
filtered_flow_counts = flow_counts[flow_counts['count'] > flow_counts['threshold']]

# Prepare the indices for the Sankey plot
source_indices = filtered_flow_counts['cluster_labels_annotated_regions'].apply(lambda x: source_labels.index(x)).tolist()
target_indices = filtered_flow_counts['annotation_post_scanvi70_broad'].apply(lambda x: target_labels.index(x) + len(source_labels)).tolist()

# Set the positions for the nodes
num_sources = len(source_labels)
num_targets = len(target_labels)

# Source coordinates
x_coords = [0.1] * num_sources
y_coords = [(i + 1) / (num_sources + 1) * 2 / 3 + 1 / 3 for i in range(num_sources)]  # Adding padding below

# Target coordinates
x_coords += [0.9] * num_targets
y_coords += [(i + 1) / (num_targets + 1) * 2 / 3 for i in range(num_targets)]  # Adding padding above

# Create the Sankey plot
fig = go.Figure(go.Sankey(
    node=dict(
        pad=15,
        thickness=20,
        line=dict(color="black", width=1),
        label=labels,
        x=x_coords,
        y=y_coords
    ),
    link=dict(
        source=source_indices,
        target=target_indices,
        value=filtered_flow_counts['count']
    )
))

# Adjust the layout of the figure
fig.update_layout(
    title_text="Flow from Annotated Regions to Coarse Labels",
    font_size=10,
    width=600,  # Adjust the width as needed
    height=1200,  # Increased height significantly
    margin=dict(l=50, r=50, t=50, b=50)  # Increased margins
)


# Show the figure
fig.show()

import pandas as pd
import plotly.graph_objects as go
import kaleido

# Assuming sorted_df is your dataframe created earlier
# Prepare data for the Sankey diagram
source_labels = sorted_df['cluster_labels_annotated_regions'].unique().tolist()
target_labels = sorted_df['annotation_post_scanvi70_broad'].unique().tolist()
labels = source_labels + target_labels

# Create a dataframe for the counts of each flow
flow_counts = sorted_df.groupby(['cluster_labels_annotated_regions', 'annotation_post_scanvi70_broad']).size().reset_index(name='count')

# Calculate the total count for each target label
target_totals = flow_counts.groupby('annotation_post_scanvi70_broad')['count'].sum().reset_index(name='total_count')

# Merge the total counts back into the flow_counts dataframe
flow_counts = flow_counts.merge(target_totals, on='annotation_post_scanvi70_broad')

# Determine the threshold for 5% of each target label's total count
flow_counts['threshold'] = 0.1 * flow_counts['total_count']

# Filter the flow_counts to include only flows greater than 5% of the total for each target label
filtered_flow_counts = flow_counts[flow_counts['count'] > flow_counts['threshold']]

# Prepare the indices for the Sankey plot
source_indices = filtered_flow_counts['cluster_labels_annotated_regions'].apply(lambda x: source_labels.index(x)).tolist()
target_indices = filtered_flow_counts['annotation_post_scanvi70_broad'].apply(lambda x: target_labels.index(x) + len(source_labels)).tolist()

# Set the positions for the nodes
num_sources = len(source_labels)
num_targets = len(target_labels)

# Source coordinates
x_coords = [0.1] * num_sources
y_coords = [(i + 1) / (num_sources + 1) * 2 / 3 for i in range(num_sources)]  # Adding padding above

# Target coordinates
x_coords += [0.9] * num_targets
y_coords += [(i + 1) / (num_targets + 1) * 2 / 3 + 1 / 3 for i in range(num_targets)]  # Adding padding below

# Normalize the y-coordinates to fit in the range [0, 1]
max_y = max(y_coords)
y_coords = [y / max_y for y in y_coords]

# Create the Sankey plot
fig = go.Figure(go.Sankey(
    node=dict(
        pad=15,
        thickness=20,
        line=dict(color="black", width=1),
        label=labels,
        x=x_coords,
        y=y_coords
    ),
    link=dict(
        source=source_indices,
        target=target_indices,
        value=filtered_flow_counts['count']
    )
))

# Adjust the layout of the figure
fig.update_layout(
    title_text="Flow from Annotated Regions to Coarse Labels",
    font_size=10,
    width=600,  # Adjust the width as needed
    height=1200,  # Increased height significantly
    margin=dict(l=50, r=50, t=50, b=50)  # Adjust margins as needed
)



# Show the figure
fig.show()

import pandas as pd
import plotly.graph_objects as go
import kaleido

# Assuming sorted_df is your dataframe created earlier
# Prepare data for the Sankey diagram
source_labels = sorted_df['cluster_labels_annotated_regions'].unique().tolist()
target_labels = sorted_df['annotation_post_scanvi70_broad'].unique().tolist()
labels = source_labels + target_labels

# Create a dataframe for the counts of each flow
flow_counts = sorted_df.groupby(['cluster_labels_annotated_regions', 'annotation_post_scanvi70_broad']).size().reset_index(name='count')

# Calculate the total count for each target label
target_totals = flow_counts.groupby('annotation_post_scanvi70_broad')['count'].sum().reset_index(name='total_count')

# Merge the total counts back into the flow_counts dataframe
flow_counts = flow_counts.merge(target_totals, on='annotation_post_scanvi70_broad')

# Determine the threshold for 5% of each target label's total count
flow_counts['threshold'] = 0.1 * flow_counts['total_count']

# Filter the flow_counts to include only flows greater than 5% of the total for each target label
filtered_flow_counts = flow_counts[flow_counts['count'] > flow_counts['threshold']]

# Prepare the indices for the Sankey plot
source_indices = filtered_flow_counts['cluster_labels_annotated_regions'].apply(lambda x: source_labels.index(x)).tolist()
target_indices = filtered_flow_counts['annotation_post_scanvi70_broad'].apply(lambda x: target_labels.index(x) + len(source_labels)).tolist()

x_coords = [0.15]  + [0.1]
y_coords = [0.55]  + [0.1]

# Create the Sankey plot
fig = go.Figure(go.Sankey(
    node=dict(
        pad=120,
        thickness=200,
        line=dict(color="black", width=5),
        label=labels,
        x=x_coords,
        y=y_coords
    ),
    link=dict(
        source=source_indices,
        target=target_indices,
        value=filtered_flow_counts['count']
    )
))

#fig.update_layout(title_text="Flow from Annotated Regions to Coarse Labels", font_size=10)
fig.update_layout(
    title_text="",
    font_size=100,
    width=6000,  # Adjust the width as needed
    height=12000   # Adjust the height as needed
)

# Save the figure
#fig.write_image("sankey_plot.png")

# Show the figure
fig.show()

adata



import matplotlib.pyplot as plt
import seaborn as sns

# Assuming new_adata_subset is your AnnData object
# Extract the relevant columns
samples = adata.obs['sample']
cluster_labels = adata.obs['cluster_labels_annotated_regions']

# Create a DataFrame from the relevant columns
data = pd.DataFrame({'sample': samples, 'cluster_labels_annotated_regions': cluster_labels})
data=data[data['cluster_labels_annotated_regions']!='outlier']
data['cluster_labels_annotated_regions'] = data['cluster_labels_annotated_regions'].cat.remove_unused_categories()
# Group by 'sample' and 'cluster_labels_annotated' and count the occurrences
grouped_data = data.groupby(['sample', 'cluster_labels_annotated_regions']).size().unstack(fill_value=0)
desired_order = [
    "Pediatric1", "HK2753", "HK3039", "HK3106", "HK3531", "HK3531_2",
    "HK3063", "HK3542", "HK2874", "HK2695", "HK3035", "HK3035_2",
     "HK2841", "HK2873","HK2844", "HK2844_2"
]

grouped_data=grouped_data.reindex(desired_order)
sample_totals = data['sample'].value_counts()
sample_totals=sample_totals.reindex(desired_order)
normalized_count_df = grouped_data.div(sample_totals, axis='index')
ax = normalized_count_df.plot(kind='bar', stacked=True, figsize=(8, 6), colormap='tab10')

# Set plot labels and title
plt.xlabel('')
plt.ylabel('Relative Amount')
plt.title('Sample Region Composition')
plt.legend(title='Region Labels', bbox_to_anchor=(1.05, 1), loc='upper left', frameon=False)
plt.tick_params(axis='x', which='both', length=0)
ax.spines['top'].set_visible(False)
ax.spines['right'].set_visible(False)
plt.tight_layout()
plt.savefig('Cortex Medulla Samples.png', dpi=450, bbox_inches='tight')
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming new_adata_subset is your AnnData object
# Extract the relevant columns
samples = adata.obs['sample']
cluster_labels = adata.obs['cluster_labels_annotated_regions']

# Create a DataFrame from the relevant columns
data = pd.DataFrame({'sample': samples, 'cluster_labels_annotated_regions': cluster_labels})
data = data[data['cluster_labels_annotated_regions'] != 'outlier']
data['cluster_labels_annotated_regions'] = data['cluster_labels_annotated_regions'].cat.remove_unused_categories()

# Group by 'sample' and 'cluster_labels_annotated_regions' and count the occurrences
grouped_data = data.groupby(['sample', 'cluster_labels_annotated_regions']).size().unstack(fill_value=0)

desired_order = [
    "Pediatric1", "HK2753", "HK3039", "HK3106", "HK3531", "HK3531_2",
    "HK3063", "HK3542", "HK2874", "HK2695", "HK3035", "HK3035_2",
    "HK2841", "HK2873", "HK2844", "HK2844_2"
]

grouped_data = grouped_data.reindex(desired_order)
sample_totals = data['sample'].value_counts()
sample_totals = sample_totals.reindex(desired_order)
normalized_count_df = grouped_data.div(sample_totals, axis='index')

# Extract specific colors from 'tab10' colormap
colors = plt.get_cmap('tab10').colors
blue = colors[0]
orange = colors[1]

# Apply the colors to the plot
ax = normalized_count_df.plot(kind='bar', stacked=True, figsize=(8, 6), color=[blue, orange])

# Set plot labels and title
plt.xlabel('')
plt.ylabel('Relative Amount')
plt.title('Sample Region Composition')
plt.legend(title='Region Labels', bbox_to_anchor=(1.05, 1), loc='upper left', frameon=False)
plt.tick_params(axis='x', which='both', length=0)
ax.spines['top'].set_visible(False)
ax.spines['right'].set_visible(False)
plt.tight_layout()
plt.savefig('Cortex_Medulla_Samples.png', dpi=450, bbox_inches='tight')
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# Assuming new_adata_subset is your AnnData object
# Extract the relevant columns
samples = adata.obs['sample']
cluster_labels = adata.obs['cluster_labels_annotated_coarse_coarse']

# Create a DataFrame from the relevant columns
data = pd.DataFrame({'sample': samples, 'cluster_labels_annotated_coarse_coarse': cluster_labels})
data = data[data['cluster_labels_annotated_coarse_coarse'] != 'outlier']
data['cluster_labels_annotated_coarse_coarse'] = data['cluster_labels_annotated_coarse_coarse'].cat.remove_unused_categories()

# Group by 'sample' and 'cluster_labels_annotated' and count the occurrences
grouped_data = data.groupby(['sample', 'cluster_labels_annotated_coarse_coarse']).size().unstack(fill_value=0)
desired_order = [
    "Pediatric1", "HK2753", "HK3039", "HK3106", "HK3531", "HK3531_2",
    "HK3063", "HK3542", "HK2874", "HK2695", "HK3035", "HK3035_2",
     "HK2841", "HK2873","HK2844", "HK2844_2"
]

grouped_data=grouped_data.reindex(desired_order)
sample_totals = data['sample'].value_counts()
sample_totals=sample_totals.reindex(desired_order)
normalized_count_df = grouped_data.div(sample_totals, axis='index')
ax = normalized_count_df.plot(kind='bar', stacked=True, figsize=(8, 8), colormap='tab10')

# Set plot labels and title
plt.xlabel('')
plt.ylabel('Relative Amount')
plt.title('Sample Niche Composition')
plt.legend(title='Niche Labels', bbox_to_anchor=(1.05, 1), loc='upper left', frameon=False)
plt.tick_params(axis='x', which='both', length=0)
ax.spines['top'].set_visible(False)
ax.spines['right'].set_visible(False)
plt.tight_layout()
plt.savefig('Kindey Niches Composition.png', dpi=450, bbox_inches='tight')
plt.show()

adata.write('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/all_Cosmx_cleaned_neighbored.h5ad')

pt_subset=adata[adata.obs['annotation_post_scanvi70_broad']=='PT'].copy()

pt_subset=pt_subset[pt_subset.obs['cluster_labels_annotated_regions']!='outlier']

pt_subset.X = pt_subset.layers["counts"]
sc.pp.log1p(pt_subset)
sc.tl.rank_genes_groups(pt_subset, groupby='cluster_labels_annotated_regions', method='wilcoxon', pts = True)
sc.pl.rank_genes_groups(pt_subset, n_genes=25, sharey=False)

result = pt_subset.uns['rank_genes_groups']

# Create a dictionary to store the dataframes for each group
dfs = {}
for group in result['names'].dtype.names:
    dfs[group] = pd.DataFrame({
        'names': result['names'][group],
        'scores': result['scores'][group],
        'logfoldchanges': result['logfoldchanges'][group],
        'pvals': result['pvals'][group],
        'pvals_adj': result['pvals_adj'][group],
        'pts': result['pts'][group],
        'pts_rest': result['pts_rest'][group]
    })

# Example: Accessing the dataframe for the first group
first_group = list(dfs.keys())[0]
print(f"DataFrame for group: {first_group}")
print(dfs[first_group])

# If you want to save each dataframe to a CSV file
for group, df in dfs.items():
    df.to_csv(f"rank_genes_groups_{group}.csv", index=False)

"""Lets do neighboring differences health vs disease"""

new_adata_subset=new_adata[new_adata.obs['total_neighbors_200']>50]
new_adata_subset

new_adata_subset.obs['type']=new_adata_subset.obs['type'].replace('Pediatric', 'Healthy')

new_adata_subset=new_adata_subset[new_adata_subset.obs['type']!='Diabetes']
new_adata_subset=new_adata_subset[new_adata_subset.obs['sample']!='HK2874']
new_adata_subset

import pandas as pd

# Convert the sparse matrix to a dense format and create a DataFrame
gene_expression_df = pd.DataFrame(
    new_adata_subset.X.toarray(),
    index=new_adata_subset.obs_names,
    columns=new_adata_subset.var_names
)

# Print the first few rows to verify
print(gene_expression_df.head())

# List of .obs columns you want to add to the DataFrame
obs_columns = [
    'total_neighbors_200', 'total_neighbors_180', 'total_neighbors_160',
    'total_neighbors_140', 'total_neighbors_120', 'total_neighbors_100',
    'total_neighbors_80', 'total_neighbors_60', 'total_neighbors_40', 'total_neighbors_20'
]

# Add the selected .obs columns to the gene expression DataFrame
gene_expression_df[obs_columns] = new_adata_subset.obs[obs_columns]

# Print the first few rows to verify the updated DataFrame
print(gene_expression_df.head())

# Loop over each suffix and corresponding total_neighbors column
for neighbors in [200, 180, 160, 140, 120, 100, 80, 60, 40, 20]:
    # Generate the suffix and obs column name
    suffix = f'_{neighbors}'
    obs_column = f'total_neighbors_{neighbors}'

    # Get all columns that end with the current suffix
    matching_columns = [col for col in gene_expression_df.columns if col.endswith(suffix)]

    # Perform the division for each matching column
    for col in matching_columns:
        gene_expression_df[col] /= gene_expression_df[obs_column]

# Print the first few rows to verify the updated DataFrame
print(gene_expression_df.head())

gene_expression_df = gene_expression_df.drop(obs_columns, axis=1)

# Print the first few rows to verify that columns are removed
print(gene_expression_df.head())

import numpy as np

# Replace infinite values with 0
gene_expression_df.replace([np.inf, -np.inf], 0, inplace=True)

# Verify changes by displaying the first few rows of the DataFrame
print(gene_expression_df.head())

gene_expression_df=gene_expression_df.fillna(0)

from scipy import sparse
sparse_matrix = scipy.sparse.csr_matrix(gene_expression_df.values)

new_adata_subset.X=sparse_matrix

print(new_adata_subset.X)

for niche in new_adata_subset.obs['cluster_labels_annotated_coarse_coarse'].unique():
  subset=new_adata_subset[new_adata_subset.obs['cluster_labels_annotated_coarse_coarse']==niche]
  sc.tl.rank_genes_groups(subset, groupby='type', method='wilcoxon', pts = True)
  sc.pl.rank_genes_groups(subset, n_genes=25, sharey=False)
  de_results = subset.uns['rank_genes_groups']
  folder_name = '/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/Neighborfraction_DEGs'

  # Iterate through each group to save DEG lists
  for group in de_results['names'].dtype.names:
        gene_names = de_results['names'][group]
        gene_scores = de_results['scores'][group]
        gene_logfoldchanges = de_results['logfoldchanges'][group]
        gene_pvals = de_results['pvals'][group]
        gene_pvals_adj = de_results['pvals_adj'][group]

        # Create a DataFrame
        df = pd.DataFrame({
            'Gene': gene_names,
            'Score': gene_scores,
            'LogFoldChange': gene_logfoldchanges,
            'pValue': gene_pvals,
            'pValue_adj': gene_pvals_adj
        })
        df = df[~df['Gene'].str.endswith('_200')]
        df = df[~df['Gene'].str.endswith('_180')]
        df = df[~df['Gene'].str.endswith('_160')]
        df = df[~df['Gene'].str.endswith('_140')]
        df = df[~df['Gene'].str.endswith('_120')]
        df = df[~df['Gene'].str.endswith('_100')]
        df = df[~df['Gene'].str.endswith('_80')]
        df = df[~df['Gene'].str.endswith('_60')]
        print(df)
        # Define the filename
        filename = os.path.join(folder_name, f"{niche}_{group}_DEG.csv")

        # Save the DataFrame to a CSV file
        df.to_csv(filename, index=False)

        print(f"Saved DEG list for group {group} to {filename}")

for niche in new_adata_subset.obs['cluster_labels_annotated_coarse_coarse'].unique():
  subset=new_adata_subset[new_adata_subset.obs['cluster_labels_annotated_coarse_coarse']==niche]
  sc.tl.rank_genes_groups(subset, groupby='type', method='wilcoxon', pts = True)
  sc.pl.rank_genes_groups(subset, n_genes=25, sharey=False)
  de_results = subset.uns['rank_genes_groups']

  markers=[]
  # Iterate through each group to save DEG lists
  for group in de_results['names'].dtype.names:
        gene_names = de_results['names'][group]
        gene_scores = de_results['scores'][group]
        gene_logfoldchanges = de_results['logfoldchanges'][group]
        gene_pvals = de_results['pvals'][group]
        gene_pvals_adj = de_results['pvals_adj'][group]

        # Create a DataFrame
        df = pd.DataFrame({
            'Gene': gene_names,
            'Score': gene_scores,
            'LogFoldChange': gene_logfoldchanges,
            'pValue': gene_pvals,
            'pValue_adj': gene_pvals_adj
        })
        df = df[~df['Gene'].str.endswith('_200')]
        df = df[~df['Gene'].str.endswith('_180')]
        df = df[~df['Gene'].str.endswith('_160')]
        df = df[~df['Gene'].str.endswith('_140')]
        df = df[~df['Gene'].str.endswith('_120')]
        df = df[~df['Gene'].str.endswith('_100')]
        df = df[~df['Gene'].str.endswith('_80')]
        df = df[~df['Gene'].str.endswith('_60')]
        df = df[df['pValue_adj']<0.01]
        df = df[df['LogFoldChange']>0]
        print(niche)
        print(group)
        print(df)
        markers=df['gene'].to_list

    # Create the dot plot
  dotplot = sc.pl.dotplot(new_adata_subset, markers, groupby='type',
                        cmap='Blues', log=False, show=False, swap_axes=True, dot_max=1, vmax=0.45)

    # Get the main plot axis
  ax = dotplot['mainplot_ax']

        # Rotate the x and y labels
  ax.set_xlabel('')
  ax.set_ylabel('')
  ax.set_xticklabels(ax.get_xticklabels(), rotation=90)
  ax.set_yticklabels(ax.get_yticklabels(), rotation=0)

        # Remove the little ticks but keep the labels
  ax.tick_params(axis='x', which='both', bottom=False, top=False)
  ax.tick_params(axis='y', which='both', left=False, right=False)

        # Invert the y-axis to flip the plot
  ax.invert_yaxis()

        # Adjust the plot layout
  plt.tight_layout()
  plt.savefig(f'{niche}_health_vs_disease_neighborfractions.png', dpi=450, bbox_inches='tight')
        # Show the plot
  plt.show()

import scanpy as sc
import matplotlib.pyplot as plt
import pandas as pd

# Loop through each unique niche
for niche in new_adata_subset.obs['cluster_labels_annotated_coarse_coarse'].unique():
    subset = new_adata_subset[new_adata_subset.obs['cluster_labels_annotated_coarse_coarse'] == niche]
    sc.tl.rank_genes_groups(subset, groupby='type', method='wilcoxon', pts=True)
    sc.pl.rank_genes_groups(subset, n_genes=25, sharey=False)
    de_results = subset.uns['rank_genes_groups']

    markers = []
    # Iterate through each group to save DEG lists
    for group in de_results['names'].dtype.names:
        gene_names = de_results['names'][group]
        gene_scores = de_results['scores'][group]
        gene_logfoldchanges = de_results['logfoldchanges'][group]
        gene_pvals = de_results['pvals'][group]
        gene_pvals_adj = de_results['pvals_adj'][group]

        # Create a DataFrame
        df = pd.DataFrame({
            'Gene': gene_names,
            'Score': gene_scores,
            'LogFoldChange': gene_logfoldchanges,
            'pValue': gene_pvals,
            'pValue_adj': gene_pvals_adj
        })
        # Filter the DataFrame
        df = df[~df['Gene'].str.endswith(('_200', '_180', '_160', '_140', '_120', '_100', '_80', '_60', '_40'))]
        df = df[df['pValue_adj'] < 0.01]
        df = df[df['LogFoldChange'] > 0.8]

        print(niche)
        print(group)
        print(df)

        # Add the genes to the markers list
        markers.extend(df['Gene'].tolist())

    # Create the dot plot
    dotplot = sc.pl.dotplot(subset, markers, groupby='type',
                            cmap='Blues', log=False, show=False, swap_axes=True)

    # Get the main plot axis
    ax = dotplot['mainplot_ax']

    # Rotate the x and y labels
    ax.set_xlabel('')
    ax.set_ylabel('')
    ax.set_xticklabels(ax.get_xticklabels(), rotation=90)
    ax.set_yticklabels(ax.get_yticklabels(), rotation=0)

    # Remove the little ticks but keep the labels
    ax.tick_params(axis='x', which='both', bottom=False, top=False)
    ax.tick_params(axis='y', which='both', left=False, right=False)

    # Invert the y-axis to flip the plot
    ax.invert_yaxis()

    # Adjust the plot layout
    plt.tight_layout()
    plt.savefig(f'{niche}_health_vs_disease_neighborfractions_20.png', dpi=450, bbox_inches='tight')
    # Show the plot
    plt.show()

import scanpy as sc
import matplotlib.pyplot as plt
import pandas as pd

# Loop through each unique niche
for niche in new_adata_subset.obs['cluster_labels_annotated_coarse_coarse'].unique():
    subset = new_adata_subset[new_adata_subset.obs['cluster_labels_annotated_coarse_coarse'] == niche]
    sc.tl.rank_genes_groups(subset, groupby='type', method='wilcoxon', pts=True)
    sc.pl.rank_genes_groups(subset, n_genes=25, sharey=False)
    de_results = subset.uns['rank_genes_groups']

    markers = []
    # Iterate through each group to save DEG lists
    for group in de_results['names'].dtype.names:
        gene_names = de_results['names'][group]
        gene_scores = de_results['scores'][group]
        gene_logfoldchanges = de_results['logfoldchanges'][group]
        gene_pvals = de_results['pvals'][group]
        gene_pvals_adj = de_results['pvals_adj'][group]

        # Create a DataFrame
        df = pd.DataFrame({
            'Gene': gene_names,
            'Score': gene_scores,
            'LogFoldChange': gene_logfoldchanges,
            'pValue': gene_pvals,
            'pValue_adj': gene_pvals_adj
        })
        # Filter the DataFrame
        df = df[~df['Gene'].str.endswith(('_200', '_180', '_160', '_140', '_120', '_100', '_80', '_60', '_20'))]
        df = df[df['pValue_adj'] < 0.01]
        df = df[df['LogFoldChange'] > 0.8]

        print(niche)
        print(group)
        print(df)

        # Add the genes to the markers list
        markers.extend(df['Gene'].tolist())

    # Create the dot plot
    dotplot = sc.pl.dotplot(subset, markers, groupby='type',
                            cmap='Blues', log=False, show=False, swap_axes=True)

    # Get the main plot axis
    ax = dotplot['mainplot_ax']

    # Rotate the x and y labels
    ax.set_xlabel('')
    ax.set_ylabel('')
    ax.set_xticklabels(ax.get_xticklabels(), rotation=90)
    ax.set_yticklabels(ax.get_yticklabels(), rotation=0)

    # Remove the little ticks but keep the labels
    ax.tick_params(axis='x', which='both', bottom=False, top=False)
    ax.tick_params(axis='y', which='both', left=False, right=False)

    # Invert the y-axis to flip the plot
    ax.invert_yaxis()

    # Adjust the plot layout
    plt.tight_layout()
    plt.savefig(f'{niche}_health_vs_disease_neighborfractions_40.png', dpi=450, bbox_inches='tight')
    # Show the plot
    plt.show()

markers = [ "iPT_20", "iPT_40"]

sc.pl.dotplot(new_adata_subset, markers, groupby='cluster_labels_annotated_coarse_coarse',cmap='Blues', log = False)

markers = [ "iPT_20", "iPT_40"]

sc.pl.dotplot(new_adata_subset, markers, groupby='cluster_labels_annotated_coarse_coarse',cmap='Blues', log = False)