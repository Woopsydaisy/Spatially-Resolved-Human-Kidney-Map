# -*- coding: utf-8 -*-
"""Neighborhood generation and figure kmeans Vol2 GSEA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-oH-Clqb-W7Zs0p8J1tKL3uo6MTfX20E
"""

!pip --quiet install scanpy
!pip --quiet install leidenalg
#!pip --quiet install squidpy

from google.colab import drive
drive.mount('/content/drive')

import csv
import anndata as ad
import gzip
import os
import scipy.io
import scanpy as sc
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from google.colab import drive
import leidenalg as la
from pathlib import Path
#import squidpy as sq

adata = sc.read_h5ad('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/all_Cosmx_cleaned_neighbored.h5ad')

for sample in adata.obs['sample'].unique():
  print(sample)

adata_all=sc.read_h5ad('/content/drive/MyDrive/Bernhard/Cosmx/SCVI/All_Runs/allCosmx_allGenes.h5ad')
adata_all

adata_all.obs['cluster_labels_annotated_coarse_coarse']=adata.obs['cluster_labels_annotated_coarse_coarse'].copy()

print(adata_all.obs['cluster_labels_annotated_coarse_coarse'])

adata_all.write('/content/drive/MyDrive/Bernhard/Cosmx/SCVI/All_Runs/allCosmx_allGenes.h5ad')

adata_all_subset=adata_all[adata_all.obs['cluster_labels_annotated_coarse_coarse']!='outlier']

#adata_all_subset.X = adata_all_subset.layers["counts"]
sc.pp.log1p(adata_all_subset)
sc.tl.rank_genes_groups(adata_all_subset, groupby='cluster_labels_annotated_coarse_coarse', method='wilcoxon', pts = True)
sc.pl.rank_genes_groups(adata_all_subset, n_genes=25, sharey=False)

de_results = adata_all_subset.uns['rank_genes_groups']
for group in de_results['names'].dtype.names:
        gene_names = de_results['names'][group]
        gene_scores = de_results['scores'][group]
        gene_logfoldchanges = de_results['logfoldchanges'][group]
        gene_pvals = de_results['pvals'][group]
        gene_pvals_adj = de_results['pvals_adj'][group]

        # Create a DataFrame
        df = pd.DataFrame({
            'Gene': gene_names,
            'Score': gene_scores,
            'LogFoldChange': gene_logfoldchanges,
            'pValue': gene_pvals,
            'pValue_adj': gene_pvals_adj
        })
        print(df.head(40))
        print(f'{group}_DEGs.csv')
        df.to_csv(f'/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/{group}_DEGs.csv')

!pip --quiet install gseapy

import gseapy
from gseapy import barplot, dotplot

human = gseapy.get_library_name(organism='Human')

for group in de_results['names'].dtype.names:
        print(f'{group}_DEGs.csv')

iTAL = pd.read_csv("/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/iTAL Niche_DEGs.csv", index_col = 'Unnamed: 0')
CNT = pd.read_csv("/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/CNT Niche_DEGs.csv", index_col = 'Unnamed: 0')
Fibro = pd.read_csv("/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/Fibroblast Niche_DEGs.csv", index_col = 'Unnamed: 0')
Glom = pd.read_csv("/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/Glomerular Niche_DEGs.csv", index_col = 'Unnamed: 0')
Immune = pd.read_csv("/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/Immune Niche_DEGs.csv", index_col = 'Unnamed: 0')
PT = pd.read_csv("/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/PT Niche_DEGs.csv", index_col = 'Unnamed: 0')
TAL = pd.read_csv("/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/TAL Niche_DEGs.csv", index_col = 'Unnamed: 0')
Vascular = pd.read_csv("/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/Vascular Niche_DEGs.csv", index_col = 'Unnamed: 0')
iPT = pd.read_csv("/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/iPT Niche_DEGs.csv", index_col = 'Unnamed: 0')
CD = pd.read_csv("/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/CD Niche_DEGs.csv", index_col = 'Unnamed: 0')

iTAL = pd.read_csv("/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/imputed/Niche_iTAL Niche_DEG.csv")
CNT = pd.read_csv("/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/imputed/Niche_CNT Niche_DEG.csv")
Fibro = pd.read_csv("/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/imputed/Niche_Fibroblast Niche_DEG.csv")
Glom = pd.read_csv("/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/imputed/Niche_Glomerular Niche_DEG.csv")
Immune = pd.read_csv("/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/imputed/Niche_Immune Niche_DEG.csv")
PT = pd.read_csv("/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/imputed/Niche_PT Niche_DEG.csv")
TAL = pd.read_csv("/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/imputed/Niche_TAL Niche_DEG.csv")
Vascular = pd.read_csv("/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/imputed/Niche_Vascular Niche_DEG.csv")
iPT = pd.read_csv("/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/imputed/Niche_iPT Niche_DEG.csv")
CD = pd.read_csv("/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/imputed/Niche_CD Niche_DEG.csv")

iTAL

CD

populations_to_test = [iTAL,CNT,Fibro,Glom,Immune,PT,TAL,Vascular,iPT,CD]
very_small_value = 1e-310

for i in populations_to_test:
    i['pValue_adj'] = i['pValue_adj'].replace(0, very_small_value)
    i["names"] = i['Gene']
    i["pi_score"] = -1 * np.log10(i["pValue_adj"]) * i["LogFoldChange"] # pvalue
    i.dropna(inplace=True)

Immune

v_library='WikiPathway_2023_Human' ## 'WikiPathway_2023_Human' ###
gset = gseapy.parser.get_library(v_library, min_size=20)



gene_rank = CD[['names','pi_score']]
gene_rank.sort_values(by=['pi_score'], inplace=True, ascending=False)
gene_rank = gene_rank.reset_index(drop=True)
background_genes = gene_rank['names'].tolist()
res = gseapy.prerank(rnk=gene_rank, gene_sets=gset)

#res.res2d.head(20)
res.res2d_plot = res.res2d[res.res2d["FWER p-val"] < 0.05]
res.res2d_plot.head(20)
CD_gset=res.res2d.copy()
CD_gset.shape

background_genes

gene_rank = iTAL[['names','pi_score']]
gene_rank.sort_values(by=['pi_score'], inplace=True, ascending=False)
gene_rank = gene_rank.reset_index(drop=True)

res = gseapy.prerank(rnk=gene_rank, gene_sets=gset)

#res.res2d.head(20)
res.res2d_plot = res.res2d[res.res2d["FWER p-val"] < 0.05]
res.res2d_plot.head(20)
iTAL_gset=res.res2d.copy()
iTAL_gset.shape

gene_rank = CNT[['names','pi_score']]
gene_rank.sort_values(by=['pi_score'], inplace=True, ascending=False)
gene_rank = gene_rank.reset_index(drop=True)

res = gseapy.prerank(rnk=gene_rank, gene_sets=gset)

#res.res2d.head(20)
res.res2d_plot = res.res2d[res.res2d["FWER p-val"] < 0.05]
res.res2d_plot.head(20)
CNT_gset=res.res2d.copy()
CNT_gset.shape

gene_rank = Fibro[['names','pi_score']]
gene_rank.sort_values(by=['pi_score'], inplace=True, ascending=False)
gene_rank = gene_rank.reset_index(drop=True)

res = gseapy.prerank(rnk=gene_rank, gene_sets=gset)

#res.res2d.head(20)
res.res2d_plot = res.res2d[res.res2d["FWER p-val"] < 0.05]
res.res2d_plot.head(20)
Fibro_gset=res.res2d.copy()
Fibro_gset.shape

gene_rank = Glom[['names','pi_score']]
gene_rank.sort_values(by=['pi_score'], inplace=True, ascending=False)
gene_rank = gene_rank.reset_index(drop=True)

res = gseapy.prerank(rnk=gene_rank, gene_sets=gset)

#res.res2d.head(20)
res.res2d_plot = res.res2d[res.res2d["FWER p-val"] < 0.05]
res.res2d_plot.head(20)
Glom_gset=res.res2d.copy()
Glom_gset.shape

gene_rank = Immune[['names','pi_score']]
gene_rank.sort_values(by=['pi_score'], inplace=True, ascending=False)
gene_rank = gene_rank.reset_index(drop=True)

res = gseapy.prerank(rnk=gene_rank, gene_sets=gset)

#res.res2d.head(20)
res.res2d_plot = res.res2d[res.res2d["FWER p-val"] < 0.05]
res.res2d_plot.head(20)
Immune_gset=res.res2d.copy()
Immune_gset.shape

gene_rank = PT[['names','pi_score']]
gene_rank.sort_values(by=['pi_score'], inplace=True, ascending=False)
gene_rank = gene_rank.reset_index(drop=True)

res = gseapy.prerank(rnk=gene_rank, gene_sets=gset)

#res.res2d.head(20)
res.res2d_plot = res.res2d[res.res2d["FWER p-val"] < 0.05]
res.res2d_plot.head(20)
PT_gset=res.res2d.copy()
PT_gset.shape

gene_rank = TAL[['names','pi_score']]
gene_rank.sort_values(by=['pi_score'], inplace=True, ascending=False)
gene_rank = gene_rank.reset_index(drop=True)

res = gseapy.prerank(rnk=gene_rank, gene_sets=gset)

#res.res2d.head(20)
res.res2d_plot = res.res2d[res.res2d["FWER p-val"] < 0.05]
res.res2d_plot.head(20)
TAL_gset=res.res2d.copy()
TAL_gset.shape

gene_rank = Vascular[['names','pi_score']]
gene_rank.sort_values(by=['pi_score'], inplace=True, ascending=False)
gene_rank = gene_rank.reset_index(drop=True)

res = gseapy.prerank(rnk=gene_rank, gene_sets=gset)

#res.res2d.head(20)
res.res2d_plot = res.res2d[res.res2d["FWER p-val"] < 0.05]
res.res2d_plot.head(20)
Vascular_gset=res.res2d.copy()
Vascular_gset.shape

gene_rank = iPT[['names','pi_score']]
gene_rank.sort_values(by=['pi_score'], inplace=True, ascending=False)
gene_rank = gene_rank.reset_index(drop=True)

res = gseapy.prerank(rnk=gene_rank, gene_sets=gset)

#res.res2d.head(20)
res.res2d_plot = res.res2d[res.res2d["FWER p-val"] < 0.05]
res.res2d_plot.head(20)
iPT_gset=res.res2d.copy()
iPT_gset.shape

iPT_gset.to_csv('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/imputed/iPT_genesets.csv')
iTAL_gset.to_csv('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/imputed/iTAL_genesets.csv')
CNT_gset.to_csv('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/imputed/CNT_genesets.csv')
Vascular_gset.to_csv('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/imputed/Vascular_gset.csv')
TAL_gset.to_csv('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/imputed/TAL_gset.csv')
PT_gset.to_csv('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/imputed/PT_gset.csv')
Immune_gset.to_csv('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/imputed/Immune_gset.csv')
Fibro_gset.to_csv('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/imputed/Fibro_gset.csv')
Glom_gset.to_csv('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/imputed/Glom_gset.csv')
CD_gset.to_csv('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/imputed/CD_gset.csv')

iPT_gset=pd.read_csv('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/imputed/iPT_genesets.csv')
iTAL_gset=pd.read_csv('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/imputed/iTAL_genesets.csv')
CNT_gset=pd.read_csv('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/imputed/CNT_genesets.csv')
Vascular_gset=pd.read_csv('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/imputed/Vascular_gset.csv')
TAL_gset=pd.read_csv('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/imputed/TAL_gset.csv')
PT_gset=pd.read_csv('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/imputed/PT_gset.csv')
Immune_gset=pd.read_csv('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/imputed/Immune_gset.csv')
Fibro_gset=pd.read_csv('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/imputed/Fibro_gset.csv')
Glom_gset=pd.read_csv('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/imputed/Glom_gset.csv')
CD_gset=pd.read_csv('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/imputed/CD_gset.csv')

iPT_gset=iPT_gset.drop('Unnamed: 0', axis=1)
iTAL_gset=iTAL_gset.drop('Unnamed: 0', axis=1)
CNT_gset=CNT_gset.drop('Unnamed: 0', axis=1)
Vascular_gset=Vascular_gset.drop('Unnamed: 0', axis=1)
PT_gset=PT_gset.drop('Unnamed: 0', axis=1)
Immune_gset=Immune_gset.drop('Unnamed: 0', axis=1)
Fibro_gset=Fibro_gset.drop('Unnamed: 0', axis=1)
Glom_gset=Glom_gset.drop('Unnamed: 0', axis=1)
CD_gset=CD_gset.drop('Unnamed: 0', axis=1)
TAL_gset=TAL_gset.drop('Unnamed: 0', axis=1)

iPT_gset

PT_gset

iPT_gset_simple=iPT_gset.set_index('Term')
iPT_gset_simple.loc[iPT_gset_simple['FDR q-val'] > 0.25, 'NES'] = 0
iPT_gset_simple=iPT_gset_simple.drop(['Name', 'ES', 'NOM p-val', 'FDR q-val','FWER p-val', 'Tag %', 'Gene %', 'Lead_genes'], axis=1)
iPT_gset_simple.rename(columns={'NES': 'iPT Niche'}, inplace=True)

iTAL_gset_simple=iTAL_gset.set_index('Term')
iTAL_gset_simple.loc[iTAL_gset_simple['FDR q-val'] > 0.25, 'NES'] = 0
iTAL_gset_simple=iTAL_gset_simple.drop(['Name', 'ES', 'NOM p-val', 'FDR q-val','FWER p-val', 'Tag %', 'Gene %', 'Lead_genes'], axis=1)
iTAL_gset_simple.rename(columns={'NES': 'iTAL Niche'}, inplace=True)

CNT_gset_simple=CNT_gset.set_index('Term')
CNT_gset_simple.loc[CNT_gset_simple['FDR q-val'] > 0.25, 'NES'] = 0
CNT_gset_simple=CNT_gset_simple.drop(['Name', 'ES', 'NOM p-val', 'FDR q-val','FWER p-val', 'Tag %', 'Gene %', 'Lead_genes'], axis=1)
CNT_gset_simple.rename(columns={'NES': 'CNT Niche'}, inplace=True)

Vascular_gset_simple=Vascular_gset.set_index('Term')
Vascular_gset_simple.loc[Vascular_gset_simple['FDR q-val'] > 0.25, 'NES'] = 0
Vascular_gset_simple=Vascular_gset_simple.drop(['Name', 'ES', 'NOM p-val', 'FDR q-val','FWER p-val', 'Tag %', 'Gene %', 'Lead_genes'], axis=1)
Vascular_gset_simple.rename(columns={'NES': 'Vascular Niche'}, inplace=True)

TAL_gset_simple=TAL_gset.set_index('Term')
TAL_gset_simple.loc[TAL_gset_simple['FDR q-val'] > 0.25, 'NES'] = 0
TAL_gset_simple=TAL_gset_simple.drop(['Name', 'ES', 'NOM p-val', 'FDR q-val','FWER p-val', 'Tag %', 'Gene %', 'Lead_genes'], axis=1)
TAL_gset_simple.rename(columns={'NES': 'TAL Niche'}, inplace=True)

PT_gset_simple=PT_gset.set_index('Term')
PT_gset_simple.loc[PT_gset_simple['FDR q-val'] > 0.25, 'NES'] = 0
PT_gset_simple=PT_gset_simple.drop(['Name', 'ES', 'NOM p-val', 'FDR q-val','FWER p-val', 'Tag %', 'Gene %', 'Lead_genes'], axis=1)
PT_gset_simple.rename(columns={'NES': 'PT Niche'}, inplace=True)

Immune_gset_simple=Immune_gset.set_index('Term')
Immune_gset_simple.loc[Immune_gset_simple['FDR q-val'] > 0.25, 'NES'] = 0
Immune_gset_simple=Immune_gset_simple.drop(['Name', 'ES', 'NOM p-val', 'FDR q-val','FWER p-val', 'Tag %', 'Gene %', 'Lead_genes'], axis=1)
Immune_gset_simple.rename(columns={'NES': 'Immune Niche'}, inplace=True)

Fibro_gset_simple=Fibro_gset.set_index('Term')
Fibro_gset_simple.loc[Fibro_gset_simple['FDR q-val'] > 0.25, 'NES'] = 0
Fibro_gset_simple=Fibro_gset_simple.drop(['Name', 'ES', 'NOM p-val', 'FDR q-val','FWER p-val', 'Tag %', 'Gene %', 'Lead_genes'], axis=1)
Fibro_gset_simple.rename(columns={'NES': 'Fibroblast Niche'}, inplace=True)

Glom_gset_simple=Glom_gset.set_index('Term')
Glom_gset_simple.loc[Glom_gset_simple['FDR q-val'] > 0.25, 'NES'] = 0
Glom_gset_simple=Glom_gset_simple.drop(['Name', 'ES', 'NOM p-val', 'FDR q-val','FWER p-val', 'Tag %', 'Gene %', 'Lead_genes'], axis=1)
Glom_gset_simple.rename(columns={'NES': 'Glomerular Niche'}, inplace=True)

CD_gset_simple=CD_gset.set_index('Term')
CD_gset_simple.loc[CD_gset_simple['FDR q-val'] > 0.25, 'NES'] = 0
CD_gset_simple=CD_gset_simple.drop(['Name', 'ES', 'NOM p-val', 'FDR q-val','FWER p-val', 'Tag %', 'Gene %', 'Lead_genes'], axis=1)
CD_gset_simple.rename(columns={'NES': 'CD Niche'}, inplace=True)

iPT_gset_simple

merged_df = pd.concat([PT_gset_simple,iPT_gset_simple], axis=1)
merged_df = pd.concat([merged_df, TAL_gset_simple], axis=1)
merged_df = pd.concat([merged_df,iTAL_gset_simple], axis=1)
merged_df = pd.concat([merged_df, CNT_gset_simple], axis=1)
merged_df = pd.concat([merged_df,CD_gset_simple], axis=1)
merged_df = pd.concat([merged_df,Vascular_gset_simple], axis=1)
merged_df = pd.concat([merged_df, Fibro_gset_simple], axis=1)
merged_df = pd.concat([merged_df, Immune_gset_simple], axis=1)
merged_df = pd.concat([merged_df,Glom_gset_simple], axis=1)
merged_df

##PT: 'Proximal Tubule Transport WP4917', 'Oxidation By Cytochrome P450 WP43','PPAR Alpha Pathway WP2878',
##iPT: 'Apoptosis WP254', 'Electron Transport Chain OXPHOS System In Mitochondria WP111', 'Overview Of Proinflammatory And Profibrotic Mediators WP5095'
##TAL
##iTAL: 'Selective Expression Of Chemokine Receptors During T Cell Polarization WP4494', 'Cytokines And Inflammatory Response WP530', 'Inflammatory Response Pathway WP453',
##'Extrafollicular And Follicular B Cell Activation By SARS CoV 2 WP5218', 'Matrix Metalloproteinases WP129'
##CD
##Vascular:'Monoamine GPCRs WP58', 'Endothelin Pathways WP2197'

merged_df = merged_df.astype(float)

merged_df

merged_df

terms_to_include = [
    'Proximal Tubule Transport WP4917',
    'Oxidation By Cytochrome P450 WP43',
    'PPAR Alpha Pathway WP2878','Electron Transport Chain OXPHOS System In Mitochondria WP111',
    'Apoptosis WP254',
    'Overview Of Proinflammatory And Profibrotic Mediators WP5095',
    'Selective Expression Of Chemokine Receptors During T Cell Polarization WP4494',
    'Cytokines And Inflammatory Response WP530',
    'Inflammatory Response Pathway WP453',
    'Extrafollicular And Follicular B Cell Activation By SARS CoV 2 WP5218',
    'Matrix Metalloproteinases WP129',
    'Monoamine GPCRs WP58',
    'Endothelin Pathways WP2197'
]
subset_df = merged_df.loc[terms_to_include]
plt.figure(figsize=(90, 60))
sns.heatmap(merged_df, annot=False, cmap='coolwarm')

# Add labels and title
plt.title('Heatmap of Gene Set Enrichment')
plt.xlabel('Niche')
plt.ylabel('Term')

# Adjust layout
plt.tight_layout()

# Show the plot
plt.show()

merged_df['sum']=merged_df.sum(axis=1)
merged_df=merged_df[merged_df['sum']!=0]
merged_df.shape

merged_df=merged_df.drop('sum', axis=1)

def filter_rows(row):
    positive_values = row[row > 0]
    if len(positive_values) == 1:
        return True
    return False

filtered_df = merged_df[merged_df.apply(filter_rows, axis=1)]
filtered_df.shape

column_order = ['PT Niche', 'iPT Niche', 'TAL Niche', 'iTAL Niche', 'CNT Niche', 'CD Niche', 'Vascular Niche', 'Fibroblast Niche', 'Immune Niche', 'Glomerular Niche']

# Identify the column with the highest value for each row
filtered_df['max_column'] = filtered_df.idxmax(axis=1)

# Create a mapping for the desired order
column_mapping = {col: i for i, col in enumerate(column_order)}

# Map the max_column to the desired order
filtered_df['max_column_order'] = filtered_df['max_column'].map(column_mapping)

# Sort the DataFrame based on the mapped order
sorted_df = filtered_df.sort_values(by='max_column_order').drop(columns=['max_column', 'max_column_order'])

# Reorder the columns according to the desired order
sorted_df = sorted_df[column_order]

# Create the heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(sorted_df, annot=False, cmap='coolwarm')

# Add labels and title
plt.title('Heatmap of Sorted Gene Set Enrichment')
plt.xlabel('Niche')
plt.ylabel('Term')

# Adjust layout
plt.tight_layout()

# Show the plot
plt.show()

plt.figure(figsize=(30, 30))
sns.heatmap(sorted_df, annot=False, cmap='coolwarm', cbar_kws={'shrink': 0.2})

# Add labels and title
plt.title('Heatmap of Sorted Gene Set Enrichment')
plt.xlabel('Niche')
plt.ylabel('Term')

# Adjust layout
plt.tight_layout()
plt.savefig('Kindey Niche Genesets all.png', dpi=450, bbox_inches='tight')
# Show the plot
plt.show()

column_order = ['PT Niche', 'iPT Niche', 'TAL Niche', 'iTAL Niche', 'CNT Niche', 'CD Niche', 'Vascular Niche', 'Fibroblast Niche', 'Immune Niche', 'Glomerular Niche']

# Identify the column with the highest value for each row
merged_df['max_column'] = merged_df.idxmax(axis=1)

# Create a mapping for the desired order
column_mapping = {col: i for i, col in enumerate(column_order)}

# Map the max_column to the desired order
merged_df['max_column_order'] = merged_df['max_column'].map(column_mapping)

# Sort the DataFrame based on the mapped order
sorted_df = merged_df.sort_values(by='max_column_order').drop(columns=['max_column', 'max_column_order'])

# Reorder the columns according to the desired order
sorted_df = sorted_df[column_order]

# Create the heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(sorted_df, annot=False, cmap='coolwarm')

# Add labels and title
plt.title('Heatmap of Sorted Gene Set Enrichment')
plt.xlabel('Niche')
plt.ylabel('Term')

# Adjust layout
plt.tight_layout()

# Show the plot
plt.show()

def filter_rows(row):
    non_zero_values = row[row != 0]
    if all(non_zero_values < 0):
        return False
    return True

# Apply the filtering function
filtered_df = sorted_df[sorted_df.apply(filter_rows, axis=1)]

# Create the heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(filtered_df, annot=False, cmap='coolwarm')

# Add labels and title
plt.title('Heatmap of Filtered and Sorted Gene Set Enrichment')
plt.xlabel('Niche')
plt.ylabel('Term')

# Adjust layout
plt.tight_layout()

# Show the plot
plt.show()

def filter_rows(row):
    positive_values = row[row > 0]
    if len(positive_values) == 1:
        return True
    return False

filtered_df = sorted_df[sorted_df.apply(filter_rows, axis=1)]

# Create the heatmap
plt.figure(figsize=(50, 40))
sns.heatmap(filtered_df, annot=False, cmap='coolwarm')

# Add labels and title
plt.title('Heatmap of Filtered and Sorted Gene Set Enrichment')
plt.xlabel('Niche')
plt.ylabel('Term')

# Adjust layout
plt.tight_layout()

# Show the plot
plt.show()

def filter_rows(row):
    positive_values = row[row > 0]
    if len(positive_values) == 1:
        return True
    return False

filtered_df = sorted_df[sorted_df.apply(filter_rows, axis=1)]

# Create the heatmap
plt.figure(figsize=(50, 40))
sns.heatmap(filtered_df, annot=False, cmap='coolwarm')

# Add labels and title
plt.title('Heatmap of Filtered and Sorted Gene Set Enrichment')
plt.xlabel('Niche')
plt.ylabel('Term')

# Adjust layout
plt.tight_layout()

# Show the plot
plt.show()

filtered_df

terms_to_include = ['Proximal Tubule Transport WP4917', 'Eicosanoid Metabolism Via Cyclooxygenases COX WP4719','Oxidation By Cytochrome P450 WP43', 'Electron Transport Chain OXPHOS System In Mitochondria WP111',
                    'Oxidative Phosphorylation WP623', 'Mitochondrial Complex I Assembly Model OXPHOS System WP4324','Cell Cycle WP179','TGF Beta Signaling Pathway WP366', 'TNF Related Weak Inducer Of Apoptosis TWEAK Signaling Pathway WP2036',
                    'Sphingolipid Metabolism In Senescence WP5121',  'TNF Alpha Signaling Pathway WP231', 'G1 To S Cell Cycle Control WP45', 'Wnt Signaling Pathway And Pluripotency WP399', 'Sildenafil Treatment WP5294',
                    'RAS And Bradykinin Pathways In COVID 19 WP4969', 'Calcium Regulation In Cardiac Cells WP536', 'Focal Adhesion PI3K Akt mTOR Signaling Pathway WP3932', 'VEGFA VEGFR2 Signaling WP3888','Type I Collagen Synthesis In The Context Of Osteogenesis Imperfecta WP4786',
                     'Complement And Coagulation Cascades WP558', 'Cytosolic DNA Sensing Pathway WP4655', 'Toll Like Receptor Signaling Related To MyD88 WP3858', 'IL 18 Signaling Pathway WP4754', 'Genes Controlling Nephrogenesis WP4823',
                    'Nephrotic Syndrome WP4758', 'Primary Focal Segmental Glomerulosclerosis FSGS WP2572']

subset_df = filtered_df.loc[terms_to_include]

desired_index_order = ['Glomerular Niche', 'Vascular Niche', 'TAL Niche', 'Fibroblast Niche', 'iTAL Niche', 'Immune Niche', 'iPT Niche', 'PT Niche', 'CD Niche', 'CNT Niche']  # Replace with actual cluster labels

#scaled_table = scaled_table.reindex(index=, columns=desired_column_order)
order = ['Genes Controlling Nephrogenesis WP4823','Nephrotic Syndrome WP4758', 'Primary Focal Segmental Glomerulosclerosis FSGS WP2572',
          'RAS And Bradykinin Pathways In COVID 19 WP4969', 'Calcium Regulation In Cardiac Cells WP536','Focal Adhesion PI3K Akt mTOR Signaling Pathway WP3932', 'VEGFA VEGFR2 Signaling WP3888',
           'Type I Collagen Synthesis In The Context Of Osteogenesis Imperfecta WP4786',
          'Complement And Coagulation Cascades WP558',
        'G1 To S Cell Cycle Control WP45', 'Wnt Signaling Pathway And Pluripotency WP399',
         'Cytosolic DNA Sensing Pathway WP4655', 'Toll Like Receptor Signaling Related To MyD88 WP3858', 'IL 18 Signaling Pathway WP4754',
         'Cell Cycle WP179','TGF Beta Signaling Pathway WP366', 'TNF Related Weak Inducer Of Apoptosis TWEAK Signaling Pathway WP2036',
        'Sphingolipid Metabolism In Senescence WP5121',  'TNF Alpha Signaling Pathway WP231',
         'Proximal Tubule Transport WP4917', 'Eicosanoid Metabolism Via Cyclooxygenases COX WP4719','Oxidation By Cytochrome P450 WP43',
         'Electron Transport Chain OXPHOS System In Mitochondria WP111',
         'Oxidative Phosphorylation WP623', 'Mitochondrial Complex I Assembly Model OXPHOS System WP4324', 'Sildenafil Treatment WP5294', ]

subset_df=subset_df.T
subset_df=subset_df.reindex(index=desired_index_order, columns=order)
subset_df=subset_df.T
subset_df.index = subset_df.index.str.replace(r' WP.*$', '', regex=True)
#subset_df=subset_df.reindex(index=terms_to_include, columns=desired_index_order)
plt.figure(figsize=(10, 8))
ax=sns.heatmap(subset_df, annot=False, cmap='coolwarm',cbar_kws={'shrink': 0.2})
ax.xaxis.set_ticks_position('top')
ax.xaxis.set_label_position('top')
ax.yaxis.set_ticks_position('right')
ax.yaxis.set_label_position('right')
plt.xticks(rotation=90)
plt.yticks(rotation=0)
# Add labels and title
plt.title('')
plt.xlabel('')
plt.ylabel('')
plt.tick_params(axis='x', which='both', length=0)
plt.tick_params(axis='y', which='both', length=0)
# Adjust layout
plt.tight_layout()
plt.savefig('Kindey Niche Genesets.png', dpi=450, bbox_inches='tight')
# Show the plot
plt.show()

subset_df.to_csv('niche_genesets.csv', index=True)



subset_df_abbr=pd.read_csv('niche_genesets.csv')

subset_df_abbr=subset_df_abbr.set_index('Term')

subset_df_abbr

plt.figure(figsize=(7, 15))
ax=sns.heatmap(subset_df_abbr, annot=False, cmap='coolwarm',cbar_kws={'shrink': 0.8, 'orientation': 'horizontal', 'pad': 0.01})
ax.xaxis.set_ticks_position('top')
ax.xaxis.set_label_position('top')
ax.yaxis.set_ticks_position('right')
ax.yaxis.set_label_position('right')
plt.xticks(rotation=90, fontsize=16)
plt.yticks(rotation=0, fontsize=10)
# Add labels and title
plt.title('')
plt.xlabel('')
plt.ylabel('')
plt.tick_params(axis='x', which='both', length=0)
plt.tick_params(axis='y', which='both', length=0)
# Adjust layout
plt.tight_layout()
plt.savefig('Kindey Niche Genesets abbr.png', dpi=450, bbox_inches='tight')
# Show the plot
plt.show()

plt.figure(figsize=(7, 15))
ax=sns.heatmap(subset_df_abbr, annot=False, cmap='coolwarm',cbar_kws={'shrink': 0.8, 'orientation': 'horizontal', 'pad': 0.01})
ax.xaxis.set_ticks_position('top')
ax.xaxis.set_label_position('top')
plt.xticks(rotation=90, fontsize=16)
plt.yticks(rotation=0, fontsize=10)
# Add labels and title
plt.title('')
plt.xlabel('')
plt.ylabel('')
plt.tick_params(axis='x', which='both', length=0)
plt.tick_params(axis='y', which='both', length=0)
# Adjust layout
plt.tight_layout()
plt.savefig('Kindey Niche Genesets abbr.png', dpi=450, bbox_inches='tight')
# Show the plot
plt.show()

filtered_df.shape

sorted_df

sorted_df



"""Now the same for niches healthy vs dkd with imputed expression"""



iTAL = pd.read_csv("/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/imputed/healthvsdisease/iTAL Niche_Healthy_DEG.csv")
CNT = pd.read_csv("/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/imputed/healthvsdisease/CNT Niche_Healthy_DEG.csv")
Fibro = pd.read_csv("/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/imputed/healthvsdisease/Fibroblast Niche_Healthy_DEG.csv")
Glom = pd.read_csv("/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/imputed/healthvsdisease/Glomerular Niche_Healthy_DEG.csv")
Immune = pd.read_csv("/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/imputed/healthvsdisease/Immune Niche_Healthy_DEG.csv")
PT = pd.read_csv("/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/imputed/healthvsdisease/PT Niche_Healthy_DEG.csv")
TAL = pd.read_csv("/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/imputed/healthvsdisease/TAL Niche_Healthy_DEG.csv")
Vascular = pd.read_csv("/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/imputed/healthvsdisease/Vascular Niche_Healthy_DEG.csv")
iPT = pd.read_csv("/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/imputed/healthvsdisease/iPT Niche_Healthy_DEG.csv")
CD = pd.read_csv("/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/imputed/healthvsdisease/CD Niche_Healthy_DEG.csv")
iTAL2 = pd.read_csv("/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/imputed/healthvsdisease/iTAL Niche_Disease_DEG.csv")
CNT2 = pd.read_csv("/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/imputed/healthvsdisease/CNT Niche_Disease_DEG.csv")
Fibro2 = pd.read_csv("/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/imputed/healthvsdisease/Fibroblast Niche_Disease_DEG.csv")
Glom2 = pd.read_csv("/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/imputed/healthvsdisease/Glomerular Niche_Disease_DEG.csv")
Immune2 = pd.read_csv("/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/imputed/healthvsdisease/Immune Niche_Disease_DEG.csv")
PT2 = pd.read_csv("/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/imputed/healthvsdisease/PT Niche_Disease_DEG.csv")
TAL2 = pd.read_csv("/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/imputed/healthvsdisease/TAL Niche_Disease_DEG.csv")
Vascular2 = pd.read_csv("/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/imputed/healthvsdisease/Vascular Niche_Disease_DEG.csv")
iPT2 = pd.read_csv("/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/imputed/healthvsdisease/iPT Niche_Disease_DEG.csv")
CD2 = pd.read_csv("/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/imputed/healthvsdisease/CD Niche_Disease_DEG.csv")

populations_to_test = [iTAL,CNT,Fibro,Glom,Immune,PT,TAL,Vascular,iPT,CD,iTAL2,CNT2,Fibro2,Glom2,Immune2,PT2,TAL2,Vascular2,iPT2,CD2]
very_small_value = 1e-310

for i in populations_to_test:
    i['pValue_adj'] = i['pValue_adj'].replace(0, very_small_value)
    i["names"] = i['Gene']
    i["pi_score"] = -1 * np.log10(i["pValue_adj"]) * i["LogFoldChange"] # pvalue
    i.dropna(inplace=True)

Immune

v_library='WikiPathway_2023_Human' ## 'WikiPathway_2023_Human' ###
gset = gseapy.parser.get_library(v_library, min_size=20)



gene_rank = CD[['names','pi_score']]
gene_rank.sort_values(by=['pi_score'], inplace=True, ascending=False)
gene_rank = gene_rank.reset_index(drop=True)
background_genes = gene_rank['names'].tolist()
res = gseapy.prerank(rnk=gene_rank, gene_sets=gset)

#res.res2d.head(20)
res.res2d_plot = res.res2d[res.res2d["FWER p-val"] < 0.05]
res.res2d_plot.head(20)
CD_gset=res.res2d.copy()
CD_gset.shape

gene_rank = CD2[['names','pi_score']]
gene_rank.sort_values(by=['pi_score'], inplace=True, ascending=False)
gene_rank = gene_rank.reset_index(drop=True)
background_genes = gene_rank['names'].tolist()
res = gseapy.prerank(rnk=gene_rank, gene_sets=gset)

#res.res2d.head(20)
res.res2d_plot = res.res2d[res.res2d["FWER p-val"] < 0.05]
res.res2d_plot.head(20)
CD_gset2=res.res2d.copy()
CD_gset2.shape

gene_rank = iTAL[['names','pi_score']]
gene_rank.sort_values(by=['pi_score'], inplace=True, ascending=False)
gene_rank = gene_rank.reset_index(drop=True)

res = gseapy.prerank(rnk=gene_rank, gene_sets=gset)

#res.res2d.head(20)
res.res2d_plot = res.res2d[res.res2d["FWER p-val"] < 0.05]
res.res2d_plot.head(20)
iTAL_gset=res.res2d.copy()
iTAL_gset.shape

gene_rank = iTAL2[['names','pi_score']]
gene_rank.sort_values(by=['pi_score'], inplace=True, ascending=False)
gene_rank = gene_rank.reset_index(drop=True)

res = gseapy.prerank(rnk=gene_rank, gene_sets=gset)

#res.res2d.head(20)
res.res2d_plot = res.res2d[res.res2d["FWER p-val"] < 0.05]
res.res2d_plot.head(20)
iTAL_gset2=res.res2d.copy()
iTAL_gset2.shape

gene_rank = CNT[['names','pi_score']]
gene_rank.sort_values(by=['pi_score'], inplace=True, ascending=False)
gene_rank = gene_rank.reset_index(drop=True)

res = gseapy.prerank(rnk=gene_rank, gene_sets=gset)

#res.res2d.head(20)
res.res2d_plot = res.res2d[res.res2d["FWER p-val"] < 0.05]
res.res2d_plot.head(20)
CNT_gset=res.res2d.copy()
CNT_gset.shape

gene_rank = CNT2[['names','pi_score']]
gene_rank.sort_values(by=['pi_score'], inplace=True, ascending=False)
gene_rank = gene_rank.reset_index(drop=True)

res = gseapy.prerank(rnk=gene_rank, gene_sets=gset)

#res.res2d.head(20)
res.res2d_plot = res.res2d[res.res2d["FWER p-val"] < 0.05]
res.res2d_plot.head(20)
CNT_gset2=res.res2d.copy()
CNT_gset2.shape

gene_rank = Fibro[['names','pi_score']]
gene_rank.sort_values(by=['pi_score'], inplace=True, ascending=False)
gene_rank = gene_rank.reset_index(drop=True)

res = gseapy.prerank(rnk=gene_rank, gene_sets=gset)

#res.res2d.head(20)
res.res2d_plot = res.res2d[res.res2d["FWER p-val"] < 0.05]
res.res2d_plot.head(20)
Fibro_gset=res.res2d.copy()
Fibro_gset.shape

gene_rank = Fibro2[['names','pi_score']]
gene_rank.sort_values(by=['pi_score'], inplace=True, ascending=False)
gene_rank = gene_rank.reset_index(drop=True)

res = gseapy.prerank(rnk=gene_rank, gene_sets=gset)

#res.res2d.head(20)
res.res2d_plot = res.res2d[res.res2d["FWER p-val"] < 0.05]
res.res2d_plot.head(20)
Fibro_gset2=res.res2d.copy()
Fibro_gset2.shape

gene_rank = Glom[['names','pi_score']]
gene_rank.sort_values(by=['pi_score'], inplace=True, ascending=False)
gene_rank = gene_rank.reset_index(drop=True)

res = gseapy.prerank(rnk=gene_rank, gene_sets=gset)

#res.res2d.head(20)
res.res2d_plot = res.res2d[res.res2d["FWER p-val"] < 0.05]
res.res2d_plot.head(20)
Glom_gset=res.res2d.copy()
Glom_gset.shape

gene_rank = Glom2[['names','pi_score']]
gene_rank.sort_values(by=['pi_score'], inplace=True, ascending=False)
gene_rank = gene_rank.reset_index(drop=True)

res = gseapy.prerank(rnk=gene_rank, gene_sets=gset)

#res.res2d.head(20)
res.res2d_plot = res.res2d[res.res2d["FWER p-val"] < 0.05]
res.res2d_plot.head(20)
Glom_gset2=res.res2d.copy()
Glom_gset2.shape

gene_rank = Immune[['names','pi_score']]
gene_rank.sort_values(by=['pi_score'], inplace=True, ascending=False)
gene_rank = gene_rank.reset_index(drop=True)

res = gseapy.prerank(rnk=gene_rank, gene_sets=gset)

#res.res2d.head(20)
res.res2d_plot = res.res2d[res.res2d["FWER p-val"] < 0.05]
res.res2d_plot.head(20)
Immune_gset=res.res2d.copy()
Immune_gset.shape

gene_rank = Immune2[['names','pi_score']]
gene_rank.sort_values(by=['pi_score'], inplace=True, ascending=False)
gene_rank = gene_rank.reset_index(drop=True)

res = gseapy.prerank(rnk=gene_rank, gene_sets=gset)

#res.res2d.head(20)
res.res2d_plot = res.res2d[res.res2d["FWER p-val"] < 0.05]
res.res2d_plot.head(20)
Immune_gset2=res.res2d.copy()
Immune_gset2.shape

gene_rank = PT[['names','pi_score']]
gene_rank.sort_values(by=['pi_score'], inplace=True, ascending=False)
gene_rank = gene_rank.reset_index(drop=True)

res = gseapy.prerank(rnk=gene_rank, gene_sets=gset)

#res.res2d.head(20)
res.res2d_plot = res.res2d[res.res2d["FWER p-val"] < 0.05]
res.res2d_plot.head(20)
PT_gset=res.res2d.copy()
PT_gset.shape

gene_rank = PT2[['names','pi_score']]
gene_rank.sort_values(by=['pi_score'], inplace=True, ascending=False)
gene_rank = gene_rank.reset_index(drop=True)

res = gseapy.prerank(rnk=gene_rank, gene_sets=gset)

#res.res2d.head(20)
res.res2d_plot = res.res2d[res.res2d["FWER p-val"] < 0.05]
res.res2d_plot.head(20)
PT_gset2=res.res2d.copy()
PT_gset2.shape

gene_rank = TAL[['names','pi_score']]
gene_rank.sort_values(by=['pi_score'], inplace=True, ascending=False)
gene_rank = gene_rank.reset_index(drop=True)

res = gseapy.prerank(rnk=gene_rank, gene_sets=gset)

#res.res2d.head(20)
res.res2d_plot = res.res2d[res.res2d["FWER p-val"] < 0.05]
res.res2d_plot.head(20)
TAL_gset=res.res2d.copy()
TAL_gset.shape

gene_rank = TAL2[['names','pi_score']]
gene_rank.sort_values(by=['pi_score'], inplace=True, ascending=False)
gene_rank = gene_rank.reset_index(drop=True)

res = gseapy.prerank(rnk=gene_rank, gene_sets=gset)

#res.res2d.head(20)
res.res2d_plot = res.res2d[res.res2d["FWER p-val"] < 0.05]
res.res2d_plot.head(20)
TAL_gset2=res.res2d.copy()
TAL_gset2.shape

gene_rank = Vascular[['names','pi_score']]
gene_rank.sort_values(by=['pi_score'], inplace=True, ascending=False)
gene_rank = gene_rank.reset_index(drop=True)

res = gseapy.prerank(rnk=gene_rank, gene_sets=gset)

#res.res2d.head(20)
res.res2d_plot = res.res2d[res.res2d["FWER p-val"] < 0.05]
res.res2d_plot.head(20)
Vascular_gset=res.res2d.copy()
Vascular_gset.shape

gene_rank = Vascular2[['names','pi_score']]
gene_rank.sort_values(by=['pi_score'], inplace=True, ascending=False)
gene_rank = gene_rank.reset_index(drop=True)

res = gseapy.prerank(rnk=gene_rank, gene_sets=gset)

#res.res2d.head(20)
res.res2d_plot = res.res2d[res.res2d["FWER p-val"] < 0.05]
res.res2d_plot.head(20)
Vascular_gset2=res.res2d.copy()
Vascular_gset2.shape

gene_rank = iPT[['names','pi_score']]
gene_rank.sort_values(by=['pi_score'], inplace=True, ascending=False)
gene_rank = gene_rank.reset_index(drop=True)

res = gseapy.prerank(rnk=gene_rank, gene_sets=gset)

#res.res2d.head(20)
res.res2d_plot = res.res2d[res.res2d["FWER p-val"] < 0.05]
res.res2d_plot.head(20)
iPT_gset=res.res2d.copy()
iPT_gset.shape

gene_rank = iPT2[['names','pi_score']]
gene_rank.sort_values(by=['pi_score'], inplace=True, ascending=False)
gene_rank = gene_rank.reset_index(drop=True)

res = gseapy.prerank(rnk=gene_rank, gene_sets=gset)

#res.res2d.head(20)
res.res2d_plot = res.res2d[res.res2d["FWER p-val"] < 0.05]
res.res2d_plot.head(20)
iPT_gset2=res.res2d.copy()
iPT_gset2.shape

iPT_gset.to_csv('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/imputed/healthvsdisease/iPT_genesets.csv')
iTAL_gset.to_csv('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/imputed/healthvsdisease/iTAL_genesets.csv')
CNT_gset.to_csv('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/imputed/healthvsdisease/CNT_genesets.csv')
Vascular_gset.to_csv('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/imputed/healthvsdisease/Vascular_gset.csv')
TAL_gset.to_csv('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/imputed/healthvsdisease/TAL_gset.csv')
PT_gset.to_csv('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/imputed/healthvsdisease/PT_gset.csv')
Immune_gset.to_csv('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/imputed/healthvsdisease/Immune_gset.csv')
Fibro_gset.to_csv('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/imputed/healthvsdisease/Fibro_gset.csv')
Glom_gset.to_csv('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/imputed/healthvsdisease/Glom_gset.csv')
CD_gset.to_csv('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/imputed/healthvsdisease/CD_gset.csv')
iPT_gset2.to_csv('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/imputed/healthvsdisease/iPT_genesets2.csv')
iTAL_gset2.to_csv('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/imputed/healthvsdisease/iTAL_genesets2.csv')
CNT_gset2.to_csv('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/imputed/healthvsdisease/CNT_genesets2.csv')
Vascular_gset2.to_csv('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/imputed/healthvsdisease/Vascular_gset2.csv')
TAL_gset2.to_csv('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/imputed/healthvsdisease/TAL_gset2.csv')
PT_gset2.to_csv('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/imputed/healthvsdisease/PT_gset2.csv')
Immune_gset2.to_csv('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/imputed/healthvsdisease/Immune_gset2.csv')
Fibro_gset2.to_csv('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/imputed/healthvsdisease/Fibro_gset2.csv')
Glom_gset2.to_csv('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/imputed/healthvsdisease/Glom_gset2.csv')
CD_gset2.to_csv('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/imputed/healthvsdisease/CD_gset2.csv')

iPT_gset=pd.read_csv('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/imputed/healthvsdisease/iPT_genesets.csv')
iTAL_gset=pd.read_csv('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/imputed/healthvsdisease/iTAL_genesets.csv')
CNT_gset=pd.read_csv('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/imputed/healthvsdisease/CNT_genesets.csv')
Vascular_gset=pd.read_csv('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/imputed/healthvsdisease/Vascular_gset.csv')
TAL_gset=pd.read_csv('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/imputed/healthvsdisease/TAL_gset.csv')
PT_gset=pd.read_csv('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/imputed/healthvsdisease/PT_gset.csv')
Immune_gset=pd.read_csv('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/imputed/healthvsdisease/Immune_gset.csv')
Fibro_gset=pd.read_csv('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/imputed/healthvsdisease/Fibro_gset.csv')
Glom_gset=pd.read_csv('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/imputed/healthvsdisease/Glom_gset.csv')
CD_gset=pd.read_csv('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/imputed/healthvsdisease/CD_gset.csv')
iPT_gset2=pd.read_csv('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/imputed/healthvsdisease/iPT_genesets2.csv')
iTAL_gset2=pd.read_csv('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/imputed/healthvsdisease/iTAL_genesets2.csv')
CNT_gset2=pd.read_csv('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/imputed/healthvsdisease/CNT_genesets2.csv')
Vascular_gset2=pd.read_csv('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/imputed/healthvsdisease/Vascular_gset2.csv')
TAL_gset2=pd.read_csv('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/imputed/healthvsdisease/TAL_gset2.csv')
PT_gset2=pd.read_csv('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/imputed/healthvsdisease/PT_gset2.csv')
Immune_gset2=pd.read_csv('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/imputed/healthvsdisease/Immune_gset2.csv')
Fibro_gset2=pd.read_csv('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/imputed/healthvsdisease/Fibro_gset2.csv')
Glom_gset2=pd.read_csv('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/imputed/healthvsdisease/Glom_gset2.csv')
CD_gset2=pd.read_csv('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/imputed/healthvsdisease/CD_gset2.csv')

iPT_gset=iPT_gset.drop('Unnamed: 0', axis=1)
iTAL_gset=iTAL_gset.drop('Unnamed: 0', axis=1)
CNT_gset=CNT_gset.drop('Unnamed: 0', axis=1)
Vascular_gset=Vascular_gset.drop('Unnamed: 0', axis=1)
PT_gset=PT_gset.drop('Unnamed: 0', axis=1)
Immune_gset=Immune_gset.drop('Unnamed: 0', axis=1)
Fibro_gset=Fibro_gset.drop('Unnamed: 0', axis=1)
Glom_gset=Glom_gset.drop('Unnamed: 0', axis=1)
CD_gset=CD_gset.drop('Unnamed: 0', axis=1)
TAL_gset=TAL_gset.drop('Unnamed: 0', axis=1)
iPT_gset2=iPT_gset2.drop('Unnamed: 0', axis=1)
iTAL_gset2=iTAL_gset2.drop('Unnamed: 0', axis=1)
CNT_gset2=CNT_gset2.drop('Unnamed: 0', axis=1)
Vascular_gset2=Vascular_gset2.drop('Unnamed: 0', axis=1)
PT_gset2=PT_gset2.drop('Unnamed: 0', axis=1)
Immune_gset2=Immune_gset2.drop('Unnamed: 0', axis=1)
Fibro_gset2=Fibro_gset2.drop('Unnamed: 0', axis=1)
Glom_gset2=Glom_gset2.drop('Unnamed: 0', axis=1)
CD_gset2=CD_gset2.drop('Unnamed: 0', axis=1)
TAL_gset2=TAL_gset2.drop('Unnamed: 0', axis=1)

Glom_gset



from gseapy import dotplot
# to save your figure, make sure that ``ofname`` is not None
ax = dotplot(iTAL_gset2,
             column="FDR q-val",
             title='iTAL Niche DKD',
             cmap=plt.cm.coolwarm,
             size=8, # adjust dot size
             figsize=(4,7), cutoff=0.25, show_ring=False)
ax.tick_params(axis='y', which='both', left=False, right=False)
ax.grid(False)
# Remove the grey grid background
ax.set_facecolor('white')
ax.set_xlabel('')
plt.savefig('iTAL_Niche_Disease_GSEA.png', dpi=450, bbox_inches='tight')

# Show the plot
plt.show()

from gseapy import dotplot
# to save your figure, make sure that ``ofname`` is not None
ax = dotplot(CNT_gset2,
             column="FDR q-val",
             title='CNT Niche DKD',
             cmap=plt.cm.coolwarm,
             size=8, # adjust dot size
             figsize=(8,10), cutoff=0.25, show_ring=False)
ax.tick_params(axis='y', which='both', left=False, right=False)
ax.grid(False)
# Remove the grey grid background
ax.set_facecolor('white')
ax.set_xlabel('')
plt.savefig('CNT_Niche_Disease_GSEA.png', dpi=450, bbox_inches='tight')

# Show the plot
plt.show()

from gseapy import dotplot
# to save your figure, make sure that ``ofname`` is not None
ax = dotplot(Vascular_gset2,
             column="FDR q-val",
             title='Vascular Niche DKD',
             cmap=plt.cm.coolwarm,
             size=8, # adjust dot size
             figsize=(8,10), cutoff=0.25, show_ring=False)
ax.tick_params(axis='y', which='both', left=False, right=False)
ax.grid(False)
# Remove the grey grid background
ax.set_facecolor('white')
ax.set_xlabel('')
plt.savefig('Vascular_Niche_Disease_GSEA.png', dpi=450, bbox_inches='tight')

# Show the plot
plt.show()

from gseapy import dotplot
# to save your figure, make sure that ``ofname`` is not None
ax = dotplot(PT_gset2,
             column="FDR q-val",
             title='PT Niche DKD',
             cmap=plt.cm.coolwarm,
             size=8, # adjust dot size
             figsize=(8,10), cutoff=0.25, show_ring=False)
ax.tick_params(axis='y', which='both', left=False, right=False)
ax.grid(False)
# Remove the grey grid background
ax.set_facecolor('white')
ax.set_xlabel('')
plt.savefig('PT_Niche_Disease_GSEA.png', dpi=450, bbox_inches='tight')

# Show the plot
plt.show()

from gseapy import dotplot
# to save your figure, make sure that ``ofname`` is not None
ax = dotplot(Immune_gset2,
             column="FDR q-val",
             title='Immune Niche DKD',
             cmap=plt.cm.coolwarm,
             size=8, # adjust dot size
             figsize=(8,10), cutoff=0.25, show_ring=False)
ax.tick_params(axis='y', which='both', left=False, right=False)
ax.grid(False)
# Remove the grey grid background
ax.set_facecolor('white')
ax.set_xlabel('')
plt.savefig('Immune_Niche_Disease_GSEA.png', dpi=450, bbox_inches='tight')

# Show the plot
plt.show()

from gseapy import dotplot
# to save your figure, make sure that ``ofname`` is not None
ax = dotplot(Fibro_gset2,
             column="FDR q-val",
             title='Fibroblast Niche DKD',
             cmap=plt.cm.coolwarm,
             size=8, # adjust dot size
             figsize=(8,10), cutoff=0.25, show_ring=False)
ax.tick_params(axis='y', which='both', left=False, right=False)
ax.grid(False)
# Remove the grey grid background
ax.set_facecolor('white')
ax.set_xlabel('')
plt.savefig('Fibroblast_Niche_Disease_GSEA.png', dpi=450, bbox_inches='tight')

# Show the plot
plt.show()

from gseapy import dotplot
# to save your figure, make sure that ``ofname`` is not None
ax = dotplot(Glom_gset2,
             column="FDR q-val",
             title='Glomerular Niche DKD',
             cmap=plt.cm.coolwarm,
             size=8, # adjust dot size
             figsize=(8,10), cutoff=0.25, show_ring=False)
ax.tick_params(axis='y', which='both', left=False, right=False)
ax.grid(False)
# Remove the grey grid background
ax.set_facecolor('white')
ax.set_xlabel('')
plt.savefig('Glomerular_Niche_Disease_GSEA.png', dpi=450, bbox_inches='tight')

# Show the plot
plt.show()

from gseapy import dotplot
# to save your figure, make sure that ``ofname`` is not None
ax = dotplot(CD_gset2,
             column="FDR q-val",
             title='CD Niche DKD',
             cmap=plt.cm.coolwarm,
             size=8, # adjust dot size
             figsize=(8,10), cutoff=0.25, show_ring=False)
ax.tick_params(axis='y', which='both', left=False, right=False)
ax.grid(False)
# Remove the grey grid background
ax.set_facecolor('white')
ax.set_xlabel('')
plt.savefig('CD_Niche_Disease_GSEA.png', dpi=450, bbox_inches='tight')

# Show the plot
plt.show()

TAL_gset2_copy=TAL_gset2.copy()
TAL_gset2_copy['Term'] = TAL_gset2_copy['Term'].str.replace(r' WP.*$', '', regex=True)

from gseapy import dotplot
# to save your figure, make sure that ``ofname`` is not None
ax = dotplot(TAL_gset2_copy,
             column="FDR q-val",
             title='',
             cmap=plt.cm.coolwarm,
             size=8, # adjust dot size
             figsize=(4,5), cutoff=0.25, show_ring=False, vmin=0)
ax.tick_params(axis='y',labelsize=25, which='both', left=False, right=False)
ax.grid(False)
# Remove the grey grid background
ax.set_facecolor('white')
ax.set_xlabel('')
plt.savefig('TAL_Niche_Disease_GSEA.png', dpi=450, bbox_inches='tight')

# Show the plot
plt.show()

from gseapy import dotplot
# to save your figure, make sure that ``ofname`` is not None
ax = dotplot(iPT_gset2,
             column="FDR q-val",
             title='iPT Niche Disease',
             cmap=plt.cm.coolwarm,
             size=8, # adjust dot size
             figsize=(8,10), cutoff=0.25, show_ring=False)
ax.tick_params(axis='y', which='both', left=False, right=False)
ax.grid(False)
# Remove the grey grid background
ax.set_facecolor('white')
ax.set_xlabel('')
plt.savefig('iPT_Niche_Disease_GSEA.png', dpi=450, bbox_inches='tight')

# Show the plot
plt.show()

iPT_gset_simple=iPT_gset.set_index('Term')
iPT_gset_simple.loc[iPT_gset_simple['FDR q-val'] > 0.25, 'NES'] = 0
iPT_gset_simple=iPT_gset_simple.drop(['Name', 'ES', 'NOM p-val', 'FDR q-val','FWER p-val', 'Tag %', 'Gene %', 'Lead_genes'], axis=1)
iPT_gset_simple.rename(columns={'NES': 'iPT Niche Control'}, inplace=True)

iTAL_gset_simple=iTAL_gset.set_index('Term')
iTAL_gset_simple.loc[iTAL_gset_simple['FDR q-val'] > 0.25, 'NES'] = 0
iTAL_gset_simple=iTAL_gset_simple.drop(['Name', 'ES', 'NOM p-val', 'FDR q-val','FWER p-val', 'Tag %', 'Gene %', 'Lead_genes'], axis=1)
iTAL_gset_simple.rename(columns={'NES': 'iTAL Niche Control'}, inplace=True)

CNT_gset_simple=CNT_gset.set_index('Term')
CNT_gset_simple.loc[CNT_gset_simple['FDR q-val'] > 0.25, 'NES'] = 0
CNT_gset_simple=CNT_gset_simple.drop(['Name', 'ES', 'NOM p-val', 'FDR q-val','FWER p-val', 'Tag %', 'Gene %', 'Lead_genes'], axis=1)
CNT_gset_simple.rename(columns={'NES': 'CNT Niche Control'}, inplace=True)

Vascular_gset_simple=Vascular_gset.set_index('Term')
Vascular_gset_simple.loc[Vascular_gset_simple['FDR q-val'] > 0.25, 'NES'] = 0
Vascular_gset_simple=Vascular_gset_simple.drop(['Name', 'ES', 'NOM p-val', 'FDR q-val','FWER p-val', 'Tag %', 'Gene %', 'Lead_genes'], axis=1)
Vascular_gset_simple.rename(columns={'NES': 'Vascular Niche Control'}, inplace=True)

TAL_gset_simple=TAL_gset.set_index('Term')
TAL_gset_simple.loc[TAL_gset_simple['FDR q-val'] > 0.25, 'NES'] = 0
TAL_gset_simple=TAL_gset_simple.drop(['Name', 'ES', 'NOM p-val', 'FDR q-val','FWER p-val', 'Tag %', 'Gene %', 'Lead_genes'], axis=1)
TAL_gset_simple.rename(columns={'NES': 'TAL Niche Control'}, inplace=True)

PT_gset_simple=PT_gset.set_index('Term')
PT_gset_simple.loc[PT_gset_simple['FDR q-val'] > 0.25, 'NES'] = 0
PT_gset_simple=PT_gset_simple.drop(['Name', 'ES', 'NOM p-val', 'FDR q-val','FWER p-val', 'Tag %', 'Gene %', 'Lead_genes'], axis=1)
PT_gset_simple.rename(columns={'NES': 'PT Niche Control'}, inplace=True)

Immune_gset_simple=Immune_gset.set_index('Term')
Immune_gset_simple.loc[Immune_gset_simple['FDR q-val'] > 0.25, 'NES'] = 0
Immune_gset_simple=Immune_gset_simple.drop(['Name', 'ES', 'NOM p-val', 'FDR q-val','FWER p-val', 'Tag %', 'Gene %', 'Lead_genes'], axis=1)
Immune_gset_simple.rename(columns={'NES': 'Immune Niche Control'}, inplace=True)

Fibro_gset_simple=Fibro_gset.set_index('Term')
Fibro_gset_simple.loc[Fibro_gset_simple['FDR q-val'] > 0.25, 'NES'] = 0
Fibro_gset_simple=Fibro_gset_simple.drop(['Name', 'ES', 'NOM p-val', 'FDR q-val','FWER p-val', 'Tag %', 'Gene %', 'Lead_genes'], axis=1)
Fibro_gset_simple.rename(columns={'NES': 'Fibroblast Niche Control'}, inplace=True)

Glom_gset_simple=Glom_gset.set_index('Term')
Glom_gset_simple.loc[Glom_gset_simple['FDR q-val'] > 0.25, 'NES'] = 0
Glom_gset_simple=Glom_gset_simple.drop(['Name', 'ES', 'NOM p-val', 'FDR q-val','FWER p-val', 'Tag %', 'Gene %', 'Lead_genes'], axis=1)
Glom_gset_simple.rename(columns={'NES': 'Glomerular Niche Control'}, inplace=True)

CD_gset_simple=CD_gset.set_index('Term')
CD_gset_simple.loc[CD_gset_simple['FDR q-val'] > 0.25, 'NES'] = 0
CD_gset_simple=CD_gset_simple.drop(['Name', 'ES', 'NOM p-val', 'FDR q-val','FWER p-val', 'Tag %', 'Gene %', 'Lead_genes'], axis=1)
CD_gset_simple.rename(columns={'NES': 'CD Niche Control'}, inplace=True)

iPT_gset_simple2=iPT_gset2.set_index('Term')
iPT_gset_simple2.loc[iPT_gset_simple2['FDR q-val'] > 0.25, 'NES'] = 0
iPT_gset_simple2=iPT_gset_simple2.drop(['Name', 'ES', 'NOM p-val', 'FDR q-val','FWER p-val', 'Tag %', 'Gene %', 'Lead_genes'], axis=1)
iPT_gset_simple2.rename(columns={'NES': 'iPT Niche DKD'}, inplace=True)

iTAL_gset_simple2=iTAL_gset2.set_index('Term')
iTAL_gset_simple2.loc[iTAL_gset_simple2['FDR q-val'] > 0.25, 'NES'] = 0
iTAL_gset_simple2=iTAL_gset_simple2.drop(['Name', 'ES', 'NOM p-val', 'FDR q-val','FWER p-val', 'Tag %', 'Gene %', 'Lead_genes'], axis=1)
iTAL_gset_simple2.rename(columns={'NES': 'iTAL Niche DKD'}, inplace=True)

CNT_gset_simple2=CNT_gset2.set_index('Term')
CNT_gset_simple2.loc[CNT_gset_simple2['FDR q-val'] > 0.25, 'NES'] = 0
CNT_gset_simple2=CNT_gset_simple2.drop(['Name', 'ES', 'NOM p-val', 'FDR q-val','FWER p-val', 'Tag %', 'Gene %', 'Lead_genes'], axis=1)
CNT_gset_simple2.rename(columns={'NES': 'CNT Niche DKD'}, inplace=True)

Vascular_gset_simple2=Vascular_gset2.set_index('Term')
Vascular_gset_simple2.loc[Vascular_gset_simple2['FDR q-val'] > 0.25, 'NES'] = 0
Vascular_gset_simple2=Vascular_gset_simple2.drop(['Name', 'ES', 'NOM p-val', 'FDR q-val','FWER p-val', 'Tag %', 'Gene %', 'Lead_genes'], axis=1)
Vascular_gset_simple2.rename(columns={'NES': 'Vascular Niche DKD'}, inplace=True)

TAL_gset_simple2=TAL_gset2.set_index('Term')
TAL_gset_simple2.loc[TAL_gset_simple2['FDR q-val'] > 0.25, 'NES'] = 0
TAL_gset_simple2=TAL_gset_simple2.drop(['Name', 'ES', 'NOM p-val', 'FDR q-val','FWER p-val', 'Tag %', 'Gene %', 'Lead_genes'], axis=1)
TAL_gset_simple2.rename(columns={'NES': 'TAL Niche DKD'}, inplace=True)

PT_gset_simple2=PT_gset2.set_index('Term')
PT_gset_simple2.loc[PT_gset_simple2['FDR q-val'] > 0.25, 'NES'] = 0
PT_gset_simple2=PT_gset_simple2.drop(['Name', 'ES', 'NOM p-val', 'FDR q-val','FWER p-val', 'Tag %', 'Gene %', 'Lead_genes'], axis=1)
PT_gset_simple2.rename(columns={'NES': 'PT Niche DKD'}, inplace=True)

Immune_gset_simple2=Immune_gset2.set_index('Term')
Immune_gset_simple2.loc[Immune_gset_simple2['FDR q-val'] > 0.25, 'NES'] = 0
Immune_gset_simple2=Immune_gset_simple2.drop(['Name', 'ES', 'NOM p-val', 'FDR q-val','FWER p-val', 'Tag %', 'Gene %', 'Lead_genes'], axis=1)
Immune_gset_simple2.rename(columns={'NES': 'Immune Niche DKD'}, inplace=True)

Fibro_gset_simple2=Fibro_gset2.set_index('Term')
Fibro_gset_simple2.loc[Fibro_gset_simple2['FDR q-val'] > 0.25, 'NES'] = 0
Fibro_gset_simple2=Fibro_gset_simple2.drop(['Name', 'ES', 'NOM p-val', 'FDR q-val','FWER p-val', 'Tag %', 'Gene %', 'Lead_genes'], axis=1)
Fibro_gset_simple2.rename(columns={'NES': 'Fibroblast Niche DKD'}, inplace=True)

Glom_gset_simple2=Glom_gset2.set_index('Term')
Glom_gset_simple2.loc[Glom_gset_simple2['FDR q-val'] > 0.25, 'NES'] = 0
Glom_gset_simple2=Glom_gset_simple2.drop(['Name', 'ES', 'NOM p-val', 'FDR q-val','FWER p-val', 'Tag %', 'Gene %', 'Lead_genes'], axis=1)
Glom_gset_simple2.rename(columns={'NES': 'Glomerular Niche DKD'}, inplace=True)

CD_gset_simple2=CD_gset2.set_index('Term')
CD_gset_simple2.loc[CD_gset_simple2['FDR q-val'] > 0.25, 'NES'] = 0
CD_gset_simple2=CD_gset_simple2.drop(['Name', 'ES', 'NOM p-val', 'FDR q-val','FWER p-val', 'Tag %', 'Gene %', 'Lead_genes'], axis=1)
CD_gset_simple2.rename(columns={'NES': 'CD Niche DKD'}, inplace=True)

merged_df = pd.concat([PT_gset_simple,PT_gset_simple2], axis=1)
merged_df = pd.concat([merged_df,iPT_gset_simple], axis=1)
merged_df = pd.concat([merged_df,iPT_gset_simple2], axis=1)
merged_df = pd.concat([merged_df, TAL_gset_simple], axis=1)
merged_df = pd.concat([merged_df, TAL_gset_simple2], axis=1)
merged_df = pd.concat([merged_df,iTAL_gset_simple], axis=1)
merged_df = pd.concat([merged_df,iTAL_gset_simple2], axis=1)
merged_df = pd.concat([merged_df, CNT_gset_simple], axis=1)
merged_df = pd.concat([merged_df, CNT_gset_simple2], axis=1)
merged_df = pd.concat([merged_df,CD_gset_simple], axis=1)
merged_df = pd.concat([merged_df,CD_gset_simple2], axis=1)
merged_df = pd.concat([merged_df,Vascular_gset_simple], axis=1)
merged_df = pd.concat([merged_df,Vascular_gset_simple2], axis=1)
merged_df = pd.concat([merged_df, Fibro_gset_simple], axis=1)
merged_df = pd.concat([merged_df, Fibro_gset_simple2], axis=1)
merged_df = pd.concat([merged_df, Immune_gset_simple], axis=1)
merged_df = pd.concat([merged_df, Immune_gset_simple2], axis=1)
merged_df = pd.concat([merged_df,Glom_gset_simple], axis=1)
merged_df = pd.concat([merged_df,Glom_gset_simple2], axis=1)
merged_df

merged_df['sum']=merged_df.sum(axis=1)
merged_df=merged_df[merged_df['sum']!=0]
merged_df=merged_df.drop('sum', axis=1)

merged_df.shape

plt.figure(figsize=(50, 40))
sns.heatmap(merged_df, annot=False, cmap='coolwarm')

# Add labels and title
plt.title('Heatmap of Sorted Gene Set Enrichment')
plt.xlabel('Niche')
plt.ylabel('Term')

# Adjust layout
plt.tight_layout()

# Show the plot
plt.show()

def filter_rows(row):
    positive_values = row[row > 0]
    if len(positive_values) == 1:
        return True
    return False

filtered_df = merged_df[merged_df.apply(filter_rows, axis=1)]
filtered_df.shape

column_order = ['PT Niche Control','PT Niche DKD', 'iPT Niche Control', 'iPT Niche DKD','TAL Niche Control','TAL Niche DKD',
                'iTAL Niche Control','iTAL Niche DKD',
                'CNT Niche Control', 'CNT Niche DKD','CD Niche Control', 'CD Niche DKD','Vascular Niche Control', 'Vascular Niche DKD',
                'Fibroblast Niche Control', 'Fibroblast Niche DKD',
                'Immune Niche Control','Immune Niche DKD', 'Glomerular Niche Control','Glomerular Niche DKD']

# Identify the column with the highest value for each row
filtered_df['max_column'] = filtered_df.idxmax(axis=1)

# Create a mapping for the desired order
column_mapping = {col: i for i, col in enumerate(column_order)}

# Map the max_column to the desired order
filtered_df['max_column_order'] = filtered_df['max_column'].map(column_mapping)

# Sort the DataFrame based on the mapped order
sorted_df = filtered_df.sort_values(by='max_column_order').drop(columns=['max_column', 'max_column_order'])

# Reorder the columns according to the desired order
sorted_df = sorted_df[column_order]

# Create the heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(sorted_df, annot=False, cmap='coolwarm')

# Add labels and title
plt.title('Heatmap of Sorted Gene Set Enrichment')
plt.xlabel('Niche')
plt.ylabel('Term')

# Adjust layout
plt.tight_layout()

# Show the plot
plt.show()

plt.figure(figsize=(30, 30))
sns.heatmap(sorted_df, annot=False, cmap='coolwarm')

# Add labels and title
plt.title('Heatmap of Sorted Gene Set Enrichment')
plt.xlabel('Niche')
plt.ylabel('Term')

# Adjust layout
plt.tight_layout()

# Show the plot
plt.show()













adata_all_subset.obs['type']=adata_all_subset.obs['type'].replace('Pediatric', 'Healthy')
adata_all_subset=adata_all_subset[adata_all_subset.obs['type']!='Diabetes']
adata_all_subset=adata_all_subset[adata_all_subset.obs['sample']!='HK2874']
adata_all_subset







adata_all_subset_glom=adata_all_subset[adata_all_subset.obs['cluster_labels_annotated_coarse_coarse']=='Glomerular Niche']

adata_all_subset_glom
adata_all_subset_glom.X = adata_all_subset_glom.layers["counts"]
sc.pp.log1p(adata_all_subset_glom)
sc.tl.rank_genes_groups(adata_all_subset_glom, groupby='type', method='wilcoxon', pts = True)
sc.pl.rank_genes_groups(adata_all_subset_glom, n_genes=25, sharey=False)

de_results = adata_all_subset_glom.uns['rank_genes_groups']
for group in de_results['names'].dtype.names:
        gene_names = de_results['names'][group]
        gene_scores = de_results['scores'][group]
        gene_logfoldchanges = de_results['logfoldchanges'][group]
        gene_pvals = de_results['pvals'][group]
        gene_pvals_adj = de_results['pvals_adj'][group]

        # Create a DataFrame
        df = pd.DataFrame({
            'Gene': gene_names,
            'Score': gene_scores,
            'LogFoldChange': gene_logfoldchanges,
            'pValue': gene_pvals,
            'pValue_adj': gene_pvals_adj
        })
        print(df.head(40))
        df.to_csv(f'/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/Glomerular_{group}_DEGs.csv')

de_results = adata_all_subset_glom.uns['rank_genes_groups']
for group in de_results['names'].dtype.names:
        print(f'Glomerular_{group}_DEGs.csv')

Glom_healthy = pd.read_csv("/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/Glomerular_Healthy_DEGs.csv", index_col = 'Unnamed: 0')
Glom_disease = pd.read_csv("/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/Glomerular_Disease_DEGs.csv", index_col = 'Unnamed: 0')

populations_to_test = [Glom_healthy,Glom_disease]
very_small_value = 1e-310

for i in populations_to_test:
    i['pValue_adj'] = i['pValue_adj'].replace(0, very_small_value)
    i["names"] = i['Gene']
    i["pi_score"] = -1 * np.log10(i["pValue_adj"]) * i["LogFoldChange"] # pvalue
    i.dropna(inplace=True)

filtered_gset = {gs: [gene for gene in genes if gene in background_genes] for gs, genes in gset.items()}

gset

filtered_gset

len(gset)

gene_rank = Glom_healthy[['names','pi_score']]
gene_rank.sort_values(by=['pi_score'], inplace=True, ascending=False)
gene_rank = gene_rank.reset_index(drop=True)

res = gseapy.prerank(rnk=gene_rank, gene_sets=gset, background=background_genes)

res.res2d.head(20)
#res.res2d_plot = res.res2d[res.res2d["FWER p-val"] < 0.05]
#res.res2d_plot.head(20)

gene_rank = Glom_healthy[['names','pi_score']]
gene_rank.sort_values(by=['pi_score'], inplace=True, ascending=False)
gene_rank = gene_rank.reset_index(drop=True)

res = gseapy.prerank(rnk=gene_rank, gene_sets=filtered_gset)

res.res2d.head(20)
#res.res2d_plot = res.res2d[res.res2d["FWER p-val"] < 0.05]
#res.res2d_plot.head(20)

gene_rank = Glom_disease[['names','pi_score']]
gene_rank.sort_values(by=['pi_score'], inplace=True, ascending=False)
gene_rank = gene_rank.reset_index(drop=True)

res = gseapy.prerank(rnk=gene_rank, gene_sets=gset)

res.res2d.head(20)
#res.res2d_plot = res.res2d[res.res2d["FWER p-val"] < 0.05]
#res.res2d_plot.head(20)





subset_mc1=adata[adata.obs['annotation_post_scanvi70_broad']=="MC1"]
subset_mc1.obs['type']=subset_mc1.obs['type'].replace('Pediatric', 'Healthy')
subset_mc1.X = subset_mc1.layers["counts"]
sc.pp.log1p(subset_mc1)
sc.tl.rank_genes_groups(subset_mc1, groupby='type', method='wilcoxon', pts = True, reference='Healthy')
sc.pl.rank_genes_groups(subset_mc1, n_genes=25, sharey=False)

adata

adata.obs['cluster_labels_annotated_coarse_coarse'].value_counts()

df= pd.DataFrame(adata.obs['cluster_labels_annotated_coarse_coarse'])
df

df=df[df['cluster_labels_annotated_coarse_coarse']!='outlier']
df

df.to_csv('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/cluster_niche_cell_ids.csv', index=True)

df.to_csv('cluster_niche_cell_ids.csv', index=True)

df = pd.DataFrame({
    'cluster_labels': adata.obs['cluster_labels_annotated_coarse_coarse'],
    'type': adata.obs['type'],
    'sample':adata.obs['sample']
}, index=adata.obs_names)
df

df=df[df['cluster_labels']!='outlier']

df['type']=df['type'].replace('Pediatric', 'Healthy')

df['type'].value_counts()

df=df[df['type']!='Diabetes']
df['sample'].value_counts()

df=df[df['sample']!='HK2874']

df['sample'].value_counts()

df

df=df.drop('sample', axis=1)

df['type'] = df['type'].cat.remove_unused_categories()

df['cluster_labels'] = df['cluster_labels'].cat.remove_unused_categories()

# Step 1: Group by 'type' and 'cluster_labels' and count the occurrences
grouped_counts = df.groupby(['type', 'cluster_labels']).size().reset_index(name='count')

# Step 2: Calculate the total number of 'cluster_labels' for each 'type'
total_counts = df.groupby('type').size().reset_index(name='total_count')

# Step 3: Merge the total counts back to the grouped counts
merged_counts = pd.merge(grouped_counts, total_counts, on='type')

# Step 4: Normalize the counts by the total counts for each 'type'
merged_counts['normalized_count'] = merged_counts['count'] / merged_counts['total_count']

# Step 5: Display the result
print(merged_counts)

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming 'merged_counts' is your DataFrame
# Split the DataFrame into Healthy and Disease
healthy_df = merged_counts[merged_counts['type'] == 'Healthy']
disease_df = merged_counts[merged_counts['type'] == 'Disease']

# Merge Healthy and Disease dataframes on 'cluster_labels'
merged = pd.merge(healthy_df, disease_df, on='cluster_labels', suffixes=('_healthy', '_disease'))

# Calculate the relative increase
merged['relative_increase'] = (merged['normalized_count_disease'] - merged['normalized_count_healthy']) / merged['normalized_count_healthy']
heatmap_data = merged.set_index('cluster_labels')[['relative_increase']]
heatmap_data_rounded = heatmap_data.round(0).astype(int)
# Plotting the heatmap
plt.figure(figsize=(2, 6))
ax = sns.heatmap(heatmap_data_rounded, annot=True, cmap='magma',fmt='d',cbar=False, vmax=30)
ax.set_xticks([])
ax.tick_params(axis='y', which='both', left=False, right=False)
# Add labels and title
plt.xlabel('')
plt.ylabel('')
plt.title('Increase in DKD')

# Save the plot
plt.tight_layout()
plt.savefig('relative_increase_heatmap.png', dpi=450)
plt.show()

heatmap_data_rounded

merged['relative_increase']

merged

"""lets try again with clustering the dataframe with neighborfractions"""

new_adata=sc.read_h5ad('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/neighbors_as_genes_all_cosmx.h5ad')

#new_adata.obs['cluster_labels_annotated_coarse_coarse']=adata.obs['cluster_labels_annotated_coarse_coarse'].copy()

#new_adata.write('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/neighbors_as_genes_all_cosmx.h5ad')

new_adata_subset=new_adata[new_adata.obs['total_neighbors_200']>50]
new_adata_subset

subset_mc=new_adata_subset[new_adata_subset.obs['annotation_post_scanvi70_broad']=="MC1"]
print(subset_mc.obs['sample'].value_counts())

print(new_adata_subset.obs['annotation_post_scanvi70_broad'].value_counts())

import pandas as pd

# Convert the sparse matrix to a dense format and create a DataFrame
gene_expression_df = pd.DataFrame(
    new_adata_subset.X.toarray(),
    index=new_adata_subset.obs_names,
    columns=new_adata_subset.var_names
)

# Print the first few rows to verify
print(gene_expression_df.head())

# List of .obs columns you want to add to the DataFrame
obs_columns = [
    'total_neighbors_200', 'total_neighbors_180', 'total_neighbors_160',
    'total_neighbors_140', 'total_neighbors_120', 'total_neighbors_100',
    'total_neighbors_80', 'total_neighbors_60', 'total_neighbors_40', 'total_neighbors_20'
]

# Add the selected .obs columns to the gene expression DataFrame
gene_expression_df[obs_columns] = new_adata_subset.obs[obs_columns]

# Print the first few rows to verify the updated DataFrame
print(gene_expression_df.head())

# Loop over each suffix and corresponding total_neighbors column
for neighbors in [200, 180, 160, 140, 120, 100, 80, 60, 40, 20]:
    # Generate the suffix and obs column name
    suffix = f'_{neighbors}'
    obs_column = f'total_neighbors_{neighbors}'

    # Get all columns that end with the current suffix
    matching_columns = [col for col in gene_expression_df.columns if col.endswith(suffix)]

    # Perform the division for each matching column
    for col in matching_columns:
        gene_expression_df[col] /= gene_expression_df[obs_column]

# Print the first few rows to verify the updated DataFrame
print(gene_expression_df.head())

gene_expression_df = gene_expression_df.drop(obs_columns, axis=1)

# Print the first few rows to verify that columns are removed
print(gene_expression_df.head())

import numpy as np

# Replace infinite values with 0
gene_expression_df.replace([np.inf, -np.inf], 0, inplace=True)

# Verify changes by displaying the first few rows of the DataFrame
print(gene_expression_df.head())

gene_expression_df=gene_expression_df.fillna(0)

from scipy import sparse
sparse_matrix = scipy.sparse.csr_matrix(gene_expression_df.values)

new_adata_subset.X=sparse_matrix

print(new_adata_subset.X)

sc.tl.rank_genes_groups(new_adata_subset, groupby='cluster_labels_annotated_coarse_coarse', method='wilcoxon', pts = True)
sc.pl.rank_genes_groups(new_adata_subset, n_genes=25, sharey=False)

markers = [ "iPT_20", "iPT_40"]

sc.pl.dotplot(new_adata_subset, markers, groupby='cluster_labels_annotated_coarse_coarse',cmap='Blues', log = False)

new_adata_subset.obs['type']=new_adata_subset.obs['type'].replace('Pediatric', 'Healthy')

for niche in new_adata_subset.obs['cluster_labels_annotated_coarse_coarse'].unique():
  subset=new_adata_subset[new_adata_subset.obs['cluster_labels_annotated_coarse_coarse']==niche]
  print(niche)
  sc.tl.rank_genes_groups(subset, groupby='type', method='wilcoxon', pts = True)
  sc.pl.rank_genes_groups(subset, n_genes=25, sharey=False)

glom_subset=new_adata_subset[new_adata_subset.obs['cluster_labels_annotated_coarse_coarse']=='Glomerular Niche']
sc.tl.rank_genes_groups(glom_subset, groupby='type', method='wilcoxon', pts = True)
sc.pl.rank_genes_groups(glom_subset, n_genes=35, sharey=False)

de_results = glom_subset.uns['rank_genes_groups']
for group in de_results['names'].dtype.names:
        gene_names = de_results['names'][group]
        gene_scores = de_results['scores'][group]
        gene_logfoldchanges = de_results['logfoldchanges'][group]
        gene_pvals = de_results['pvals'][group]
        gene_pvals_adj = de_results['pvals_adj'][group]

        # Create a DataFrame
        df = pd.DataFrame({
            'Gene': gene_names,
            'Score': gene_scores,
            'LogFoldChange': gene_logfoldchanges,
            'pValue': gene_pvals,
            'pValue_adj': gene_pvals_adj
        })
        print(df.head(40))

glom_subset=glom_subset[glom_subset.obs['type']!='Diabetes']

glom_subset=glom_subset[glom_subset.obs['sample']!='HK2874']
print(glom_subset.obs['sample'].value_counts())

sc.tl.rank_genes_groups(glom_subset, groupby='type', method='wilcoxon', pts = True)
sc.pl.rank_genes_groups(glom_subset, n_genes=35, sharey=False)

de_results = glom_subset.uns['rank_genes_groups']
for group in de_results['names'].dtype.names:
        gene_names = de_results['names'][group]
        gene_scores = de_results['scores'][group]
        gene_logfoldchanges = de_results['logfoldchanges'][group]
        gene_pvals = de_results['pvals'][group]
        gene_pvals_adj = de_results['pvals_adj'][group]

        # Create a DataFrame
        df = pd.DataFrame({
            'Gene': gene_names,
            'Score': gene_scores,
            'LogFoldChange': gene_logfoldchanges,
            'pValue': gene_pvals,
            'pValue_adj': gene_pvals_adj
        })
        df=df[df['LogFoldChange']>1]
        df = df[~df['Gene'].str.endswith('_200')]
        df = df[~df['Gene'].str.endswith('_180')]
        df = df[~df['Gene'].str.endswith('_160')]
        df = df[~df['Gene'].str.endswith('_140')]
        df = df[~df['Gene'].str.endswith('_120')]
        df = df[~df['Gene'].str.endswith('_100')]
        print(df.head(40))

markers = [ "Immune_20","Fibroblast_20","CNT_20", "IC A_20","PC_20",  "Immune_40","Fibroblast_40", "MC1_40"]

sc.pl.dotplot(glom_subset, markers, groupby='type',cmap='Blues', log = False)

new_adata_subset

delete_suffixes = ['200', '180', '160', '140', '120', '100']

# Filter out columns that end with the specified suffixes
columns_to_keep = [col for col in gene_expression_df.columns if not any(col.endswith(suffix) for suffix in delete_suffixes)]
filtered_df = gene_expression_df[columns_to_keep]
print(filtered_df.shape)

import pandas as pd
import numpy as np
from sklearn.cluster import MiniBatchKMeans, AgglomerativeClustering
from scipy.cluster.hierarchy import dendrogram, linkage
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
import umap.umap_ as umap
from sklearn.decomposition import PCA

# Assuming your dataframe is already loaded as gene_expression_df
# Normalize the data
scaler = StandardScaler()
features_normalized = scaler.fit_transform(filtered_df)

pca = PCA(n_components=25)  # Reduce to 10 dimensions for UMAP input
principal_components = pca.fit_transform(features_normalized)

# Perform MiniBatchKMeans clustering
kmeans = MiniBatchKMeans(n_clusters=15, random_state=42)
labels = kmeans.fit_predict(principal_components)
filtered_df['cluster_labels'] = labels

principal_components

from sklearn.cluster import MiniBatchKMeans
from sklearn.metrics import silhouette_score

inertias = []
k = range(1, 20)

# dissimilarity would not be defined for a single cluster, thus, minimum number of clusters should be 2
for K in k:
    # Create a MiniBatchKMeans object, arbitrarily choosing 10 clusters
    kmeans = MiniBatchKMeans(n_clusters=K)
    # Fit the model to the data
    kmeans.fit(principal_components)
    inertias.append(kmeans.inertia_)

plt.plot(k, inertias, 'bx-')
plt.xlabel('Values of K')
plt.ylabel('Inertia')
plt.title('The Elbow Method using Inertia')
plt.show()

new_adata_subset.obs['cluster_labels']=filtered_df['cluster_labels'].copy()

print(new_adata_subset.obs['cluster_labels'])

new_adata_subset.obs['cluster_labels']=new_adata_subset.obs['cluster_labels'].astype('category')

sc.tl.rank_genes_groups(new_adata_subset, groupby='cluster_labels', method='wilcoxon', pts = True)
sc.pl.rank_genes_groups(new_adata_subset, n_genes=25, sharey=False)

new_adata_subset.obs["cluster_labels"].value_counts()
cell_identities = {0: 'PC Neighborhood', 1: 'PT Neighborhood 1', 2: 'CNT Neighborhood 1', 3: 'Glomerular Neighborhood',
                   4: 'DTL_ATL Neighborhood', 5: 'Immune Neighborhood', 6: 'DCT Neighborhood', 7: 'iPT Neighborhood',
                   8: 'Fibroblast Neighborhood', 9: 'TAL Neighborhood', 10:'Vascular Neighborhood', 11:'Glomerular Neighborhood 2',
                   12:'iTAL Neighborhood', 13:'IC Neighborhood', 14:'Vascular Neighborhood 2'}
new_adata_subset.obs["cluster_labels_annotated"] = new_adata_subset.obs['cluster_labels'].map(cell_identities).astype('category')

import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

# Assuming new_adata_subset is your AnnData object and it has the annotations and cluster labels
# Extract the relevant columns
annotations = new_adata_subset.obs["annotation_post_scanvi70_broad"]
cluster_labels = new_adata_subset.obs["cluster_labels_annotated"]

# Create a contingency table (cross-tabulation)
contingency_table = pd.crosstab(cluster_labels, annotations)

# Normalize the data (optional)
# normalized_table = contingency_table.div(contingency_table.sum(axis=1), axis=0)

# Plot the heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(contingency_table, annot=True, fmt="d", cmap="viridis")
plt.title('Matrix Plot of Cluster Labels and Annotations')
plt.xlabel('Cluster Labels')
plt.ylabel('Annotations')
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

# Assuming new_adata_subset is your AnnData object and it has the annotations and cluster labels
# Extract the relevant columns
annotations = new_adata_subset.obs["annotation_post_scanvi70_broad"]
cluster_labels = new_adata_subset.obs["cluster_labels_annotated"]

# Create a contingency table (cross-tabulation)
contingency_table = pd.crosstab(cluster_labels, annotations)

# Normalize the data across the columns (annotations)
normalized_table = contingency_table.div(contingency_table.sum(axis=0), axis=1)

# Plot the heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(normalized_table, annot=False, fmt=".2f", cmap="plasma")
plt.title('Normalized Matrix Plot of Cluster Labels and Annotations')
plt.xlabel('Cluster Labels')
plt.ylabel('Annotations')
plt.show()

normalized_table

print(scaled_table.T.columns)

import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.preprocessing import MinMaxScaler

# Assuming new_adata_subset is your AnnData object and it has the annotations and cluster labels
# Extract the relevant columns
annotations = new_adata_subset.obs["annotation_post_scanvi70_broad"]
cluster_labels = new_adata_subset.obs["cluster_labels_annotated"]

# Create a contingency table (cross-tabulation)
contingency_table = pd.crosstab(cluster_labels, annotations)

# Normalize the data across the columns (annotations)
normalized_table = contingency_table.div(contingency_table.sum(axis=0), axis=1)

# Scale the data from 0 to 1 across the columns
scaler = MinMaxScaler()
scaled_table = pd.DataFrame(scaler.fit_transform(normalized_table),
                            index=normalized_table.index,
                            columns=normalized_table.columns)
desired_column_order = ['PT','EC_Peritub','iPT','TAL','iTAL','DTL_ATL','CNT', 'DCT','IC A', 'IC B', 'PC', 'EC_DVR', 'VSMC',
                  'Fibroblast',  'Immune',   'EC_glom','PEC',
                    'Podo',  'MC1']  # Replace with actual annotation labels
desired_index_order = ['PT Neighborhood 1','iPT Neighborhood','TAL Neighborhood', 'iTAL Neighborhood', 'DTL_ATL Neighborhood',
                       'CNT Neighborhood 1','DCT Neighborhood','IC Neighborhood','PC Neighborhood', 'Vascular Neighborhood',
                  'Vascular Neighborhood 2','Immune Neighborhood',
                   'Fibroblast Neighborhood','Glomerular Neighborhood','Glomerular Neighborhood 2',



                   ]  # Replace with actual cluster labels
scaled_table = scaled_table.reindex(index=desired_index_order, columns=desired_column_order)
# Plot the heatmap
plt.figure(figsize=(5, 8))
sns.heatmap(scaled_table.T, annot=False, fmt=".2f", cmap="Blues")
plt.title('Kidney Niches')
plt.xlabel('Niche Labels')
plt.ylabel('Cell Types')
plt.show()

new_adata_subset.obs["cluster_labels"].value_counts()
cell_identities = {0: 'PC Neighborhood', 1: 'PT Neighborhood', 2: 'CNT Neighborhood', 3: 'Glomerular Neighborhood',
                   4: 'DTL_ATL Neighborhood', 5: 'Immune Neighborhood', 6: 'DCT Neighborhood', 7: 'iPT Neighborhood',
                   8: 'Fibroblast Neighborhood', 9: 'TAL Neighborhood', 10:'Vascular Neighborhood', 11:'Glomerular Neighborhood',
                   12:'iTAL Neighborhood', 13:'IC Neighborhood', 14:'Vascular Neighborhood'}
new_adata_subset.obs["cluster_labels_annotated_coarse"] = new_adata_subset.obs['cluster_labels'].map(cell_identities).astype('category')

import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.preprocessing import MinMaxScaler

# Assuming new_adata_subset is your AnnData object and it has the annotations and cluster labels
# Extract the relevant columns
annotations = new_adata_subset.obs["annotation_post_scanvi70_broad"]
cluster_labels = new_adata_subset.obs["cluster_labels_annotated_coarse"]

# Create a contingency table (cross-tabulation)
contingency_table = pd.crosstab(cluster_labels, annotations)

# Normalize the data across the columns (annotations)
normalized_table = contingency_table.div(contingency_table.sum(axis=0), axis=1)

# Scale the data from 0 to 1 across the columns
scaler = MinMaxScaler()
scaled_table = pd.DataFrame(scaler.fit_transform(normalized_table),
                            index=normalized_table.index,
                            columns=normalized_table.columns)
desired_column_order = ['PT','EC_Peritub','iPT','TAL','iTAL','DTL_ATL','CNT', 'DCT','IC A', 'IC B', 'PC', 'EC_DVR', 'VSMC',
                  'Fibroblast',  'Immune',   'EC_glom','PEC',
                    'Podo',  'MC1']  # Replace with actual annotation labels
desired_index_order = ['PT Neighborhood','iPT Neighborhood','TAL Neighborhood', 'iTAL Neighborhood', 'DTL_ATL Neighborhood',
                       'CNT Neighborhood','DCT Neighborhood','IC Neighborhood','PC Neighborhood', 'Vascular Neighborhood',
                   'Fibroblast Neighborhood','Immune Neighborhood','Glomerular Neighborhood',]  # Replace with actual cluster labels
scaled_table = scaled_table.reindex(index=desired_index_order, columns=desired_column_order)
# Plot the heatmap
plt.figure(figsize=(5, 8))
sns.heatmap(scaled_table.T, annot=False, fmt=".2f", cmap="Blues")
plt.title('Kidney Niches')
plt.tick_params(axis='x', which='both', length=0)
plt.tick_params(axis='y', which='both', length=0)
plt.xlabel('Niche Labels')
plt.ylabel('Cell Types')
plt.savefig('Kindey Niches.png', dpi=450, bbox_inches='tight')
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# Assuming new_adata_subset is your AnnData object
# Extract the relevant columns
samples = new_adata_subset.obs['sample']
cluster_labels = new_adata_subset.obs['cluster_labels_annotated_coarse']

# Create a DataFrame from the relevant columns
data = pd.DataFrame({'sample': samples, 'cluster_labels_annotated_coarse': cluster_labels})

# Group by 'sample' and 'cluster_labels_annotated' and count the occurrences
grouped_data = data.groupby(['sample', 'cluster_labels_annotated_coarse']).size().unstack(fill_value=0)
desired_order = [
    "Pediatric1", "HK2753", "HK3039", "HK3106", "HK3531", "HK3531_2",
    "HK3063", "HK3542", "HK2874", "HK2695", "HK3035", "HK3035_2",
     "HK2841", "HK2873","HK2844", "HK2844_2"
]

grouped_data=grouped_data.reindex(desired_order)
sample_totals = data['sample'].value_counts()
sample_totals=sample_totals.reindex(desired_order)
normalized_count_df = grouped_data.div(sample_totals, axis='index')
ax = normalized_count_df.plot(kind='bar', stacked=True, figsize=(8, 8), colormap='tab20')

# Set plot labels and title
plt.xlabel('')
plt.ylabel('Relative Amount')
plt.title('Sample Niche Composition')
plt.legend(title='Niche Labels', bbox_to_anchor=(1.05, 1), loc='upper left', frameon=False)
plt.tick_params(axis='x', which='both', length=0)
ax.spines['top'].set_visible(False)
ax.spines['right'].set_visible(False)
plt.tight_layout()
plt.savefig('Kindey Niches Samples.png', dpi=450, bbox_inches='tight')
plt.show()

adata.obs['cluster_labels_annotated_coarse']=new_adata_subset.obs["cluster_labels_annotated_coarse"].copy()

adata.obs['cluster_labels_annotated_coarse'] = adata.obs['cluster_labels_annotated_coarse'].cat.add_categories(['outlier'])
adata.obs["cluster_labels_annotated_coarse"]=adata.obs["cluster_labels_annotated_coarse"].fillna('outlier')

filtered_df['cluster_labels']=new_adata_subset.obs['cluster_labels_annotated_coarse']

print(new_adata_subset.obs['cluster_labels_annotated_coarse'])

aggregated_df = filtered_df.groupby('cluster_labels').mean()

# Scale the aggregated data
scaler = StandardScaler()
aggregated_normalized = scaler.fit_transform(aggregated_df)

# Compute the linkage matrix using the Ward method
linkage_matrix = linkage(aggregated_normalized, method='ward')

# Plot the dendrogram
plt.figure(figsize=(15, 10))
dendrogram(linkage_matrix, labels=aggregated_df.index.astype(str).tolist(), leaf_rotation=90, leaf_font_size=10)
plt.title('Hierarchical Clustering Dendrogram (Ward)')
plt.xlabel('Cluster label')
plt.ylabel('Distance')
plt.show()

# Set a random seed for reproducibility
random_seed = 42

# Sample 1000 rows from each cluster
sampled_df = filtered_df.groupby('cluster_labels').apply(lambda x: x.sample(min(len(x), 1000), random_state=random_seed)).reset_index(drop=True)

# Aggregate the sampled dataframe by cluster_labels
aggregated_df = sampled_df.groupby('cluster_labels').mean()

# Scale the aggregated data
scaler = StandardScaler()
aggregated_normalized = scaler.fit_transform(aggregated_df)

# Compute the linkage matrix using the Ward method
linkage_matrix = linkage(aggregated_normalized, method='ward')

# Plot the dendrogram
plt.figure(figsize=(15, 10))
dendrogram(linkage_matrix, labels=aggregated_df.index.astype(str).tolist(), leaf_rotation=90, leaf_font_size=10)
plt.title('Hierarchical Clustering Dendrogram (Ward)')
plt.xlabel('Cluster label')
plt.ylabel('Distance')
plt.show()

adata.obs["cluster_labels_annotated_coarse"].value_counts()
cell_identities = {'PC Neighborhood':'CD Niche', 'PT Neighborhood': 'PT Niche', 'CNT Neighborhood': 'CNT Niche',
                   'DTL_ATL Neighborhood':'TAL Niche', 'Immune Neighborhood': 'Immune Niche', 'DCT Neighborhood': 'PT Niche',
                   'iPT Neighborhood': 'iPT Niche',
                   'Fibroblast Neighborhood': 'Fibroblast Niche', 'TAL Neighborhood': 'TAL Niche', 'Glomerular Neighborhood':'Glomerular Niche',
                   'iTAL Neighborhood':'iTAL Niche', 'IC Neighborhood':'CNT Niche', 'Vascular Neighborhood':'Vascular Niche', 'outlier':'outlier'}
adata.obs["cluster_labels_annotated_coarse_coarse"] = adata.obs['cluster_labels_annotated_coarse'].map(cell_identities).astype('category')

adata.obs['cluster_labels_annotated_coarse_coarse'].unique()

import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.preprocessing import MinMaxScaler

# Assuming new_adata_subset is your AnnData object and it has the annotations and cluster labels
# Extract the relevant columns
annotations = adata.obs["annotation_post_scanvi70_broad"]
cluster_labels = adata.obs["cluster_labels_annotated_coarse_coarse"]

# Create a contingency table (cross-tabulation)
contingency_table = pd.crosstab(cluster_labels, annotations)
contingency_table=contingency_table.drop('outlier', axis=0)

# Normalize the data across the columns (annotations)
normalized_table = contingency_table.div(contingency_table.sum(axis=1), axis=0)

# Scale the data from 0 to 1 across the columns
scaler = MinMaxScaler()
scaled_table = pd.DataFrame(scaler.fit_transform(normalized_table),
                            index=normalized_table.index,
                            columns=normalized_table.columns)
desired_column_order = ['PT','DCT','EC_Peritub','iPT','TAL','DTL_ATL','iTAL','CNT', 'IC A', 'IC B', 'PC', 'EC_DVR', 'VSMC',
                  'Fibroblast',  'Immune',   'EC_glom','PEC',
                    'Podo',  'MC1']  # Replace with actual annotation labels
desired_index_order = ['PT Niche','iPT Niche', 'TAL Niche', 'iTAL Niche','CNT Niche','CD Niche', 'Vascular Niche', 'Immune Niche',  'Fibroblast Niche','Glomerular Niche', ]  # Replace with actual cluster labels
scaled_table = scaled_table.reindex(index=desired_index_order, columns=desired_column_order)
# Plot the heatmap
plt.figure(figsize=(5, 8))
sns.heatmap(scaled_table.T, annot=False, fmt=".2f", cmap="Blues")
plt.title('Kidney Niches')
plt.tick_params(axis='x', which='both', length=0)
plt.tick_params(axis='y', which='both', length=0)
plt.xlabel('Niche Labels')
plt.ylabel('Cell Types')
plt.savefig('Kindey Niches Coarse.png', dpi=450, bbox_inches='tight')
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.preprocessing import MinMaxScaler

# Assuming new_adata_subset is your AnnData object and it has the annotations and cluster labels
# Extract the relevant columns
annotations = adata.obs["annotation_post_scanvi70_broad"]
cluster_labels = adata.obs["cluster_labels_annotated_coarse"]

# Create a contingency table (cross-tabulation)
contingency_table = pd.crosstab(cluster_labels, annotations)

# Normalize the data across the columns (annotations)
normalized_table = contingency_table.div(contingency_table.sum(axis=1), axis=0)

# Scale the data from 0 to 1 across the columns
scaler = MinMaxScaler()
scaled_table = pd.DataFrame(scaler.fit_transform(normalized_table),
                            index=normalized_table.index,
                            columns=normalized_table.columns)
desired_column_order = ['PT','EC_Peritub','iPT','TAL','iTAL','DTL_ATL','CNT', 'DCT','IC A', 'IC B', 'PC', 'EC_DVR', 'VSMC',
                  'Fibroblast',  'Immune',   'EC_glom','PEC',
                    'Podo',  'MC1']  # Replace with actual annotation labels
desired_index_order = ['PT Neighborhood','iPT Neighborhood','TAL Neighborhood', 'iTAL Neighborhood', 'DTL_ATL Neighborhood',
                       'CNT Neighborhood','DCT Neighborhood','IC Neighborhood','PC Neighborhood', 'Vascular Neighborhood',
                   'Fibroblast Neighborhood','Immune Neighborhood','Glomerular Neighborhood',]  # Replace with actual cluster labels
scaled_table = scaled_table.reindex(index=desired_index_order, columns=desired_column_order)
# Plot the heatmap
plt.figure(figsize=(5, 8))
sns.heatmap(scaled_table.T, annot=False, fmt=".2f", cmap="Blues")
plt.title('Kidney Niches')
plt.tick_params(axis='x', which='both', length=0)
plt.tick_params(axis='y', which='both', length=0)
plt.xlabel('Niche Labels')
plt.ylabel('Cell Types')
plt.savefig('Kindey Niches.png', dpi=450, bbox_inches='tight')
plt.show()

contingency_table.sum(axis=0)

print(contingency_table.sum(axis=1))

normalized_table

adata

import matplotlib.pyplot as plt
import seaborn as sns

# Assuming new_adata_subset is your AnnData object
# Extract the relevant columns
samples = adata.obs['sample']
cluster_labels = adata.obs['cluster_labels_annotated_coarse']

# Create a DataFrame from the relevant columns
data = pd.DataFrame({'sample': samples, 'cluster_labels_annotated_coarse': cluster_labels})

# Group by 'sample' and 'cluster_labels_annotated' and count the occurrences
grouped_data = data.groupby(['sample', 'cluster_labels_annotated_coarse']).size().unstack(fill_value=0)
desired_order = [
    "Pediatric1", "HK2753", "HK3039", "HK3106", "HK3531", "HK3531_2",
    "HK3063", "HK3542", "HK2874", "HK2695", "HK3035", "HK3035_2",
     "HK2841", "HK2873","HK2844", "HK2844_2"
]

grouped_data=grouped_data.reindex(desired_order)
sample_totals = data['sample'].value_counts()
sample_totals=sample_totals.reindex(desired_order)
normalized_count_df = grouped_data.div(sample_totals, axis='index')
ax = normalized_count_df.plot(kind='bar', stacked=True, figsize=(8, 8), colormap='tab20')

# Set plot labels and title
plt.xlabel('')
plt.ylabel('Relative Amount')
plt.title('Sample Niche Composition')
plt.legend(title='Niche Labels', bbox_to_anchor=(1.05, 1), loc='upper left', frameon=False)
plt.tick_params(axis='x', which='both', length=0)
ax.spines['top'].set_visible(False)
ax.spines['right'].set_visible(False)
plt.tight_layout()
plt.savefig('Kindey Niches Samples with outliers.png', dpi=450, bbox_inches='tight')
plt.show()

adata.write('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/all_Cosmx_cleaned_neighbored.h5ad')

for sample in adata.obs['sample'].unique():
  subset=adata[adata.obs['sample']==sample]
  sc.pl.scatter(
    subset,
    x="CenterX_global_px",
    y="CenterY_global_px",
    color="cluster_labels_annotated_coarse",
    size=3,
    title=f'Niche Composition for {sample}',
  )

pediatric=adata[adata.obs['sample']=='Pediatric1']

pediatric.obs['fov']=pediatric.obs['fov'].astype(int)
#pediatric.obs['fov']=pediatric.obs['fov'].astype(str)

fov_include = [39, 40, 41, 44, 45, 46, 51, 52, 53, 60, 61, 62, 70,71,72,81,82,83,
               92,93,94,103,104,105,114,115,116,125, 126,127,136,137,138,
               147,148,149,157,158,159,168,169,170,179,180,181,187,188,189,197,198, 199]
# Filter the data to include only FOVs in fov_include
pediatric = pediatric[pediatric.obs['fov'].isin(fov_include)]

pediatric

sc.pl.scatter(
    pediatric,
    x="CenterX_global_px",
    y="CenterY_global_px",
    color="cluster_labels_annotated_coarse",
    size=10,
    title=f'Niche Composition healthy kidney',
)

import matplotlib.pyplot as plt
import scanpy as sc

# Create a figure and axis with the desired size
fig, ax = plt.subplots(figsize=(3, 8))  # width, height in inches

# Create the scatter plot
sc.pl.scatter(
    pediatric,
    x="CenterX_global_px",
    y="CenterY_global_px",
    color="cluster_labels_annotated_coarse_coarse",
    size=10,  # size of individual points
    title='Niche Composition healthy kidney',
    ax=ax  # pass the axis object to the plot
)

# Show the plot
plt.show()

sc.pl.scatter(
    pediatric,
    x="CenterX_global_px",
    y="CenterY_global_px",
    color="fov",
    size=3,
    legend_loc='on data',
    title=f'Niche Composition healthy kidney',
)

"""lets do cortex medulla"""

import pandas as pd
import numpy as np
from sklearn.cluster import MiniBatchKMeans, AgglomerativeClustering
from scipy.cluster.hierarchy import dendrogram, linkage
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
import umap.umap_ as umap
from sklearn.decomposition import PCA

# Assuming your dataframe is already loaded as gene_expression_df
# Normalize the data
scaler = StandardScaler()
features_normalized = scaler.fit_transform(filtered_df)

pca = PCA(n_components=25)  # Reduce to 10 dimensions for UMAP input
principal_components = pca.fit_transform(features_normalized)

# Perform MiniBatchKMeans clustering
kmeans = MiniBatchKMeans(n_clusters=2, random_state=42)
labels = kmeans.fit_predict(principal_components)
filtered_df['cluster_labels'] = labels

new_adata_subset.obs['cluster_labels']=filtered_df['cluster_labels']

new_adata_subset.obs['cluster_labels']=new_adata_subset.obs['cluster_labels'].astype('category')

sc.tl.rank_genes_groups(new_adata_subset, groupby='cluster_labels', method='wilcoxon', pts = True)
sc.pl.rank_genes_groups(new_adata_subset, n_genes=25, sharey=False)

new_adata_subset.obs["cluster_labels"].value_counts()
cell_identities = {0: 'Cortex', 1: 'Medulla'}
new_adata_subset.obs["cluster_labels_annotated"] = new_adata_subset.obs['cluster_labels'].map(cell_identities).astype('category')

import matplotlib.pyplot as plt
import seaborn as sns

# Assuming new_adata_subset is your AnnData object
# Extract the relevant columns
samples = new_adata_subset.obs['sample']
cluster_labels = new_adata_subset.obs['cluster_labels_annotated']

# Create a DataFrame from the relevant columns
data = pd.DataFrame({'sample': samples, 'cluster_labels_annotated': cluster_labels})

# Group by 'sample' and 'cluster_labels_annotated' and count the occurrences
grouped_data = data.groupby(['sample', 'cluster_labels_annotated']).size().unstack(fill_value=0)
desired_order = [
    "Pediatric1", "HK2753", "HK3039", "HK3106", "HK3531", "HK3531_2",
    "HK3063", "HK3542", "HK2874", "HK2695", "HK3035", "HK3035_2",
     "HK2841", "HK2873","HK2844", "HK2844_2"
]

grouped_data=grouped_data.reindex(desired_order)
sample_totals = data['sample'].value_counts()
sample_totals=sample_totals.reindex(desired_order)
normalized_count_df = grouped_data.div(sample_totals, axis='index')
ax = normalized_count_df.plot(kind='bar', stacked=True, figsize=(8, 6), colormap='tab10b')

# Set plot labels and title
plt.xlabel('')
plt.ylabel('Relative Amount')
plt.title('Sample Region Composition')
plt.legend(title='Region Labels', bbox_to_anchor=(1.05, 1), loc='upper left', frameon=False)
plt.tick_params(axis='x', which='both', length=0)
ax.spines['top'].set_visible(False)
ax.spines['right'].set_visible(False)
plt.tight_layout()
plt.savefig('Cortex Medulla Samples.png', dpi=450, bbox_inches='tight')
plt.show()

adata.obs['cluster_labels_annotated_regions']=new_adata_subset.obs["cluster_labels_annotated"].copy()

for sample in adata.obs['sample'].unique():
  subset=adata[adata.obs['sample']==sample]
  sc.pl.scatter(
    subset,
    x="CenterX_global_px",
    y="CenterY_global_px",
    color="cluster_labels_annotated_regions",
    size=3,
    title=f'Niche Composition for {sample}',
  )

adata.obs['cluster_labels_annotated_regions'] = adata.obs['cluster_labels_annotated_regions'].cat.add_categories(['outlier'])
adata.obs["cluster_labels_annotated_regions"]=adata.obs["cluster_labels_annotated_regions"].fillna('outlier')

annotated_regions = adata.obs['cluster_labels_annotated_regions']
coarse_labels = adata.obs['cluster_labels_annotated_coarse']

# Create a dataframe
df = pd.DataFrame({
    'cluster_labels_annotated_regions': annotated_regions,
    'cluster_labels_annotated_coarse': coarse_labels
})
df

df=df[df['cluster_labels_annotated_coarse']!='outlier']

medulla_df = df[df['cluster_labels_annotated_regions'] == 'Medulla']
cortex_df = df[df['cluster_labels_annotated_regions'] == 'Cortex']

# Sort each dataframe by the 'cluster_labels_coarse' column
medulla_df_sorted = medulla_df.sort_values(by='cluster_labels_annotated_coarse')
cortex_df_sorted = cortex_df.sort_values(by='cluster_labels_annotated_coarse')

# Combine the sorted dataframes back into one
sorted_df = pd.concat([medulla_df_sorted, cortex_df_sorted])

print(sorted_df)

!pip install plotly

sorted_df['cluster_labels_annotated_regions'] = sorted_df['cluster_labels_annotated_regions'].cat.remove_unused_categories()
sorted_df['cluster_labels_annotated_coarse'] = sorted_df['cluster_labels_annotated_coarse'].cat.remove_unused_categories()

import pandas as pd
import plotly.graph_objects as go

# Assuming sorted_df is your dataframe created earlier
# Prepare data for the Sankey diagram
source_labels = sorted_df['cluster_labels_annotated_regions'].unique().tolist()
target_labels = sorted_df['cluster_labels_annotated_coarse'].unique().tolist()
labels = source_labels + target_labels

source_indices = sorted_df['cluster_labels_annotated_regions'].apply(lambda x: source_labels.index(x)).tolist()
target_indices = sorted_df['cluster_labels_annotated_coarse'].apply(lambda x: target_labels.index(x) + len(source_labels)).tolist()

# Create a dataframe for the counts of each flow
flow_counts = sorted_df.groupby(['cluster_labels_annotated_regions', 'cluster_labels_annotated_coarse']).size().reset_index(name='count')

# Create the Sankey plot
fig = go.Figure(go.Sankey(
    node=dict(
        pad=15,
        thickness=20,
        line=dict(color="black", width=0.5),
        label=labels
    ),
    link=dict(
        source=flow_counts['cluster_labels_annotated_regions'].apply(lambda x: source_labels.index(x)).tolist(),
        target=flow_counts['cluster_labels_annotated_coarse'].apply(lambda x: target_labels.index(x) + len(source_labels)).tolist(),
        value=flow_counts['count']
    )
))

fig.update_layout(title_text="Flow from Annotated Regions to Coarse Labels", font_size=10)
fig.show()

adata

annotated_regions = adata.obs['cluster_labels_annotated_regions']
coarse_labels = adata.obs['annotation_post_scanvi70_broad']

# Create a dataframe
df = pd.DataFrame({
    'cluster_labels_annotated_regions': annotated_regions,
    'annotation_post_scanvi70_broad': coarse_labels
})
df

df=df[df['cluster_labels_annotated_regions']!='outlier']

medulla_df = df[df['cluster_labels_annotated_regions'] == 'Medulla']
cortex_df = df[df['cluster_labels_annotated_regions'] == 'Cortex']

# Sort each dataframe by the 'cluster_labels_coarse' column
medulla_df_sorted = medulla_df.sort_values(by='annotation_post_scanvi70_broad')
cortex_df_sorted = cortex_df.sort_values(by='annotation_post_scanvi70_broad')

# Combine the sorted dataframes back into one
sorted_df = pd.concat([medulla_df_sorted, cortex_df_sorted])

print(sorted_df)

!pip install plotly

sorted_df['annotation_post_scanvi70_broad'] = sorted_df['annotation_post_scanvi70_broad'].cat.remove_unused_categories()
sorted_df['cluster_labels_annotated_regions'] = sorted_df['cluster_labels_annotated_regions'].cat.remove_unused_categories()

import pandas as pd
import plotly.graph_objects as go

# Assuming sorted_df is your dataframe created earlier
# Prepare data for the Sankey diagram
source_labels = sorted_df['cluster_labels_annotated_regions'].unique().tolist()
target_labels = sorted_df['annotation_post_scanvi70_broad'].unique().tolist()
labels = source_labels + target_labels

source_indices = sorted_df['cluster_labels_annotated_regions'].apply(lambda x: source_labels.index(x)).tolist()
target_indices = sorted_df['annotation_post_scanvi70_broad'].apply(lambda x: target_labels.index(x) + len(source_labels)).tolist()

# Create a dataframe for the counts of each flow
flow_counts = sorted_df.groupby(['cluster_labels_annotated_regions', 'annotation_post_scanvi70_broad']).size().reset_index(name='count')

# Create the Sankey plot
fig = go.Figure(go.Sankey(
    node=dict(
        pad=15,
        thickness=20,
        line=dict(color="black", width=0.5),
        label=labels
    ),
    link=dict(
        source=flow_counts['cluster_labels_annotated_regions'].apply(lambda x: source_labels.index(x)).tolist(),
        target=flow_counts['annotation_post_scanvi70_broad'].apply(lambda x: target_labels.index(x) + len(source_labels)).tolist(),
        value=flow_counts['count']
    )
))

fig.update_layout(title_text="Flow from Annotated Regions to Coarse Labels", font_size=10)
fig.show()

flow_counts.shape

filtered_flow_counts.shape

!pip install -U kaleido

!pip install -U kaleido

import pandas as pd
import plotly.graph_objects as go
import kaleido

# Assuming sorted_df is your dataframe created earlier
# Prepare data for the Sankey diagram
source_labels = sorted_df['cluster_labels_annotated_regions'].unique().tolist()
target_labels = sorted_df['annotation_post_scanvi70_broad'].unique().tolist()
labels = source_labels + target_labels

# Create a dataframe for the counts of each flow
flow_counts = sorted_df.groupby(['cluster_labels_annotated_regions', 'annotation_post_scanvi70_broad']).size().reset_index(name='count')

# Calculate the total count for each target label
target_totals = flow_counts.groupby('annotation_post_scanvi70_broad')['count'].sum().reset_index(name='total_count')

# Merge the total counts back into the flow_counts dataframe
flow_counts = flow_counts.merge(target_totals, on='annotation_post_scanvi70_broad')

# Determine the threshold for 5% of each target label's total count
flow_counts['threshold'] = 0.1 * flow_counts['total_count']

# Filter the flow_counts to include only flows greater than 5% of the total for each target label
filtered_flow_counts = flow_counts[flow_counts['count'] > flow_counts['threshold']]

# Prepare the indices for the Sankey plot
source_indices = filtered_flow_counts['cluster_labels_annotated_regions'].apply(lambda x: source_labels.index(x)).tolist()
target_indices = filtered_flow_counts['annotation_post_scanvi70_broad'].apply(lambda x: target_labels.index(x) + len(source_labels)).tolist()

# Create the Sankey plot
fig = go.Figure(go.Sankey(
    node=dict(
        pad=150,
        thickness=200,
        line=dict(color="black", width=5),
        label=labels
    ),
    link=dict(
        source=source_indices,
        target=target_indices,
        value=filtered_flow_counts['count']
    )
))

#fig.update_layout(title_text="Flow from Annotated Regions to Coarse Labels", font_size=10)
fig.update_layout(
    title_text="",
    font_size=100,
    width=6000,  # Adjust the width as needed
    height=8000   # Adjust the height as needed
)

# Save the figure
#fig.write_image("sankey_plot.png")

# Show the figure
fig.show()

import pandas as pd
import plotly.graph_objects as go
import kaleido

# Assuming sorted_df is your dataframe created earlier
# Prepare data for the Sankey diagram
source_labels = sorted_df['cluster_labels_annotated_regions'].unique().tolist()
target_labels = sorted_df['annotation_post_scanvi70_broad'].unique().tolist()
labels = source_labels + target_labels

# Create a dataframe for the counts of each flow
flow_counts = sorted_df.groupby(['cluster_labels_annotated_regions', 'annotation_post_scanvi70_broad']).size().reset_index(name='count')

# Calculate the total count for each target label
target_totals = flow_counts.groupby('annotation_post_scanvi70_broad')['count'].sum().reset_index(name='total_count')

# Merge the total counts back into the flow_counts dataframe
flow_counts = flow_counts.merge(target_totals, on='annotation_post_scanvi70_broad')

# Determine the threshold for 5% of each target label's total count
flow_counts['threshold'] = 0.1 * flow_counts['total_count']

# Filter the flow_counts to include only flows greater than 5% of the total for each target label
filtered_flow_counts = flow_counts[flow_counts['count'] > flow_counts['threshold']]

# Prepare the indices for the Sankey plot
source_indices = filtered_flow_counts['cluster_labels_annotated_regions'].apply(lambda x: source_labels.index(x)).tolist()
target_indices = filtered_flow_counts['annotation_post_scanvi70_broad'].apply(lambda x: target_labels.index(x) + len(source_labels)).tolist()

x_coords = [0.1]  + [0.1]
y_coords = [0.4]  + [0.1]

# Create the Sankey plot
fig = go.Figure(go.Sankey(
    node=dict(
        pad=15,
        thickness=20,
        line=dict(color="black", width=1),
        label=labels,
        x=x_coords,
        y=y_coords
    ),
    link=dict(
        source=source_indices,
        target=target_indices,
        value=filtered_flow_counts['count']
    )
))

#fig.update_layout(title_text="Flow from Annotated Regions to Coarse Labels", font_size=10)
fig.update_layout(
    title_text="",
    font_size=10,
    width=600,  # Adjust the width as needed
    height=800   # Adjust the height as needed
)

# Save the figure
#fig.write_image("sankey_plot.png")

# Show the figure
fig.show()



import pandas as pd
import plotly.graph_objects as go
import kaleido

# Assuming sorted_df is your dataframe created earlier
# Prepare data for the Sankey diagram
source_labels = sorted_df['cluster_labels_annotated_regions'].unique().tolist()
target_labels = sorted_df['annotation_post_scanvi70_broad'].unique().tolist()
labels = source_labels + target_labels

# Create a dataframe for the counts of each flow
flow_counts = sorted_df.groupby(['cluster_labels_annotated_regions', 'annotation_post_scanvi70_broad']).size().reset_index(name='count')

# Calculate the total count for each target label
target_totals = flow_counts.groupby('annotation_post_scanvi70_broad')['count'].sum().reset_index(name='total_count')

# Merge the total counts back into the flow_counts dataframe
flow_counts = flow_counts.merge(target_totals, on='annotation_post_scanvi70_broad')

# Determine the threshold for 5% of each target label's total count
flow_counts['threshold'] = 0.1 * flow_counts['total_count']

# Filter the flow_counts to include only flows greater than 5% of the total for each target label
filtered_flow_counts = flow_counts[flow_counts['count'] > flow_counts['threshold']]

# Prepare the indices for the Sankey plot
source_indices = filtered_flow_counts['cluster_labels_annotated_regions'].apply(lambda x: source_labels.index(x)).tolist()
target_indices = filtered_flow_counts['annotation_post_scanvi70_broad'].apply(lambda x: target_labels.index(x) + len(source_labels)).tolist()

x_coords = [0.1]  + [0.1]
y_coords = [0.4]  + [0.1]

# Create the Sankey plot
fig = go.Figure(go.Sankey(
    node=dict(
        pad=150,  # Increased padding by 10x
        thickness=200,  # Increased thickness by 10x
        line=dict(color="black", width=10),  # Increased line width by 10x
        label=labels,
        x=x_coords,
        y=y_coords
    ),
    link=dict(
        source=source_indices,
        target=target_indices,
        value=filtered_flow_counts['count']
    )
))

# Adjust the layout of the figure
fig.update_layout(
    title_text="",
    font_size=100,  # Increased font size by 10x
    width=6000,  # Increased width by 10x
    height=8000,  # Increased height by 10x
    margin=dict(l=200, r=200, t=200, b=200)  # Increased margins by 10x
)


# Show the figure
fig.show()

import pandas as pd
import plotly.graph_objects as go
import kaleido

# Assuming sorted_df is your dataframe created earlier
# Prepare data for the Sankey diagram
source_labels = sorted_df['cluster_labels_annotated_regions'].unique().tolist()
target_labels = sorted_df['annotation_post_scanvi70_broad'].unique().tolist()
labels = source_labels + target_labels

# Create a dataframe for the counts of each flow
flow_counts = sorted_df.groupby(['cluster_labels_annotated_regions', 'annotation_post_scanvi70_broad']).size().reset_index(name='count')

# Calculate the total count for each target label
target_totals = flow_counts.groupby('annotation_post_scanvi70_broad')['count'].sum().reset_index(name='total_count')

# Merge the total counts back into the flow_counts dataframe
flow_counts = flow_counts.merge(target_totals, on='annotation_post_scanvi70_broad')

# Determine the threshold for 5% of each target label's total count
flow_counts['threshold'] = 0.1 * flow_counts['total_count']

# Filter the flow_counts to include only flows greater than 5% of the total for each target label
filtered_flow_counts = flow_counts[flow_counts['count'] > flow_counts['threshold']]

# Prepare the indices for the Sankey plot
source_indices = filtered_flow_counts['cluster_labels_annotated_regions'].apply(lambda x: source_labels.index(x)).tolist()
target_indices = filtered_flow_counts['annotation_post_scanvi70_broad'].apply(lambda x: target_labels.index(x) + len(source_labels)).tolist()

x_coords = [0.1]  + [0.1]
y_coords = [0.6]  + [0.1]

# Create the Sankey plot
fig = go.Figure(go.Sankey(
    node=dict(
        pad=60,
        thickness=30,
        line=dict(color="black", width=4),
        label=labels,
        x=x_coords,
        y=y_coords
    ),
    link=dict(
        source=source_indices,
        target=target_indices,
        value=filtered_flow_counts['count']
    )
))

#fig.update_layout(title_text="Flow from Annotated Regions to Coarse Labels", font_size=10)
fig.update_layout(
    title_text="",
    font_size=60,
    width=2400,  # Adjust the width as needed
    height=3600,   # Adjust the height as needed
    margin=dict(l=20, r=20, t=20, b=20)
)

# Save the figure
#fig.write_image("sankey_plot.png")

# Show the figure
fig.show()

import pandas as pd
import plotly.graph_objects as go
import kaleido

# Assuming sorted_df is your dataframe created earlier
# Prepare data for the Sankey diagram
source_labels = sorted_df['cluster_labels_annotated_regions'].unique().tolist()
target_labels = sorted_df['annotation_post_scanvi70_broad'].unique().tolist()
labels = source_labels + target_labels

# Create a dataframe for the counts of each flow
flow_counts = sorted_df.groupby(['cluster_labels_annotated_regions', 'annotation_post_scanvi70_broad']).size().reset_index(name='count')

# Calculate the total count for each target label
target_totals = flow_counts.groupby('annotation_post_scanvi70_broad')['count'].sum().reset_index(name='total_count')

# Merge the total counts back into the flow_counts dataframe
flow_counts = flow_counts.merge(target_totals, on='annotation_post_scanvi70_broad')

# Determine the threshold for 5% of each target label's total count
flow_counts['threshold'] = 0.1 * flow_counts['total_count']

# Filter the flow_counts to include only flows greater than 5% of the total for each target label
filtered_flow_counts = flow_counts[flow_counts['count'] > flow_counts['threshold']]

# Prepare the indices for the Sankey plot
source_indices = filtered_flow_counts['cluster_labels_annotated_regions'].apply(lambda x: source_labels.index(x)).tolist()
target_indices = filtered_flow_counts['annotation_post_scanvi70_broad'].apply(lambda x: target_labels.index(x) + len(source_labels)).tolist()

# Set the positions for the nodes
num_sources = len(source_labels)
num_targets = len(target_labels)

# Source coordinates
x_coords = [0.1] * num_sources
y_coords = [(i + 1) / (num_sources + 1) * 2 / 3 + 1 / 3 for i in range(num_sources)]  # Adding padding below

# Target coordinates
x_coords += [0.9] * num_targets
y_coords += [(i + 1) / (num_targets + 1) * 2 / 3 for i in range(num_targets)]  # Adding padding above

# Create the Sankey plot
fig = go.Figure(go.Sankey(
    node=dict(
        pad=15,
        thickness=20,
        line=dict(color="black", width=1),
        label=labels,
        x=x_coords,
        y=y_coords
    ),
    link=dict(
        source=source_indices,
        target=target_indices,
        value=filtered_flow_counts['count']
    )
))

# Adjust the layout of the figure
fig.update_layout(
    title_text="Flow from Annotated Regions to Coarse Labels",
    font_size=10,
    width=600,  # Adjust the width as needed
    height=1200,  # Increased height significantly
    margin=dict(l=50, r=50, t=50, b=50)  # Increased margins
)


# Show the figure
fig.show()

import pandas as pd
import plotly.graph_objects as go
import kaleido

# Assuming sorted_df is your dataframe created earlier
# Prepare data for the Sankey diagram
source_labels = sorted_df['cluster_labels_annotated_regions'].unique().tolist()
target_labels = sorted_df['annotation_post_scanvi70_broad'].unique().tolist()
labels = source_labels + target_labels

# Create a dataframe for the counts of each flow
flow_counts = sorted_df.groupby(['cluster_labels_annotated_regions', 'annotation_post_scanvi70_broad']).size().reset_index(name='count')

# Calculate the total count for each target label
target_totals = flow_counts.groupby('annotation_post_scanvi70_broad')['count'].sum().reset_index(name='total_count')

# Merge the total counts back into the flow_counts dataframe
flow_counts = flow_counts.merge(target_totals, on='annotation_post_scanvi70_broad')

# Determine the threshold for 5% of each target label's total count
flow_counts['threshold'] = 0.1 * flow_counts['total_count']

# Filter the flow_counts to include only flows greater than 5% of the total for each target label
filtered_flow_counts = flow_counts[flow_counts['count'] > flow_counts['threshold']]

# Prepare the indices for the Sankey plot
source_indices = filtered_flow_counts['cluster_labels_annotated_regions'].apply(lambda x: source_labels.index(x)).tolist()
target_indices = filtered_flow_counts['annotation_post_scanvi70_broad'].apply(lambda x: target_labels.index(x) + len(source_labels)).tolist()

# Set the positions for the nodes
num_sources = len(source_labels)
num_targets = len(target_labels)

# Source coordinates
x_coords = [0.1] * num_sources
y_coords = [(i + 1) / (num_sources + 1) * 2 / 3 for i in range(num_sources)]  # Adding padding above

# Target coordinates
x_coords += [0.9] * num_targets
y_coords += [(i + 1) / (num_targets + 1) * 2 / 3 + 1 / 3 for i in range(num_targets)]  # Adding padding below

# Normalize the y-coordinates to fit in the range [0, 1]
max_y = max(y_coords)
y_coords = [y / max_y for y in y_coords]

# Create the Sankey plot
fig = go.Figure(go.Sankey(
    node=dict(
        pad=15,
        thickness=20,
        line=dict(color="black", width=1),
        label=labels,
        x=x_coords,
        y=y_coords
    ),
    link=dict(
        source=source_indices,
        target=target_indices,
        value=filtered_flow_counts['count']
    )
))

# Adjust the layout of the figure
fig.update_layout(
    title_text="Flow from Annotated Regions to Coarse Labels",
    font_size=10,
    width=600,  # Adjust the width as needed
    height=1200,  # Increased height significantly
    margin=dict(l=50, r=50, t=50, b=50)  # Adjust margins as needed
)



# Show the figure
fig.show()

import pandas as pd
import plotly.graph_objects as go
import kaleido

# Assuming sorted_df is your dataframe created earlier
# Prepare data for the Sankey diagram
source_labels = sorted_df['cluster_labels_annotated_regions'].unique().tolist()
target_labels = sorted_df['annotation_post_scanvi70_broad'].unique().tolist()
labels = source_labels + target_labels

# Create a dataframe for the counts of each flow
flow_counts = sorted_df.groupby(['cluster_labels_annotated_regions', 'annotation_post_scanvi70_broad']).size().reset_index(name='count')

# Calculate the total count for each target label
target_totals = flow_counts.groupby('annotation_post_scanvi70_broad')['count'].sum().reset_index(name='total_count')

# Merge the total counts back into the flow_counts dataframe
flow_counts = flow_counts.merge(target_totals, on='annotation_post_scanvi70_broad')

# Determine the threshold for 5% of each target label's total count
flow_counts['threshold'] = 0.1 * flow_counts['total_count']

# Filter the flow_counts to include only flows greater than 5% of the total for each target label
filtered_flow_counts = flow_counts[flow_counts['count'] > flow_counts['threshold']]

# Prepare the indices for the Sankey plot
source_indices = filtered_flow_counts['cluster_labels_annotated_regions'].apply(lambda x: source_labels.index(x)).tolist()
target_indices = filtered_flow_counts['annotation_post_scanvi70_broad'].apply(lambda x: target_labels.index(x) + len(source_labels)).tolist()

x_coords = [0.15]  + [0.1]
y_coords = [0.55]  + [0.1]

# Create the Sankey plot
fig = go.Figure(go.Sankey(
    node=dict(
        pad=120,
        thickness=200,
        line=dict(color="black", width=5),
        label=labels,
        x=x_coords,
        y=y_coords
    ),
    link=dict(
        source=source_indices,
        target=target_indices,
        value=filtered_flow_counts['count']
    )
))

#fig.update_layout(title_text="Flow from Annotated Regions to Coarse Labels", font_size=10)
fig.update_layout(
    title_text="",
    font_size=100,
    width=6000,  # Adjust the width as needed
    height=12000   # Adjust the height as needed
)

# Save the figure
#fig.write_image("sankey_plot.png")

# Show the figure
fig.show()

adata



import matplotlib.pyplot as plt
import seaborn as sns

# Assuming new_adata_subset is your AnnData object
# Extract the relevant columns
samples = adata.obs['sample']
cluster_labels = adata.obs['cluster_labels_annotated_regions']

# Create a DataFrame from the relevant columns
data = pd.DataFrame({'sample': samples, 'cluster_labels_annotated_regions': cluster_labels})
data=data[data['cluster_labels_annotated_regions']!='outlier']
data['cluster_labels_annotated_regions'] = data['cluster_labels_annotated_regions'].cat.remove_unused_categories()
# Group by 'sample' and 'cluster_labels_annotated' and count the occurrences
grouped_data = data.groupby(['sample', 'cluster_labels_annotated_regions']).size().unstack(fill_value=0)
desired_order = [
    "Pediatric1", "HK2753", "HK3039", "HK3106", "HK3531", "HK3531_2",
    "HK3063", "HK3542", "HK2874", "HK2695", "HK3035", "HK3035_2",
     "HK2841", "HK2873","HK2844", "HK2844_2"
]

grouped_data=grouped_data.reindex(desired_order)
sample_totals = data['sample'].value_counts()
sample_totals=sample_totals.reindex(desired_order)
normalized_count_df = grouped_data.div(sample_totals, axis='index')
ax = normalized_count_df.plot(kind='bar', stacked=True, figsize=(8, 6), colormap='tab10')

# Set plot labels and title
plt.xlabel('')
plt.ylabel('Relative Amount')
plt.title('Sample Region Composition')
plt.legend(title='Region Labels', bbox_to_anchor=(1.05, 1), loc='upper left', frameon=False)
plt.tick_params(axis='x', which='both', length=0)
ax.spines['top'].set_visible(False)
ax.spines['right'].set_visible(False)
plt.tight_layout()
plt.savefig('Cortex Medulla Samples.png', dpi=450, bbox_inches='tight')
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming new_adata_subset is your AnnData object
# Extract the relevant columns
samples = adata.obs['sample']
cluster_labels = adata.obs['cluster_labels_annotated_regions']

# Create a DataFrame from the relevant columns
data = pd.DataFrame({'sample': samples, 'cluster_labels_annotated_regions': cluster_labels})
data = data[data['cluster_labels_annotated_regions'] != 'outlier']
data['cluster_labels_annotated_regions'] = data['cluster_labels_annotated_regions'].cat.remove_unused_categories()

# Group by 'sample' and 'cluster_labels_annotated_regions' and count the occurrences
grouped_data = data.groupby(['sample', 'cluster_labels_annotated_regions']).size().unstack(fill_value=0)

desired_order = [
    "Pediatric1", "HK2753", "HK3039", "HK3106", "HK3531", "HK3531_2",
    "HK3063", "HK3542", "HK2874", "HK2695", "HK3035", "HK3035_2",
    "HK2841", "HK2873", "HK2844", "HK2844_2"
]

grouped_data = grouped_data.reindex(desired_order)
sample_totals = data['sample'].value_counts()
sample_totals = sample_totals.reindex(desired_order)
normalized_count_df = grouped_data.div(sample_totals, axis='index')

# Extract specific colors from 'tab10' colormap
colors = plt.get_cmap('tab10').colors
blue = colors[0]
orange = colors[1]

# Apply the colors to the plot
ax = normalized_count_df.plot(kind='bar', stacked=True, figsize=(8, 6), color=[blue, orange])

# Set plot labels and title
plt.xlabel('')
plt.ylabel('Relative Amount')
plt.title('Sample Region Composition')
plt.legend(title='Region Labels', bbox_to_anchor=(1.05, 1), loc='upper left', frameon=False)
plt.tick_params(axis='x', which='both', length=0)
ax.spines['top'].set_visible(False)
ax.spines['right'].set_visible(False)
plt.tight_layout()
plt.savefig('Cortex_Medulla_Samples.png', dpi=450, bbox_inches='tight')
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# Assuming new_adata_subset is your AnnData object
# Extract the relevant columns
samples = adata.obs['sample']
cluster_labels = adata.obs['cluster_labels_annotated_coarse_coarse']

# Create a DataFrame from the relevant columns
data = pd.DataFrame({'sample': samples, 'cluster_labels_annotated_coarse_coarse': cluster_labels})
data = data[data['cluster_labels_annotated_coarse_coarse'] != 'outlier']
data['cluster_labels_annotated_coarse_coarse'] = data['cluster_labels_annotated_coarse_coarse'].cat.remove_unused_categories()

# Group by 'sample' and 'cluster_labels_annotated' and count the occurrences
grouped_data = data.groupby(['sample', 'cluster_labels_annotated_coarse_coarse']).size().unstack(fill_value=0)
desired_order = [
    "Pediatric1", "HK2753", "HK3039", "HK3106", "HK3531", "HK3531_2",
    "HK3063", "HK3542", "HK2874", "HK2695", "HK3035", "HK3035_2",
     "HK2841", "HK2873","HK2844", "HK2844_2"
]

grouped_data=grouped_data.reindex(desired_order)
sample_totals = data['sample'].value_counts()
sample_totals=sample_totals.reindex(desired_order)
normalized_count_df = grouped_data.div(sample_totals, axis='index')
ax = normalized_count_df.plot(kind='bar', stacked=True, figsize=(8, 8), colormap='tab10')

# Set plot labels and title
plt.xlabel('')
plt.ylabel('Relative Amount')
plt.title('Sample Niche Composition')
plt.legend(title='Niche Labels', bbox_to_anchor=(1.05, 1), loc='upper left', frameon=False)
plt.tick_params(axis='x', which='both', length=0)
ax.spines['top'].set_visible(False)
ax.spines['right'].set_visible(False)
plt.tight_layout()
plt.savefig('Kindey Niches Composition.png', dpi=450, bbox_inches='tight')
plt.show()

adata.write('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/all_Cosmx_cleaned_neighbored.h5ad')

pt_subset=adata[adata.obs['annotation_post_scanvi70_broad']=='PT'].copy()

pt_subset=pt_subset[pt_subset.obs['cluster_labels_annotated_regions']!='outlier']

pt_subset.X = pt_subset.layers["counts"]
sc.pp.log1p(pt_subset)
sc.tl.rank_genes_groups(pt_subset, groupby='cluster_labels_annotated_regions', method='wilcoxon', pts = True)
sc.pl.rank_genes_groups(pt_subset, n_genes=25, sharey=False)

result = pt_subset.uns['rank_genes_groups']

# Create a dictionary to store the dataframes for each group
dfs = {}
for group in result['names'].dtype.names:
    dfs[group] = pd.DataFrame({
        'names': result['names'][group],
        'scores': result['scores'][group],
        'logfoldchanges': result['logfoldchanges'][group],
        'pvals': result['pvals'][group],
        'pvals_adj': result['pvals_adj'][group],
        'pts': result['pts'][group],
        'pts_rest': result['pts_rest'][group]
    })

# Example: Accessing the dataframe for the first group
first_group = list(dfs.keys())[0]
print(f"DataFrame for group: {first_group}")
print(dfs[first_group])

# If you want to save each dataframe to a CSV file
for group, df in dfs.items():
    df.to_csv(f"rank_genes_groups_{group}.csv", index=False)