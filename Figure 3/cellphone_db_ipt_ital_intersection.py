# -*- coding: utf-8 -*-
"""Cellphone DB iPT/iTAL intersection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Bng-qd8Cs9jqRKD7i1eQVCQPo8KJWY8d
"""

!pip install --quiet scanpy
!pip install --quiet leidenalg

import csv
import anndata as ad
import gzip
import os
import scipy.io
import scanpy as sc
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from google.colab import drive
import leidenalg as la
from pathlib import Path

from google.colab import drive
drive.mount('/content/drive')

import gc
gc.collect()

#sn_imputed_all_cells=sc.read_h5ad('/content/drive/MyDrive/Bernhard/Immune_Cell_Atlas/ECM/Plasma_ECM_Neighborhood.h5ad')
#sn_imputed_all_cells=sc.read_h5ad('/content/drive/MyDrive/Bernhard/Immune_Cell_Atlas/ECM/Macro_ECM_Neighborhood.h5ad')
sn_imputed_all_cells=sc.read_h5ad('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/iTAL_subclustering/intersection_iPT_iTAL_ME.h5ad')
sn_imputed_immune=sc.read_h5ad('/content/drive/MyDrive/Bernhard/Immune_Cell_Atlas/Cosmx_immune_imputed_from_SN_np.h5ad')

sn_imputed_immune

sn_imputed_all_cells

##first transfer annotations
#adata=sc.read_h5ad('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/all_Cosmx_cleaned_neighbored.h5ad')

#cell_ids_in_sn_imputed_all_cells = sn_imputed_all_cells.obs.index
#cell_ids_in_adata = adata.obs.index
#common_cell_ids = cell_ids_in_sn_imputed_all_cells.intersection(cell_ids_in_adata)
#adata = adata[common_cell_ids].copy()

#sn_imputed_all_cells.obs['annotation_post_scanvi70_broad']=adata.obs['annotation_post_scanvi70_broad']
#sn_imputed_all_cells.obs['annotation_post_scanvi70_coarse']=adata.obs['annotation_post_scanvi70_coarse']
#sn_imputed_all_cells.obs['type']=adata.obs['type']

#sn_imputed_all_cells.obs['annotations_after_scanvi_simple']=sn_imputed_all_cells.obs['annotation_post_scanvi70_coarse']

#check for common cells in both datasets and subset the immune cells
cell_ids_in_sn_imputed_all_cells = sn_imputed_all_cells.obs.index
cell_ids_in_sn_imputed_immune = sn_imputed_immune.obs.index
common_cell_ids = cell_ids_in_sn_imputed_immune.intersection(cell_ids_in_sn_imputed_all_cells)
sn_imputed_immune = sn_imputed_immune[common_cell_ids].copy()
sn_imputed_all_cells = sn_imputed_all_cells[~sn_imputed_all_cells.obs.index.isin(common_cell_ids)].copy()

sn_imputed_all_cells.obs['annotations_after_scanvi_simple']=sn_imputed_all_cells.obs['annotation_post_scanvi70_broad'].copy()

sn_imputed_immune.obs['annotations_after_scanvi_simple'].value_counts()

intersection_neighborhood= sn_imputed_all_cells.concatenate(sn_imputed_immune, join='outer', index_unique=None, batch_key=None)
intersection_neighborhood

intersection_neighborhood.obs['annotations_after_scanvi_simple'].value_counts()

pops=['IC A', 'pDC', 'PC', 'Baso/Mast', 'PEC', 'IC B']
mask = intersection_neighborhood.obs['annotations_after_scanvi_simple'].isin(pops)
intersection_neighborhood=intersection_neighborhood[~mask]

intersection_neighborhood.obs['annotations_after_scanvi_simple'].value_counts()

b_subset.obs['type']=b_subset.obs['type'].astype('str')
b_subset.obs['annotations_after_scanvi_simple']=b_subset.obs['annotations_after_scanvi_simple'].astype('str')
b_subset.obs['microenvironment']='Macro_'+b_subset.obs['type']
b_subset.obs['microenvironment_cell_types'] = b_subset.obs['annotations_after_scanvi_simple'].astype(str) + '_' + b_subset.obs['microenvironment'].astype(str)

b_subset.obs['type']=b_subset.obs['type'].astype('category')
b_subset.obs['annotations_after_scanvi_simple']=b_subset.obs['annotations_after_scanvi_simple'].astype('category')
b_subset.obs['microenvironment_cell_types']=b_subset.obs['microenvironment_cell_types'].astype('category')
b_subset.obs['microenvironment']=b_subset.obs['microenvironment'].astype('category')

b_subset.obs['microenvironment'].value_counts()

intersection_neighborhood.write('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/iTAL_subclustering/intersection_iPT_iTAL_ME_immune_added.h5ad')

metaData = intersection_neighborhood.obs[["annotations_after_scanvi_simple"]]
metaData["barcode_sample"] = intersection_neighborhood.obs.index
metaData = metaData.rename(columns={'annotations_after_scanvi_simple': 'cell_type'})
metaData = metaData[['barcode_sample', 'cell_type']]
print(metaData)

metaData.to_csv("/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/iTAL_subclustering/metadata_intersection.csv", index = False)
meta_file_path = "/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/iTAL_subclustering/metadata_intersection.csv"
metadata = pd.read_csv(meta_file_path, sep = ',')

#microenvironment=adata.obs[["microenvironment"]]
#microenvironment["microenvironment_cell_types"] = adata.obs["microenvironment_cell_types"]
#microenvironment = microenvironment.rename(columns={'microenvironment_cell_types': 'cell_type'})
#microenvironment = microenvironment[['cell_type','microenvironment']]
#print(microenvironment)

#microenvironment.to_csv("/content/drive/MyDrive/Bernhard/Immune_Cell_Atlas/ECM/microenvironment_ECM.csv", index = False)
#microenvs_file_path = "/content/drive/MyDrive/Bernhard/Immune_Cell_Atlas/ECM/microenvironment_ECM.csv"
#microenvironment = pd.read_csv(microenvs_file_path, sep = ',')

#microenvironment['cell_type'].value_counts()

out_path = '/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/iTAL_subclustering/cellphoneintersection/'
cpdb_file_path = '/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Macrophage_Neighborhood/cellphonedb.zip'

adata=sc.read_h5ad('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/iTAL_subclustering/intersection_iPT_iTAL_ME_immune_added.h5ad')

#adata.write_h5ad(filename ='/content/drive/MyDrive/Bernhard/Immune_Cell_Atlas/ECM/ECM_Neighborhood_for_cellphone.h5ad')
counts_file_path = '/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/iTAL_subclustering/intersection_iPT_iTAL_ME_immune_added.h5ad'

!pip install --quiet cellphonedb

list(adata.obs.index).sort() == list(metadata['barcode_sample']).sort()

from cellphonedb.src.core.methods import cpdb_statistical_analysis_method

cpdb_results = cpdb_statistical_analysis_method.call(
    cpdb_file_path = cpdb_file_path,                 # mandatory: CellphoneDB database zip file.
    meta_file_path = meta_file_path,                 # mandatory: tsv file defining barcodes to cell label.
    counts_file_path = counts_file_path,             # mandatory: normalized count matrix - a path to the counts file, or an in-memory AnnData object
    counts_data = 'hgnc_symbol',                     # defines the gene annotation in counts matrix.
    microenvs_file_path = False,       # optional (default: None): defines cells per microenvironment.
    score_interactions = True,                       # optional: whether to score interactions or not.
    iterations = 1000,                               # denotes the number of shufflings performed in the analysis.
    threshold = 0.1,                                 # defines the min % of cells expressing a gene for this to be employed in the analysis.
    threads = 5,                                     # number of threads to use in the analysis.
    debug_seed = 42,                                 # debug randome seed. To disable >=0.
    result_precision = 3,                            # Sets the rounding for the mean values in significan_means.
    pvalue = 0.01,                                   # P-value threshold to employ for significance.
    subsampling = False,                             # To enable subsampling the data (geometri sketching).
    subsampling_log = False,                         # (mandatory) enable subsampling log1p for non log-transformed data inputs.
    #subsampling_num_pc = 100,                        # Number of componets to subsample via geometric skectching (dafault: 100).
    #subsampling_num_cells = 1000,                    # Number of cells to subsample (integer) (default: 1/3 of the dataset).
    separator = '|',                                 # Sets the string to employ to separate cells in the results dataframes "cellA|CellB".
    debug = False,                                   # Saves all intermediate tables employed during the analysis in pkl format.
    output_path = out_path,                          # Path to save results.
    output_suffix = None                             # Replaces the timestamp in the output files by a user defined string in the  (default: None).
    )

# Commented out IPython magic to ensure Python compatibility.
from cellphonedb.src.core.methods import cpdb_degs_analysis_method
import os
import ktplotspy as kpy
import matplotlib.pyplot as plt
# %matplotlib inline

list(cpdb_results.keys())

df = pd.read_csv('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/iTAL_subclustering/cellphoneintersection/statistical_analysis_significant_means_06_07_2024_123042.txt', sep='\t')

df

df['classification']=df['classification'].fillna('0')

cpdb_results['significant_means']

cpdb_results['pvalues']

df=cpdb_results['significant_means']

df

df['classification']=df['classification'].fillna('0')

print(df['classification'].unique())

pathways=['Signaling by WNT','Signaling by Annexin','0','Signaling by FC receptor','Signaling by Transforming growth factor','Signaling by Chemokines',
'Signaling by Interleukin','Adhesion by ICAM','Signaling by Platelet-derived growth factor','Signaling by Colony-Stimulating factor','Signaling by Notch','Signaling by Tumor necrosis factor']
mask=df['classification'].isin(pathways)
subset_df=df[mask]

pathways=['Signaling by Chemokines']
mask=df['classification'].isin(pathways)
subset_df=df[mask]

subset_df=df.copy()

subset_df=subset_df.drop(['id_cp_interaction','partner_a', 'partner_b',
       'gene_a', 'gene_b', 'secreted', 'receptor_a', 'receptor_b',
       'annotation_strategy','is_integrin', 'directionality', 'classification', 'rank'],axis=1)

subset_df.columns

subset_df=subset_df.set_index('interacting_pair')

subset_df



# Filter columns that start with 'iTAL' or 'iPT'
filtered_columns = subset_df.columns[subset_df.columns.str.startswith('iTAL') | subset_df.columns.str.startswith('iPT')]

# Create a new DataFrame with the filtered columns
filtered_df = subset_df[filtered_columns]

#print(filtered_df)

filtered_df

prefixes_to_remove = [
     'IC B', 'PC', 'DTL_ATL', 'Fibroblast',
    'EC_Peritub', 'Immune', 'EC_DVR', 'VSMC', 'DCT',
    'CNT', 'Podo', 'EC_glom', 'PEC', 'IC A', 'MC1', 'Macro III', 'Macro IV', 'cycling Lymphocytes'
]

# Create a list of columns to drop
columns_to_drop = [col for col in filtered_df.columns if any(prefix in col for prefix in prefixes_to_remove)]

# Drop the columns from the DataFrame
filtered_df = filtered_df.drop(columns=columns_to_drop)

print(filtered_df)

mask = (subset_df.columns.str.contains('\|Fibroblast')) | (subset_df.columns == 'interacting_pair')
#subset_df.columns.str.startswith('Plasma'))|  subset_df.columns.str.startswith('B') | subset_df.columns.str.contains('\|Stroma'))
# Subset the DataFrame using the mask  subset_df.columns.str.startswith('B'))
filtered_df = subset_df.loc[:, mask]

# Display the first few rows to check the result
#print(filtered_df.head())

#subset_df.to_csv('all_interaction.csv', index=True)

filtered_df.shape

filtered_df=filtered_df.set_index('interacting_pair')

filtered_df=filtered_df.fillna(0)

filtered_df['sum']=filtered_df.sum(axis=1)

filtered_df.shape

filtered_df=filtered_df[filtered_df['sum']>0]

filtered_df.shape

filtered_df=filtered_df.drop('sum', axis=1)

filtered_df.columns

filtered_df=filtered_df.drop(['iPT|TAL', 'iPT|iPT','iPT|iTAL', 'iTAL|iTAL', 'iTAL|iPT'], axis=1)

filtered_df['sum']=filtered_df.sum(axis=1)

filtered_df=filtered_df[filtered_df['sum']>0]

filtered_df.shape

filtered_df=filtered_df.drop('sum', axis=1)

filtered_df.columns

filtered_df=filtered_df.drop(['iTAL|PT','iTAL|TAL','iPT|PT'], axis=1)

filtered_df.shape

filtered_df['sum']=filtered_df.sum(axis=1)
filtered_df=filtered_df[filtered_df['sum']>0]
filtered_df=filtered_df.drop('sum', axis=1)
filtered_df.shape

filtered_df.head(3)

filtered_df_subset = filtered_df.where(filtered_df <= 0, 1)

#print(filtered_df_subset)

filtered_df_subset['sum']=filtered_df_subset.sum(axis=1)
filtered_df_subset=filtered_df_subset[filtered_df_subset['sum']<3]
filtered_df_subset.shape

filtered_df_subset

filtered_df_subset

# Assuming filtered_df is your DataFrame
filtered_columns = filtered_df.filter(regex='^(iTAL|iPT)', axis=1)

# Print the resulting DataFrame
print(filtered_columns)

filtered_columns['sum']=filtered_columns.sum(axis=1)

filtered_columns=filtered_columns[filtered_columns['sum']>0]
filtered_columns.shape

filtered_columns=filtered_columns.drop('sum', axis=1)

filtered_columns=filtered_columns.astype(float)

filtered_rows = filtered_columns[(filtered_columns['iPT|Fibroblast'] > 0.2) | (filtered_columns['iTAL|Fibroblast'] > 0.2)]
filtered_rows

filtered_rows.head(3)

df_iPT_Fibroblast = filtered_rows[['iPT|Fibroblast']].copy()
df_iTAL_Fibroblast = filtered_rows[['iTAL|Fibroblast']].copy()

# Rename the columns for clarity
df_iPT_Fibroblast.columns = ['Fibroblast']
df_iTAL_Fibroblast.columns = ['Fibroblast']

df_iPT_Fibroblast=df_iPT_Fibroblast[df_iPT_Fibroblast['Fibroblast']>0]
df_iTAL_Fibroblast=df_iTAL_Fibroblast[df_iTAL_Fibroblast['Fibroblast']>0]

df_iPT_Fibroblast.index = 'iPT' + df_iPT_Fibroblast.index.astype(str)
df_iTAL_Fibroblast.index = 'iTAL' + df_iTAL_Fibroblast.index.astype(str)
# Display the modified dataframe
#print(df_iPT_Fibroblast)

combined_df = pd.concat([df_iPT_Fibroblast, df_iTAL_Fibroblast], axis=0)

# Display the combined dataframe
print(combined_df)

# Split the interacting pair into index and column names
combined_df['Interacting_Pair'] = combined_df.index
combined_df[['Index', 'Column']] = combined_df['Interacting_Pair'].str.split('_', expand=True)

# Pivot the dataframe
df_pivoted = combined_df.pivot(index='Index', columns='Column', values='Fibroblast')

# Drop the intermediate columns used for pivoting
#df_pivoted = df_pivoted.reset_index()

# Display the pivoted dataframe
print(df_pivoted)

df_pivoted=df_pivoted.fillna(0)

df_pivoted



# Remove 'iPT' prefix from the index
df_pivoted.index = df_pivoted.index.str.replace('^iPT', 'i ', regex=True)
df_pivoted.index = df_pivoted.index.str.replace('^iTAL', 'o  ', regex=True)
# Display the updated dataframe
print(df_pivoted)

# Append ' iPT' or ' iTAL' to the indices that originally had those prefixes
df_pivoted.index = df_pivoted.index.str.replace('^(.*)$', lambda m: m.group(1) + ' iPT' if m.group(1).startswith('i') else m.group(1) + ' iTAL', regex=True)
df_pivoted

df_pivoted.index = df_pivoted.index.str.replace('^i ', '', regex=True)
df_pivoted.index = df_pivoted.index.str.replace('^o ', '', regex=True)

df_pivoted

df_pivoted.to_csv('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/iTAL_subclustering/cellphoneintersection/chordplotforitaliptintersectionfibroblastreceiver.csv', index=True)



# Split the interacting pair into index and column names
df_iPT_Fibroblast['Interacting_Pair'] = df_iPT_Fibroblast.index
df_iPT_Fibroblast[['Index', 'Column']] = df_iPT_Fibroblast['Interacting_Pair'].str.split('_', expand=True)

# Pivot the dataframe
df_pivoted = df_iPT_Fibroblast.pivot(index='Index', columns='Column', values='Fibroblast')

# Drop the intermediate columns used for pivoting
df_pivoted = df_pivoted.reset_index()

# Display the pivoted dataframe
print(df_pivoted)





filtered_rows.shape

second_part = filtered_rows.index.str.split('_').str[0]
original_columns=filtered_rows.columns
receiver_new_df = filtered_rows.copy()

if not isinstance(second_part, pd.Series):
    second_part = pd.Series(second_part, index=filtered_rows.index)

for col in filtered_rows.columns:
    indices_with_ones = filtered_rows[filtered_rows[col] == 1].index
    for idx in indices_with_ones:
        suffix = second_part.loc[idx]  # Now it should work as second_part is a Series
        new_column_name = f"{col}_{suffix}"
        if new_column_name not in receiver_new_df.columns:
            receiver_new_df[new_column_name] = 0
        receiver_new_df.loc[idx, new_column_name] = 1
receiver_new_df = receiver_new_df.drop(columns=original_columns)
receiver_new_df.index = ['_'.join(idx.split('_')[1:]) for idx in receiver_new_df.index]
receiver_new_df





filtered_df.to_csv('iTAL_receiver.csv', index=True)

save_copy=filtered_df.copy()

filtered_df=save_copy.copy()

print(filtered_df.columns)

#filtered_df.columns = filtered_df.columns.str.split('|').map(lambda x: x[1] if len(x) > 1 else x[0])
#filtered_df = filtered_df.sort_index(axis=1)
#filtered_df
filtered_df.columns = filtered_df.columns.str.replace(r'\|.*', '', regex=True)
#filtered_df

filtered_df=filtered_df.fillna(0)
filtered_df = filtered_df.where(filtered_df <= 0, 1)

# Display the updated DataFrame to verify
#print(filtered_df)

filtered_df.columns

import pandas as pd

# Assuming df is your DataFrame and it already contains the columns like 'B_no', 'B_yes', etc.

# Initialize a list to hold the categories
categories = set()

# Extract the category part before the '_no' or '_yes'
for col in filtered_df.columns:
    if 'Plasma_Healthy' in col or 'Plasma_Disease' in col:
        category = col.split('_')[0]
        categories.add(category)

# For each category, calculate the difference and create a new column
for category in categories:
    no_column = f"{category}_Plasma_Healthy"
    yes_column = f"{category}_Plasma_Disease"
    # Ensure both columns exist in the DataFrame to avoid KeyErrors
    if no_column in filtered_df.columns and yes_column in filtered_df.columns:
        filtered_df[f"{category}_Plasma_diff"] = filtered_df[yes_column] - filtered_df[no_column]

for col in filtered_df.columns:
    if 'B_Healthy' in col or 'B_Disease' in col:
        category = col.split('_')[0]
        categories.add(category)

# For each category, calculate the difference and create a new column
for category in categories:
    no_column = f"{category}_B_Healthy"
    yes_column = f"{category}_B_Disease"
    # Ensure both columns exist in the DataFrame to avoid KeyErrors
    if no_column in filtered_df.columns and yes_column in filtered_df.columns:
        filtered_df[f"{category}_B_diff"] = filtered_df[yes_column] - filtered_df[no_column]

for col in filtered_df.columns:
    if 'Macro_Healthy' in col or 'Macro_Disease' in col:
        category = col.split('_')[0]
        categories.add(category)

# For each category, calculate the difference and create a new column
for category in categories:
    no_column = f"{category}_Macro_Healthy"
    yes_column = f"{category}_Macro_Disease"
    # Ensure both columns exist in the DataFrame to avoid KeyErrors
    if no_column in filtered_df.columns and yes_column in filtered_df.columns:
        filtered_df[f"{category}_Macro_diff"] = filtered_df[yes_column] - filtered_df[no_column]

# Display the updated DataFrame to verify the new columns
print(filtered_df.columns)

columns_to_drop = []

# Loop through the column names
for col in filtered_df.columns:
    if not col.endswith('_diff'):
        columns_to_drop.append(col)

# Drop the identified columns
filtered_df.drop(columns=columns_to_drop, inplace=True)

# Display the updated DataFrame to verify the dropped columns
print(filtered_df.columns)

filtered_df.shape

filtered_df['sum']=filtered_df.sum(axis=1)

filtered_df=filtered_df[filtered_df['sum']!=0]
filtered_df.shape

filtered_df=filtered_df.drop('sum', axis=1)

filtered_df.to_csv('/content/drive/MyDrive/Bernhard/Immune_Cell_Atlas/ECM/B_receiver.csv', index=True)

plt.figure(figsize=(8, 40))
ax=sns.heatmap(filtered_df,cmap='plasma', annot=False, cbar=False, vmax=1, vmin=-1) # cbar is set to False as it doesn't directly apply to our custom mapping
#plt.title('Neighborhoods in Health and Diabetic Nephropathy')
#plt.ylabel('')
ax.tick_params(axis='x', length=0)  # Remove x-axis ticks
ax.tick_params(axis='y', length=0)
ax.set_ylabel('')
plt.xticks(rotation=90)
#plt.savefig('B_sender.png', dpi=900, bbox_inches='tight')
plt.show()

filtered_df = filtered_df.sort_index(axis=1)

plt.figure(figsize=(8, 40))
ax=sns.heatmap(filtered_df,cmap='plasma', annot=False, cbar=False, vmax=1, vmin=-1) # cbar is set to False as it doesn't directly apply to our custom mapping
#plt.title('Neighborhoods in Health and Diabetic Nephropathy')
#plt.ylabel('')
ax.tick_params(axis='x', length=0)  # Remove x-axis ticks
ax.tick_params(axis='y', length=0)
ax.set_ylabel('')
plt.xticks(rotation=90)
#plt.savefig('B_sender.png', dpi=900, bbox_inches='tight')
plt.show()

df_contains_pipe_B = df_contains_pipe_B.sort_index(axis=1)
print(df_contains_pipe_B.columns)

df_contains_pipe_B=df_contains_pipe_B.fillna(0)
df_contains_pipe_B = df_contains_pipe_B.where(df_contains_pipe_B <= 0, 1)

# Display the updated DataFrame to verify
print(df_contains_pipe_B)

save_df=df_contains_pipe_B.copy()

#df_contains_pipe_B=save_df.copy()

df_contains_pipe_B.columns = df_contains_pipe_B.columns.str.replace(r'\|.*', '', regex=True)
df_contains_pipe_B

df_contains_pipe_B

categories = set()

# Extract the category part before the '_no' or '_yes'
for col in df_contains_pipe_B.columns:
    if '_no' in col or '_yes' in col:
        category = col.split('_')[0]
        categories.add(category)

# For each category, calculate the difference and create a new column
for category in categories:
    no_column = f"{category}_no"
    yes_column = f"{category}_yes"
    # Ensure both columns exist in the DataFrame to avoid KeyErrors
    if no_column in df_contains_pipe_B.columns and yes_column in df_contains_pipe_B.columns:
        df_contains_pipe_B[f"{category}_diff"] = df_contains_pipe_B[yes_column] - df_contains_pipe_B[no_column]

# Display the updated DataFrame to verify the new columns
print(df_contains_pipe_B.columns)

columns_to_drop = []

# Loop through the column names
for col in df_contains_pipe_B.columns:
    if not col.endswith('_diff'):
        columns_to_drop.append(col)

# Drop the identified columns
df_contains_pipe_B.drop(columns=columns_to_drop, inplace=True)

# Display the updated DataFrame to verify the dropped columns
print(df_contains_pipe_B.columns)

df_contains_pipe_B['sum']=df_contains_pipe_B.sum(axis=1)

df_contains_pipe_B.shape

df_contains_pipe_B

df_contains_pipe_B=df_contains_pipe_B[df_contains_pipe_B['sum']!=0]
df_contains_pipe_B.shape

df_contains_pipe_B

#df_contains_pipe_B.to_csv('/content/drive/MyDrive/Bernhard/Immune_Cell_Atlas/B_Cell/B_Cell_Neighborhood/B_receiver.csv', index=True)
df_starts_with_B.to_csv('/content/drive/MyDrive/Bernhard/Immune_Cell_Atlas/B_Cell/B_Cell_Neighborhood/B_sender.csv', index=True)

df_contains_pipe_B=df_contains_pipe_B.drop('sum', axis=1)

plt.figure(figsize=(10, 30))
ax=sns.heatmap(df_contains_pipe_B, cmap='plasma',vmax=1, vmin=-1, annot=False, cbar=False) # cbar is set to False as it doesn't directly apply to our custom mapping
#plt.title('Neighborhoods in Health and Diabetic Nephropathy')
#plt.ylabel('')
ax.tick_params(axis='x', length=0)  # Remove x-axis ticks
ax.tick_params(axis='y', length=0)
ax.set_ylabel('')
plt.xticks(rotation=90)
#plt.savefig('B_receiver.png', dpi=900, bbox_inches='tight')
plt.show()



subset_df=subset_df.set_index('interacting_pair')

subset_df

df_starts_with_PT = subset_df.loc[:, subset_df.columns.str.startswith('PT')]
df_starts_with_TAL = subset_df.loc[:, subset_df.columns.str.startswith('TAL')]

df_starts_with_PT.shape

df_starts_with_PT

df_starts_with_TAL

df_starts_with_TAL.shape

df_starts_with_PT['sum']=df_starts_with_PT.sum(axis=1)
df_starts_with_PT=df_starts_with_PT[df_starts_with_PT['sum']>0]
df_starts_with_TAL['sum']=df_starts_with_TAL.sum(axis=1)
df_starts_with_TAL=df_starts_with_TAL[df_starts_with_TAL['sum']>0]

df_starts_with_PT=df_starts_with_PT.drop('sum', axis=1)
df_starts_with_TAL=df_starts_with_TAL.drop('sum', axis=1)

df_starts_with_PT.shape

df_starts_with_TAL.shape

df_starts_with_PT.columns
df_starts_with_PT.columns = df_starts_with_PT.columns.str.split('|').map(lambda x: x[1] if len(x) > 1 else x[0])
df_starts_with_PT = df_starts_with_PT.sort_index(axis=1)
df_starts_with_PT=df_starts_with_PT.fillna(0)
df_starts_with_PT = df_starts_with_PT.where(df_starts_with_PT <= 0, 1)
print(df_starts_with_PT)

df_starts_with_TAL.columns
df_starts_with_TAL.columns = df_starts_with_TAL.columns.str.split('|').map(lambda x: x[1] if len(x) > 1 else x[0])
df_starts_with_TAL = df_starts_with_TAL.sort_index(axis=1)
df_starts_with_TAL=df_starts_with_TAL.fillna(0)
df_starts_with_TAL = df_starts_with_TAL.where(df_starts_with_TAL <= 0, 1)
print(df_starts_with_TAL)

categories = set()

# Extract the category part before the '_no' or '_yes'
for col in df_starts_with_PT.columns:
    if '_no' in col or '_yes' in col:
        category = col.split('_')[0]
        categories.add(category)

# For each category, calculate the difference and create a new column
for category in categories:
    no_column = f"{category}_no"
    yes_column = f"{category}_yes"
    # Ensure both columns exist in the DataFrame to avoid KeyErrors
    if no_column in df_starts_with_PT.columns and yes_column in df_starts_with_PT.columns:
        df_starts_with_PT[f"{category}_diff"] = df_starts_with_PT[yes_column] - df_starts_with_PT[no_column]

# Display the updated DataFrame to verify the new columns
print(df_starts_with_PT.columns)

categories = set()

# Extract the category part before the '_no' or '_yes'
for col in df_starts_with_TAL.columns:
    if '_no' in col or '_yes' in col:
        category = col.split('_')[0]
        categories.add(category)

# For each category, calculate the difference and create a new column
for category in categories:
    no_column = f"{category}_no"
    yes_column = f"{category}_yes"
    # Ensure both columns exist in the DataFrame to avoid KeyErrors
    if no_column in df_starts_with_TAL.columns and yes_column in df_starts_with_TAL.columns:
        df_starts_with_TAL[f"{category}_diff"] = df_starts_with_TAL[yes_column] - df_starts_with_TAL[no_column]

# Display the updated DataFrame to verify the new columns
print(df_starts_with_TAL.columns)

columns_to_drop = []

# Loop through the column names
for col in df_starts_with_PT.columns:
    if not col.endswith('_diff'):
        columns_to_drop.append(col)

# Drop the identified columns
df_starts_with_PT.drop(columns=columns_to_drop, inplace=True)

# Display the updated DataFrame to verify the dropped columns
print(df_starts_with_PT.columns)

columns_to_drop = []

# Loop through the column names
for col in df_starts_with_TAL.columns:
    if not col.endswith('_diff'):
        columns_to_drop.append(col)

# Drop the identified columns
df_starts_with_TAL.drop(columns=columns_to_drop, inplace=True)

# Display the updated DataFrame to verify the dropped columns
print(df_starts_with_TAL.columns)

df_starts_with_TAL['sum']=df_starts_with_TAL.sum(axis=1)
df_starts_with_TAL=df_starts_with_TAL[df_starts_with_TAL['sum']!=0]
df_starts_with_TAL.shape

df_starts_with_PT['sum']=df_starts_with_PT.sum(axis=1)
df_starts_with_PT=df_starts_with_PT[df_starts_with_PT['sum']!=0]
df_starts_with_PT.shape

df_starts_with_PT=df_starts_with_PT.drop('sum', axis=1)
df_starts_with_TAL=df_starts_with_TAL.drop('sum', axis=1)

df_starts_with_TAL.to_csv('/content/drive/MyDrive/Bernhard/Immune_Cell_Atlas/B_Cell/B_Cell_Neighborhood/TAL_sender.csv', index=True)
df_starts_with_PT.to_csv('/content/drive/MyDrive/Bernhard/Immune_Cell_Atlas/B_Cell/B_Cell_Neighborhood/PT_sender.csv', index=True)

plt.figure(figsize=(10, 30))
ax=sns.heatmap(df_starts_with_PT, cmap='plasma',vmax=1, vmin=-1, annot=False, cbar=False) # cbar is set to False as it doesn't directly apply to our custom mapping
#plt.title('Neighborhoods in Health and Diabetic Nephropathy')
#plt.ylabel('')
ax.tick_params(axis='x', length=0)  # Remove x-axis ticks
ax.tick_params(axis='y', length=0)
ax.set_ylabel('')
plt.xticks(rotation=90)
#plt.savefig('PT_sender.png', dpi=900, bbox_inches='tight')
plt.show()

plt.figure(figsize=(10, 30))
ax=sns.heatmap(df_starts_with_TAL, cmap='plasma',vmax=1, vmin=-1, annot=False, cbar=False) # cbar is set to False as it doesn't directly apply to our custom mapping
#plt.title('Neighborhoods in Health and Diabetic Nephropathy')
#plt.ylabel('')
ax.tick_params(axis='x', length=0)  # Remove x-axis ticks
ax.tick_params(axis='y', length=0)
ax.set_ylabel('')
plt.xticks(rotation=90)
#plt.savefig('PT_sender.png', dpi=900, bbox_inches='tight')
plt.show()

df_contains_stroma = subset_df.loc[:, subset_df.columns.str.contains('\|Stroma')]

df_contains_stroma.shape

df_contains_stroma['sum']=df_contains_stroma.sum(axis=1)
df_contains_stroma=df_contains_stroma[df_contains_stroma['sum']>0]
df_contains_stroma=df_contains_stroma.drop(['sum'], axis=1)

df_contains_stroma.shape

df_contains_stroma = df_contains_stroma.sort_index(axis=1)
df_contains_stroma=df_contains_stroma.fillna(0)
df_contains_stroma = df_contains_stroma.where(df_contains_stroma <= 0, 1)

df_contains_stroma

df_contains_stroma.shape

df_contains_stroma.columns = df_contains_stroma.columns.str.replace(r'\|.*', '', regex=True)
df_contains_stroma
categories = set()

# Extract the category part before the '_no' or '_yes'
for col in df_contains_stroma.columns:
    if '_no' in col or '_yes' in col:
        category = col.split('_')[0]
        categories.add(category)

# For each category, calculate the difference and create a new column
for category in categories:
    no_column = f"{category}_no"
    yes_column = f"{category}_yes"
    # Ensure both columns exist in the DataFrame to avoid KeyErrors
    if no_column in df_contains_stroma.columns and yes_column in df_contains_stroma.columns:
        df_contains_stroma[f"{category}_diff"] = df_contains_stroma[yes_column] - df_contains_stroma[no_column]

# Display the updated DataFrame to verify the new columns
print(df_contains_stroma.columns)
columns_to_drop = []

# Loop through the column names
for col in df_contains_stroma.columns:
    if not col.endswith('_diff'):
        columns_to_drop.append(col)

# Drop the identified columns
df_contains_stroma.drop(columns=columns_to_drop, inplace=True)

# Display the updated DataFrame to verify the dropped columns
print(df_contains_stroma.columns)

df_contains_stroma['sum']=df_contains_stroma.sum(axis=1)
df_contains_stroma=df_contains_stroma[df_contains_stroma['sum']!=0]
df_contains_stroma.shape

df_contains_stroma=df_contains_stroma.drop('sum', axis=1)

df_contains_stroma

df_contains_stroma.to_csv('/content/drive/MyDrive/Bernhard/Immune_Cell_Atlas/B_Cell/B_Cell_Neighborhood/Stroma_receiver.csv', index=True)

plt.figure(figsize=(10, 30))
ax=sns.heatmap(df_contains_stroma, cmap='plasma',vmax=1, vmin=-1, annot=False, cbar=False) # cbar is set to False as it doesn't directly apply to our custom mapping
#plt.title('Neighborhoods in Health and Diabetic Nephropathy')
#plt.ylabel('')
ax.tick_params(axis='x', length=0)  # Remove x-axis ticks
ax.tick_params(axis='y', length=0)
ax.set_ylabel('')
plt.xticks(rotation=90)
#plt.savefig('PT_sender.png', dpi=900, bbox_inches='tight')
plt.show()









filtered_df=filtered_df[filtered_df["classification"]!='Signaling by Amyloid-beta precursor protein']
filtered_df=filtered_df[filtered_df["classification"]!='Signaling by Galectin']
filtered_df=filtered_df[filtered_df["classification"]!='Signaling by Semaphorin']
filtered_df=filtered_df[filtered_df["classification"]!='Signaling by Annexin']
filtered_df=filtered_df[filtered_df["classification"]!='Signaling by Notch']
filtered_df=filtered_df[filtered_df["classification"]!='Signaling by Cholesterol']
filtered_df=filtered_df[filtered_df["classification"]!='Adhesion by ICAM']
filtered_df=filtered_df[filtered_df["classification"]!='Signaling by Thrombospondin precursor']
print(filtered_df.shape)

print(filtered_df.columns)

filtered_df=filtered_df[filtered_df["classification"]!='Signaling by Amyloid-like protein']

filtered_df=filtered_df.drop(['id_cp_interaction', 'is_integrin','annotation_strategy','receptor_a', 'receptor_b','secreted','partner_a', 'partner_b'], axis=1)

filtered_df=filtered_df[filtered_df["gene_a"]!='HEBP1']
filtered_df=filtered_df[filtered_df["gene_a"]!='LRPAP1']
filtered_df=filtered_df[filtered_df["gene_a"]!='PPIA']
filtered_df=filtered_df[filtered_df["gene_a"]!='CD99']
filtered_df=filtered_df[filtered_df["gene_a"]!='KLRB1']
filtered_df=filtered_df[filtered_df["gene_a"]!='CLEC2B']
filtered_df=filtered_df[filtered_df["gene_a"]!='CYSLTR1']
filtered_df=filtered_df[filtered_df["gene_a"]!='SELPLG']
filtered_df=filtered_df[filtered_df["gene_a"]!='CD320']
filtered_df=filtered_df[filtered_df["gene_a"]!='CD99']
filtered_df=filtered_df[filtered_df["gene_a"]!='KLRB1']
filtered_df=filtered_df[filtered_df["gene_a"]!='LAIR1']

filtered_df=filtered_df[filtered_df["gene_b"]!='LAIR1']

filtered_df=filtered_df[filtered_df["gene_b"]!='CYSLTR1']

filtered_df=filtered_df[filtered_df["classification"]!='Signaling by Prostaglandin']

print(filtered_df.columns)

filtered_df

import pandas as pd

# Find the index of the 'B Cells|B Cells' column
start_col_index = filtered_df.columns.get_loc('B Cells|B Cells')

# Define a function to round values less than 20 to 0
def round_if_less_than_20(val):
    if val < 20:
        return 0
    else:
        return val

# Apply this function to all relevant columns
filtered_df.iloc[:, start_col_index:] = filtered_df.iloc[:, start_col_index:].applymap(round_if_less_than_20)

# Now, all values less than 20 in the specified columns have been rounded to 0

filtered_df

print(filtered_df.columns)

filtered_df=filtered_df.drop('sum', axis=1)

lists = filtered_df.columns.to_list()[5:]
print(lists)

edges = pd.DataFrame()
for cell in lists:
  tmp =filtered_df[['interacting_pair',cell]]
  tmp.rename(columns={0:'interacting_pair',cell:'value'})
  tmp.rename(columns={cell: 'value'}, inplace=True)
  tmp['pair']=cell
  edges = pd.concat([edges, tmp], axis=0)

edges

edges=edges[edges['value']>0]
print(edges)

edges[['gene', 'receptor']] = edges['interacting_pair'].str.split('_', n=1, expand=True)
print(edges)

edges[['source', 'target']] = edges['pair'].str.split('|', expand=True)
print(edges)

edges['gene_source']=edges['gene']+'_'+edges['source']
edges['receptor_target']=edges['receptor']+'_'+edges['target']
edges

input=edges.copy()

edges = edges[['gene_source', 'receptor_target', 'value']]
edges.rename(columns={'gene_source': 'source'}, inplace=True)
edges.rename(columns={'receptor_target': 'target'}, inplace=True)

print(edges)

# Splitting and swapping for 'source' column
edges['source'] = edges['source'].apply(lambda x: '_'.join(x.split('_')[::-1]))

# Splitting and swapping for 'target' column
edges['target'] = edges['target'].apply(lambda x: '_'.join(x.split('_')[::-1]))

print(edges)

pivot_df = edges.pivot(index='target', columns='source', values='value').fillna(0)

print(pivot_df)

print(pivot_df.columns)

pivot_df

import pandas as pd

# Assuming your DataFrame is named df

# For column names
pivot_df.columns = [col.rsplit('_', 1)[-1] if '_' in col else col for col in pivot_df.columns]

# For row index (if your DataFrame rows have names like the columns)
pivot_df.index = [idx.rsplit('_', 1)[-1] if '_' in idx else idx for idx in pivot_df.index]

# Now df has both row and column names updated

pivot_df

import pandas as pd
import string

# Assuming df is your DataFrame

# Function to append suffixes to duplicates
def append_suffix_to_duplicates(names):
    seen = {}
    updated_names = []
    for name in names:
        if name in seen:
            seen[name] += 1
            new_name = f"{name} {string.ascii_lowercase[seen[name]-1]}"
        else:
            seen[name] = 0
            new_name = name if seen[name] == 0 else f"{name} {string.ascii_lowercase[seen[name]-1]}"
        updated_names.append(new_name)
    return updated_names

# Update column names
pivot_df.columns = append_suffix_to_duplicates(pivot_df.columns)

# Update row names if necessary
pivot_df.index = append_suffix_to_duplicates(pivot_df.index)
pivot_df
# Now df has unique row and column names

pivot_df.to_csv('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Immune_TLO/Penumbra_TLO/TLO_only_immune/input_for_chord_plot.csv', index=True)