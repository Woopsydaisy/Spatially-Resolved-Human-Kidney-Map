# -*- coding: utf-8 -*-
"""Neighborhood generation iTAL Subcluster and figure kmeans.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TwD3C0G-0bCqGksifJ6BjQL4MXDWAGoW
"""

!pip --quiet install scanpy
!pip --quiet install leidenalg
#!pip --quiet install squidpy

from google.colab import drive
drive.mount('/content/drive')

import csv
import anndata as ad
import gzip
import os
import scipy.io
import scanpy as sc
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from google.colab import drive
import leidenalg as la
from pathlib import Path
#import squidpy as sq

adata = sc.read_h5ad('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/all_Cosmx_cleaned_neighbored.h5ad')

adata_all=sc.read_h5ad('/content/drive/MyDrive/Bernhard/Cosmx/SCVI/All_Runs/allCosmx_allGenes.h5ad')

"""lets try again with clustering the dataframe with neighborfractions"""

new_adata=sc.read_h5ad('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/neighbors_as_genes_all_cosmx.h5ad')

new_adata.obs['iPT_subcluster_ME']=adata.obs['iPT_subcluster_ME'].copy()
new_adata.obs['iTAL_subcluster_ME']=adata.obs['iTAL_subcluster_ME'].copy()

new_adata_subset=new_adata[new_adata.obs['total_neighbors_200']>50]
new_adata_subset

new_adata_subset=new_adata_subset[new_adata_subset.obs['annotation_post_scanvi70_broad']=='iTAL']
new_adata_subset

print(new_adata_subset.obs['annotation_post_scanvi70_broad'].value_counts())

import pandas as pd

# Convert the sparse matrix to a dense format and create a DataFrame
gene_expression_df = pd.DataFrame(
    new_adata_subset.X.toarray(),
    index=new_adata_subset.obs_names,
    columns=new_adata_subset.var_names
)

# Print the first few rows to verify
print(gene_expression_df.head())

# List of .obs columns you want to add to the DataFrame
obs_columns = [
    'total_neighbors_80', 'total_neighbors_60', 'total_neighbors_40', 'total_neighbors_20'
]

# Add the selected .obs columns to the gene expression DataFrame
gene_expression_df[obs_columns] = new_adata_subset.obs[obs_columns]

# Print the first few rows to verify the updated DataFrame
print(gene_expression_df.head())

# Loop over each suffix and corresponding total_neighbors column
for neighbors in [80, 60, 40, 20]:
    # Generate the suffix and obs column name
    suffix = f'_{neighbors}'
    obs_column = f'total_neighbors_{neighbors}'

    # Get all columns that end with the current suffix
    matching_columns = [col for col in gene_expression_df.columns if col.endswith(suffix)]

    # Perform the division for each matching column
    for col in matching_columns:
        gene_expression_df[col] /= gene_expression_df[obs_column]

# Print the first few rows to verify the updated DataFrame
print(gene_expression_df.head())

gene_expression_df = gene_expression_df.drop(obs_columns, axis=1)

# Print the first few rows to verify that columns are removed
print(gene_expression_df.head())

import numpy as np

# Replace infinite values with 0
gene_expression_df.replace([np.inf, -np.inf], 0, inplace=True)

# Verify changes by displaying the first few rows of the DataFrame
print(gene_expression_df.head())

gene_expression_df=gene_expression_df.fillna(0)

delete_suffixes = ['200', '180', '160', '140', '120', '100']

# Filter out columns that end with the specified suffixes
columns_to_keep = [col for col in gene_expression_df.columns if not any(col.endswith(suffix) for suffix in delete_suffixes)]
filtered_df = gene_expression_df[columns_to_keep]
print(filtered_df.shape)

filtered_df

from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

scaler = StandardScaler()
features_normalized = scaler.fit_transform(filtered_df)

pca = PCA()
principal_components = pca.fit_transform(features_normalized)

# Create a scree plot
explained_variance = pca.explained_variance_ratio_
cumulative_variance = np.cumsum(explained_variance)

plt.figure(figsize=(10, 6))
plt.plot(range(1, len(explained_variance) + 1), explained_variance, marker='o', linestyle='--', label='Individual explained variance')
plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o', label='Cumulative explained variance')
plt.axhline(y=0.95, color='r', linestyle='--', label='95% explained variance')
plt.title('Scree Plot')
plt.xlabel('Principal Component')
plt.ylabel('Explained Variance Ratio')
plt.legend()
plt.grid()
plt.show()

import pandas as pd
import numpy as np
from sklearn.cluster import MiniBatchKMeans, AgglomerativeClustering
from scipy.cluster.hierarchy import dendrogram, linkage
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
import umap.umap_ as umap
from sklearn.decomposition import PCA

# Assuming your dataframe is already loaded as gene_expression_df
# Normalize the data
scaler = StandardScaler()
features_normalized = scaler.fit_transform(filtered_df)

pca = PCA(n_components=25)  # Reduce to 10 dimensions for UMAP input
principal_components = pca.fit_transform(features_normalized)

from sklearn.cluster import MiniBatchKMeans
from sklearn.metrics import silhouette_score

inertias = []
k = range(1, 10)

# dissimilarity would not be defined for a single cluster, thus, minimum number of clusters should be 2
for K in k:
    # Create a MiniBatchKMeans object, arbitrarily choosing 10 clusters
    kmeans = MiniBatchKMeans(n_clusters=K)
    # Fit the model to the data
    kmeans.fit(principal_components)
    inertias.append(kmeans.inertia_)

plt.plot(k, inertias, 'bx-')
plt.xlabel('Values of K')
plt.ylabel('Inertia')
plt.title('The Elbow Method using Inertia')
plt.show()

# Perform MiniBatchKMeans clustering
kmeans = MiniBatchKMeans(n_clusters=4, random_state=42)
labels = kmeans.fit_predict(principal_components)
filtered_df['cluster_labels'] = labels

new_adata_subset.obs['cluster_labels']=filtered_df['cluster_labels'].copy()

print(new_adata_subset.obs['cluster_labels'])

new_adata_subset.obs['cluster_labels']=new_adata_subset.obs['cluster_labels'].astype('category')

new_adata_subset

from scipy import sparse
sparse_matrix = scipy.sparse.csr_matrix(gene_expression_df.values)
new_adata_subset.X=sparse_matrix

sc.tl.rank_genes_groups(new_adata_subset, groupby='cluster_labels', method='wilcoxon', pts = True)
sc.pl.rank_genes_groups(new_adata_subset, n_genes=25, sharey=False)

new_adata_subset.obs["cluster_labels"].value_counts()
cell_identities = {0: 'iTAL ME 1', 1: 'iTAL ME 2', 2: 'iTAL ME 3', 3: 'iTAL ME 4'}
new_adata_subset.obs["cluster_labels_annotated_iTAL"] = new_adata_subset.obs['cluster_labels'].map(cell_identities).astype('category')

de_results = new_adata_subset.uns['rank_genes_groups']
for group in de_results['names'].dtype.names:
        gene_names = de_results['names'][group]
        gene_scores = de_results['scores'][group]
        gene_logfoldchanges = de_results['logfoldchanges'][group]
        gene_pvals = de_results['pvals'][group]
        gene_pvals_adj = de_results['pvals_adj'][group]

        # Create a DataFrame
        df = pd.DataFrame({
            'Gene': gene_names,
            'Score': gene_scores,
            'LogFoldChange': gene_logfoldchanges,
            'pValue': gene_pvals,
            'pValue_adj': gene_pvals_adj
        })
        df=df[df['LogFoldChange']>0]
        df=df[df['pValue_adj']<0.01]
        df = df[~df['Gene'].str.endswith('_200')]
        df = df[~df['Gene'].str.endswith('_180')]
        df = df[~df['Gene'].str.endswith('_160')]
        df = df[~df['Gene'].str.endswith('_140')]
        df = df[~df['Gene'].str.endswith('_120')]
        df = df[~df['Gene'].str.endswith('_100')]
        df = df[~df['Gene'].str.endswith('_80')]

        print(df.head(40))

markers = [ "PT_20","TAL_20","Podo_20", "Immune_20","Fibroblast_20","iPT_20","iTAL_20", "CNT_20", ]

sc.pl.dotplot(new_adata_subset, markers, groupby='cluster_labels_annotated_iTAL',cmap='Blues', log = False)

import scanpy as sc
import matplotlib.pyplot as plt

# Define markers
markers = ["PC_20","TAL_20","PT_20","Immune_20","Fibroblast_20", "iTAL_20", ]

# Create the dot plot
dotplot = sc.pl.dotplot(new_adata_subset, markers, groupby='cluster_labels_annotated_iTAL',
                        cmap='Blues', log=False, show=False, swap_axes=True, dot_max=1, vmax=0.45)

# Get the main plot axis
ax = dotplot['mainplot_ax']

# Rotate the x and y labels
ax.set_xlabel('')
ax.set_ylabel('')
ax.set_xticklabels(ax.get_xticklabels(), rotation=90)
ax.set_yticklabels(ax.get_yticklabels(), rotation=0)

# Remove the little ticks but keep the labels
ax.tick_params(axis='x', which='both', bottom=False, top=False)
ax.tick_params(axis='y', which='both', left=False, right=False)

# Invert the y-axis to flip the plot
ax.invert_yaxis()

# Adjust the plot layout
plt.tight_layout()
plt.savefig('iTAL MEs 20um.png', dpi=450, bbox_inches='tight')
# Show the plot
plt.show()

import scanpy as sc
import matplotlib.pyplot as plt

# Define markers
markers = ["PC_40","TAL_40","PT_40","Immune_40","Fibroblast_40", "iTAL_40"]

# Create the dot plot
dotplot = sc.pl.dotplot(new_adata_subset, markers, groupby='cluster_labels_annotated_iTAL',
                        cmap='Blues', log=False, show=False, swap_axes=True, dot_max=1, vmax=0.45)

# Get the main plot axis
ax = dotplot['mainplot_ax']

# Rotate the x and y labels
ax.set_xlabel('')
ax.set_ylabel('')
ax.set_xticklabels(ax.get_xticklabels(), rotation=90)
ax.set_yticklabels(ax.get_yticklabels(), rotation=0)

# Remove the little ticks but keep the labels
ax.tick_params(axis='x', which='both', bottom=False, top=False)
ax.tick_params(axis='y', which='both', left=False, right=False)

# Invert the y-axis to flip the plot
ax.invert_yaxis()

# Adjust the plot layout
plt.tight_layout()
plt.savefig('iTAL MEs 40um.png', dpi=450, bbox_inches='tight')
# Show the plot
plt.show()

import scanpy as sc
import matplotlib.pyplot as plt

# Define markers
markers = ["PC_60","TAL_60","PT_60","Immune_60","Fibroblast_60", "iTAL_60"]

# Create the dot plot
dotplot = sc.pl.dotplot(new_adata_subset, markers, groupby='cluster_labels_annotated_iTAL',
                        cmap='Blues', log=False, show=False, swap_axes=True, dot_max=1, vmax=0.45)

# Get the main plot axis
ax = dotplot['mainplot_ax']

# Rotate the x and y labels
ax.set_xlabel('')
ax.set_ylabel('')
ax.set_xticklabels(ax.get_xticklabels(), rotation=90)
ax.set_yticklabels(ax.get_yticklabels(), rotation=0)

# Remove the little ticks but keep the labels
ax.tick_params(axis='x', which='both', bottom=False, top=False)
ax.tick_params(axis='y', which='both', left=False, right=False)

# Invert the y-axis to flip the plot
ax.invert_yaxis()

# Adjust the plot layout
plt.tight_layout()
plt.savefig('iTAL MEs 60um.png', dpi=450, bbox_inches='tight')
# Show the plot
plt.show()

"""Lets make pie charts"""

delete_suffixes = ['200', '180', '160', '140', '120', '100', '80', '60', '40']

# Filter out columns that end with the specified suffixes
columns_to_keep = [col for col in gene_expression_df.columns if not any(col.endswith(suffix) for suffix in delete_suffixes)]
filtered_df = gene_expression_df[columns_to_keep]
print(filtered_df.shape)

new_adata_subset

filtered_df['cluster_labels']=new_adata_subset.obs['iTAL_subcluster_ME'].copy()

cell_colors = {
    'PC': (0.12156862745098039, 0.4666666666666667, 0.7058823529411765),
    'CNT': (0.6823529411764706, 0.7803921568627451, 0.9098039215686274),
    'DCT': (1.0, 0.4980392156862745, 0.054901960784313725),
    'DTL_ATL': (1.0, 0.7333333333333333, 0.47058823529411764),
    'EC_DVR': (0.17254901960784313, 0.6274509803921569, 0.17254901960784313),
    'EC_Peritub': (0.596078431372549, 0.8745098039215686, 0.5411764705882353),
    'EC_glom': (0.8392156862745098, 0.15294117647058825, 0.1568627450980392),
    'IC A': (1.0, 0.596078431372549, 0.5882352941176471),
    'IC B': (0.5803921568627451, 0.403921568627451, 0.7411764705882353),
    'Immune': (0.7725490196078432, 0.6901960784313725, 0.8352941176470589),
    'Podo': (0.5490196078431373, 0.33725490196078434, 0.29411764705882354),
    'Fibroblast': (0.7686274509803922, 0.611764705882353, 0.5803921568627451),
    'PEC': (0.8901960784313725, 0.4666666666666667, 0.7607843137254902),
    'PT': (0.9686274509803922, 0.7137254901960784, 0.8235294117647058),
    'MC1': (0.4980392156862745, 0.4980392156862745, 0.4980392156862745),
    'iPT': (0.7803921568627451, 0.7803921568627451, 0.7803921568627451),
    'iTAL': (0.8588235294117647, 0.8588235294117647, 0.5529411764705883),
    'TAL': (0.09019607843137255, 0.7450980392156863, 0.8117647058823529),
    'VSMC': (0.6196078431372549, 0.8549019607843137, 0.8980392156862745),
    'CD4+': (0.5529411764705883, 0.8274509803921568, 0.7803921568627451),
    'Baso/Mast': (0.2235, 0.2314, 0.4745),
    'B': (0.7450980392156863, 0.7294117647058823, 0.8549019607843137),
    'CD8+ I': (0.6784, 0.2863, 0.2902),
    'Macro II': (0.9921568627450981, 0.7058823529411765, 0.3843137254901961),
    'Neutrophil': (0.9882352941176471, 0.803921568627451, 0.8980392156862745),
    'Macro I': (0.3, 0.3, 0.3),
    'NK': (0.7372549019607844, 0.5019607843137255, 0.7411764705882353),
    'cDC': (0.9176470588235294, 0.5019607843137255, 0.23529411764705882),
    'mDC': (0.34509803921568627, 0.33725490196078434, 0.6745098039215687),
    'pDC': (0.6509803921568628, 0.4627450980392157, 0.11372549019607843),
    'Macro III': (0.9294117647058824, 0.6941176470588235, 0.12549019607843137),
    'Macro IV': (0.41568627450980394, 0.23921568627450981, 0.6039215686274509),
    'cycling Lymphocytes': (0.8, 0.47058823529411764, 0.7372549019607844),
    'CD8+ II': (0.9921568627450981, 0.7529411764705882, 0.5254901960784314),
    'Plasma': (0.2196078431372549, 0.4235294117647059, 0.6901960784313725),
    'other':(0.5,0.5,0.5,0.5)
    #'cytotox B': (0.4, 0.7607843137254902, 0.6470588235294118)
    }

import pandas as pd
import matplotlib.pyplot as plt

# Assuming filtered_df is your dataframe
# Step 1: Filter columns that end with '_20'
columns_to_keep = [col for col in filtered_df.columns if col.endswith('_20')]
columns_to_keep.append('cluster_labels')  # Make sure to keep the 'cluster_labels' column

filtered_40_df = filtered_df[columns_to_keep]

# Step 2: Rename the columns to remove the '_20' suffix
filtered_40_df.columns = [col[:-3] if col.endswith('_20') else col for col in filtered_40_df.columns]

# Step 3: Group by 'cluster_labels' and sum the values for each group
grouped_df = filtered_40_df.groupby('cluster_labels').sum()

# Step 4: Normalize the grouped values so that each pie chart equals 1
normalized_df = grouped_df.div(grouped_df.sum(axis=1), axis=0)

# Map the colors to the columns
colors = [cell_colors.get(col, 'grey') for col in normalized_df.columns]

# Step 5: Plot a pie chart for each cluster label
for label in normalized_df.index:
    print(label)
    plt.figure(figsize=(8, 8))
    plt.pie(normalized_df.loc[label],  colors=colors, autopct=None) ##labels=normalized_df.columns,
    plt.title('')

    # Remove the frame around the plot
    plt.gca().spines['top'].set_visible(False)
    plt.gca().spines['right'].set_visible(False)
    plt.gca().spines['left'].set_visible(False)
    plt.gca().spines['bottom'].set_visible(False)

    # Save the figure
    plt.savefig(f'iTAL_Cluster_20um_pie_chart_{label}.png', dpi=450, bbox_inches='tight')
    plt.show()

palette={
    'iPT ME 1': '#A7C8EB',
    'iPT ME 2': '#8CC47A',
    'iPT ME 3': '#C9AFD5',
    'iPT ME 4': '#F18C93',
    'iTAL ME 1': '#F0B836',
    'iTAL ME 2': '#007ABA',
    'iTAL ME 3':'#8F65A8',
    'iTAL ME 4': '#D975AC',
    'outlier':'#A8A8A7'
}##'#92D2DF',

import matplotlib.pyplot as plt
import seaborn as sns

# Assuming new_adata_subset is your AnnData object
# Extract the relevant columns
samples = new_adata_subset.obs['sample']
cluster_labels = new_adata_subset.obs['iTAL_subcluster_ME']

# Create a DataFrame from the relevant columns
data = pd.DataFrame({'sample': samples, 'iTAL_subcluster_ME': cluster_labels})

# Group by 'sample' and 'cluster_labels_annotated' and count the occurrences
grouped_data = data.groupby(['sample', 'iTAL_subcluster_ME']).size().unstack(fill_value=0)
desired_order = [
    "Pediatric1", "HK2753", "HK3039", "HK3106", "HK3531", "HK3531_2",
    "HK3063", "HK3542", "HK2874", "HK2695", "HK3035", "HK3035_2",
     "HK2841", "HK2873","HK2844", "HK2844_2"
]

grouped_data=grouped_data.reindex(desired_order)
sample_totals = data['sample'].value_counts()
sample_totals=sample_totals.reindex(desired_order)
normalized_count_df = grouped_data.div(sample_totals, axis='index')
colors = [palette[col] for col in normalized_count_df.columns]
ax = normalized_count_df.plot(kind='bar', stacked=True, figsize=(6, 7), color=colors)

# Set plot labels and title
plt.xlabel('')
plt.ylabel('Relative Amount')
plt.title('')
plt.legend(title='', bbox_to_anchor=(0.5, 0.95), loc='lower center',ncol=4, frameon=False)
plt.tick_params(axis='x', which='both', length=0)
ax.spines['top'].set_visible(False)
ax.spines['right'].set_visible(False)
plt.tight_layout()
plt.savefig('iTAL relative niche amount longer.png', dpi=450, bbox_inches='tight')
plt.show()

dotplot

import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.preprocessing import MinMaxScaler

# Assuming new_adata_subset is your AnnData object and it has the annotations and cluster labels
# Extract the relevant columns
annotations = new_adata_subset.obs["annotation_post_scanvi70_broad"]
cluster_labels = new_adata_subset.obs["cluster_labels"]

# Create a contingency table (cross-tabulation)
contingency_table = pd.crosstab(cluster_labels, annotations)

# Normalize the data across the columns (annotations)
normalized_table = contingency_table.div(contingency_table.sum(axis=0), axis=1)

# Scale the data from 0 to 1 across the columns
scaler = MinMaxScaler()
scaled_table = pd.DataFrame(scaler.fit_transform(normalized_table),
                            index=normalized_table.index,
                            columns=normalized_table.columns)
#desired_column_order = ['PT','EC_Peritub','iPT','TAL','iTAL','DTL_ATL','CNT', 'DCT','IC A', 'IC B', 'PC', 'EC_DVR', 'VSMC',
 #                 'Fibroblast',  'Immune',   'EC_glom','PEC',
  #                  'Podo',  'MC1']  # Replace with actual annotation labels
#desired_index_order = ['PT Neighborhood 1','iPT Neighborhood','TAL Neighborhood', 'iTAL Neighborhood', 'DTL_ATL Neighborhood',
 #                      'CNT Neighborhood 1','DCT Neighborhood','IC Neighborhood','PC Neighborhood', 'Vascular Neighborhood',
  #                'Vascular Neighborhood 2','Immune Neighborhood',
   #                'Fibroblast Neighborhood','Glomerular Neighborhood','Glomerular Neighborhood 2',



    #               ]  # Replace with actual cluster labels
#scaled_table = scaled_table.reindex(index=desired_index_order, columns=desired_column_order)
# Plot the heatmap
plt.figure(figsize=(3, 8))
sns.heatmap(scaled_table.T, annot=False, fmt=".2f", cmap="Blues")
plt.title('Kidney Niches')
plt.xlabel('Niche Labels')
plt.ylabel('Cell Types')
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.preprocessing import MinMaxScaler

# Assuming new_adata_subset is your AnnData object and it has the annotations and cluster labels
# Extract the relevant columns
annotations = new_adata_subset.obs["annotation_post_scanvi70_broad"]
cluster_labels = new_adata_subset.obs["cluster_labels_annotated"]

# Create a contingency table (cross-tabulation)
contingency_table = pd.crosstab(cluster_labels, annotations)

# Normalize the data across the columns (annotations)
normalized_table = contingency_table.div(contingency_table.sum(axis=0), axis=1)

# Scale the data from 0 to 1 across the columns
scaler = MinMaxScaler()
scaled_table = pd.DataFrame(scaler.fit_transform(normalized_table),
                            index=normalized_table.index,
                            columns=normalized_table.columns)
desired_column_order = ['PT','EC_Peritub','iPT','TAL','iTAL','DTL_ATL','CNT', 'DCT','IC A', 'IC B', 'PC', 'EC_DVR', 'VSMC',
                  'Fibroblast',  'Immune',   'EC_glom','PEC',
                    'Podo',  'MC1']  # Replace with actual annotation labels
desired_index_order = ['PT Neighborhood 1','iPT Neighborhood','TAL Neighborhood', 'iTAL Neighborhood', 'DTL_ATL Neighborhood',
                       'CNT Neighborhood 1','DCT Neighborhood','IC Neighborhood','PC Neighborhood', 'Vascular Neighborhood',
                  'Vascular Neighborhood 2','Immune Neighborhood',
                   'Fibroblast Neighborhood','Glomerular Neighborhood','Glomerular Neighborhood 2',



                   ]  # Replace with actual cluster labels
scaled_table = scaled_table.reindex(index=desired_index_order, columns=desired_column_order)
# Plot the heatmap
plt.figure(figsize=(5, 8))
sns.heatmap(scaled_table.T, annot=False, fmt=".2f", cmap="Blues")
plt.title('Kidney Niches')
plt.xlabel('Niche Labels')
plt.ylabel('Cell Types')
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# Assuming new_adata_subset is your AnnData object
# Extract the relevant columns
samples = new_adata_subset.obs['sample']
cluster_labels = new_adata_subset.obs['cluster_labels_annotated_iTAL']

# Create a DataFrame from the relevant columns
data = pd.DataFrame({'sample': samples, 'cluster_labels_annotated_iTAL': cluster_labels})

# Group by 'sample' and 'cluster_labels_annotated' and count the occurrences
grouped_data = data.groupby(['sample', 'cluster_labels_annotated_iTAL']).size().unstack(fill_value=0)
desired_order = [
    "Pediatric1", "HK2753", "HK3039", "HK3106", "HK3531", "HK3531_2",
    "HK3063", "HK3542", "HK2874", "HK2695", "HK3035", "HK3035_2",
     "HK2841", "HK2873","HK2844", "HK2844_2"
]

grouped_data=grouped_data.reindex(desired_order)

ax = grouped_data.plot(kind='bar', stacked=True, figsize=(8, 8), colormap='tab20')

# Set plot labels and title
plt.xlabel('')
plt.ylabel('Relative Amount')
plt.title('Sample Niche Composition')
plt.legend(title='Niche Labels', bbox_to_anchor=(1.05, 1), loc='upper left', frameon=False)
plt.tick_params(axis='x', which='both', length=0)
ax.spines['top'].set_visible(False)
ax.spines['right'].set_visible(False)
plt.tight_layout()
plt.savefig('iTAL total ME abundance.png', dpi=450, bbox_inches='tight')
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# Assuming new_adata_subset is your AnnData object
# Extract the relevant columns
samples = new_adata_subset.obs['sample']
cluster_labels = new_adata_subset.obs['cluster_labels_annotated_iTAL']

# Create a DataFrame from the relevant columns
data = pd.DataFrame({'sample': samples, 'cluster_labels_annotated_iTAL': cluster_labels})

# Group by 'sample' and 'cluster_labels_annotated' and count the occurrences
grouped_data = data.groupby(['sample', 'cluster_labels_annotated_iTAL']).size().unstack(fill_value=0)
desired_order = [
    "Pediatric1", "HK2753", "HK3039", "HK3106", "HK3531", "HK3531_2",
    "HK3063", "HK3542", "HK2874", "HK2695", "HK3035", "HK3035_2",
     "HK2841", "HK2873","HK2844", "HK2844_2"
]

grouped_data=grouped_data.reindex(desired_order)
sample_totals = data['sample'].value_counts()
sample_totals=sample_totals.reindex(desired_order)
normalized_count_df = grouped_data.div(sample_totals, axis='index')
ax = normalized_count_df.plot(kind='bar', stacked=True, figsize=(8, 8), colormap='tab20')

# Set plot labels and title
plt.xlabel('')
plt.ylabel('Relative Amount')
plt.title('Sample Niche Composition')
plt.legend(title='Niche Labels', bbox_to_anchor=(1.05, 1), loc='upper left', frameon=False)
plt.tick_params(axis='x', which='both', length=0)
ax.spines['top'].set_visible(False)
ax.spines['right'].set_visible(False)
plt.tight_layout()
plt.savefig('iTAL relative niche amount.png', dpi=450, bbox_inches='tight')
plt.show()

adata

palette={'iPT ME 1':'#9CD0E6',
         'iPT ME 2':'#A7CD7E',
         'iPT ME 3':'#F19096',
         'iPT ME 4':'#F9B865',
         'iPT ME 5':'#E1C9E3',
         'outlier':'#d3d3d3',}

sc.pl.umap(adata, color = "iPT_subcluster_ME", palette=palette, frameon =False)

iPT=adata[adata.obs['iPT_subcluster_ME']!='outlier']
sc.pl.umap(iPT, color = "iPT_subcluster_ME")

sc.pl.umap(iPT, color = "iPT_subcluster_ME", groups='iPT ME 1')

sc.pl.umap(iPT, color = "iPT_subcluster_ME", groups='iPT ME 4')

new_adata_subset

adata.obs['iTAL_subcluster_ME']=new_adata_subset.obs['cluster_labels_annotated_iTAL'].copy()
adata_all.obs['iTAL_subcluster_ME']=new_adata_subset.obs['cluster_labels_annotated_iTAL'].copy()

adata.obs['iTAL_subcluster_ME'] = adata.obs['iTAL_subcluster_ME'].cat.add_categories(['outlier'])
adata.obs["iTAL_subcluster_ME"]=adata.obs["iTAL_subcluster_ME"].fillna('outlier')

adata_all.obs['iTAL_subcluster_ME'] = adata_all.obs['iTAL_subcluster_ME'].cat.add_categories(['outlier'])
adata_all.obs["iTAL_subcluster_ME"]=adata_all.obs["iTAL_subcluster_ME"].fillna('outlier')

print(adata_all.obs['iTAL_subcluster_ME'])

print(adata.obs['iTAL_subcluster_ME'])



adata.write('/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/all_Cosmx_cleaned_neighbored.h5ad')

adata_all.write('/content/drive/MyDrive/Bernhard/Cosmx/SCVI/All_Runs/allCosmx_allGenes.h5ad')

iTAL_subset=adata_all[adata_all.obs['iTAL_subcluster_ME']!='outlier']

iTAL_subset.obs['iTAL_subcluster_ME'].value_counts()

iTAL_subset.X = iTAL_subset.layers["counts"]
sc.pp.normalize_total(iTAL_subset, inplace=True)
sc.pp.log1p(iTAL_subset)
sc.tl.rank_genes_groups(iTAL_subset, groupby='iTAL_subcluster_ME', method='wilcoxon', pts = True)
sc.pl.rank_genes_groups(iTAL_subset, n_genes=25, sharey=False)

de_results = iTAL_subset.uns['rank_genes_groups']
for group in de_results['names'].dtype.names:
        gene_names = de_results['names'][group]
        gene_scores = de_results['scores'][group]
        gene_logfoldchanges = de_results['logfoldchanges'][group]
        gene_pvals = de_results['pvals'][group]
        gene_pvals_adj = de_results['pvals_adj'][group]

        # Create a DataFrame
        df = pd.DataFrame({
            'Gene': gene_names,
            'Score': gene_scores,
            'LogFoldChange': gene_logfoldchanges,
            'pValue': gene_pvals,
            'pValue_adj': gene_pvals_adj
        })
        df=df[df['pValue_adj']<0.01]
        df=df[df['LogFoldChange']>0.3]
        print(group)
        print(df.head(55))
        #df.to_csv(f'/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/Spatial_Neighborhoods/Niches/Glomerular_{group}_DEGs.csv')

import pandas as pd
from scipy.sparse import issparse
# Assuming 'adata' is your AnnData object
markers = ["MMP7","IFITM3", "SPP1", "ANXA2", "SOX4", "SOX9", "HIF1A", "ACSL4", "IL32", "CCL2", "TNFRSF12A", "ADIRF", "CRYAB", "UMOD", "JAG1", "KRT19"
           ]

# Ensure markers list only includes valid gene names
markers = [gene for gene in markers if gene in iTAL_subset.var_names]

# Subset the .X matrix for the markers and convert to a DataFrame
gene_expression_df_unimputed = pd.DataFrame(
    iTAL_subset[:, markers].X.toarray() if issparse(iTAL_subset.X) else iTAL_subset[:, markers].X,
    index=iTAL_subset.obs_names,
    columns=markers
)

gene_expression_df_unimputed['iTAL_subcluster_ME'] = iTAL_subset.obs['iTAL_subcluster_ME'].values
mean_expression_per_cluster_unimputed = gene_expression_df_unimputed.groupby('iTAL_subcluster_ME').mean()

from sklearn.preprocessing import MinMaxScaler

# Create a MinMaxScaler object
scaler = MinMaxScaler()

# Scale each column individually
gene_expression_scaled_unimputed = scaler.fit_transform(mean_expression_per_cluster_unimputed)

# Convert the scaled array back to a DataFrame
gene_expression_scaled_df_unimputed = pd.DataFrame(gene_expression_scaled_unimputed, index=mean_expression_per_cluster_unimputed.index, columns=mean_expression_per_cluster_unimputed.columns)

# Display the first few rows of the scaled dataframe
#print(gene_expression_scaled_df_unimputed)

import matplotlib.pyplot as plt
import seaborn as sns

# Assuming 'gene_expression_scaled_df_unimputed' is your DataFrame and it's correctly formatted
# The '.T' is transposing the DataFrame, which is typical in heatmaps for better visualization of genes on one axis

plt.figure(figsize=(3, 8))  # Adjust the figure size as needed
heatmap = sns.heatmap(gene_expression_scaled_df_unimputed.T, cmap='plasma', vmax=1, cbar_kws={"shrink": .3})

# Adjust the size of the colorbar
heatmap.collections[0].colorbar.ax.set_ylabel('')
heatmap.collections[0].colorbar.ax.set_xlabel('')
# Removing the tick marks as requested (optional)
plt.tick_params(axis='both', which='both', length=0)  # Ensures no tick marks on both x and y axes

# Save the figure
plt.savefig('heatmap_iTAL_ME.png', dpi=450, bbox_inches='tight')

# Show the plot
plt.show()

df = pd.DataFrame({
    'cluster_labels': iTAL_subset.obs['iTAL_subcluster_ME'],
    'type': iTAL_subset.obs['type'],
    'sample':iTAL_subset.obs['sample']
}, index=iTAL_subset.obs_names)
df

df.to_csv('iTAL_ME.csv', index=True)

df=df[df['cluster_labels']!='outlier']

df['type']=df['type'].replace('Pediatric', 'Healthy')

df['type'].value_counts()

df=df[df['type']!='Diabetes']
df['sample'].value_counts()

df=df[df['sample']!='HK2874']

df['sample'].value_counts()

df

df=df.drop('sample', axis=1)

df['type'] = df['type'].cat.remove_unused_categories()

df['cluster_labels'] = df['cluster_labels'].cat.remove_unused_categories()

df

# Step 1: Group by 'type' and 'cluster_labels' and count the occurrences
grouped_counts = df.groupby(['type', 'cluster_labels']).size().reset_index(name='count')

# Step 2: Calculate the total number of 'cluster_labels' for each 'type'
total_counts = df.groupby('type').size().reset_index(name='total_count')

# Step 3: Merge the total counts back to the grouped counts
merged_counts = pd.merge(grouped_counts, total_counts, on='type')

# Step 4: Normalize the counts by the total counts for each 'type'
merged_counts['normalized_count'] = merged_counts['count'] / merged_counts['total_count']

# Step 5: Display the result
print(merged_counts)

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming 'merged_counts' is your DataFrame
# Split the DataFrame into Healthy and Disease
healthy_df = merged_counts[merged_counts['type'] == 'Healthy']
disease_df = merged_counts[merged_counts['type'] == 'Disease']

# Merge Healthy and Disease dataframes on 'cluster_labels'
merged = pd.merge(healthy_df, disease_df, on='cluster_labels', suffixes=('_healthy', '_disease'))

# Calculate the relative increase
merged['relative_increase'] = (merged['normalized_count_disease'] - merged['normalized_count_healthy']) / merged['normalized_count_healthy']
heatmap_data = merged.set_index('cluster_labels')[['relative_increase']]
heatmap_data_rounded = heatmap_data.round(0).astype(int)
# Plotting the heatmap
plt.figure(figsize=(1, 4))
ax = sns.heatmap(heatmap_data_rounded, annot=True, cmap='magma',fmt='d',cbar=False, vmax=3)
ax.set_xticks([])
ax.tick_params(axis='y', which='both', left=False, right=False)
# Add labels and title
plt.xlabel('')
plt.ylabel('')
plt.title('Increase in DKD')

# Save the plot
plt.tight_layout()
plt.savefig('relative_increase_iTAL_heatmap.png', dpi=450)
plt.show()



mean_expression_per_cluster_unimputed.shape

# Scale the aggregated data
scaler = StandardScaler()
aggregated_normalized = scaler.fit_transform(mean_expression_per_cluster_unimputed)

# Compute the linkage matrix using the Ward method
linkage_matrix = linkage(aggregated_normalized, method='ward')

# Plot the dendrogram
plt.figure(figsize=(15, 10))
dendrogram(linkage_matrix, labels=mean_expression_per_cluster_unimputed.index.astype(str).tolist(), leaf_rotation=90, leaf_font_size=10)
plt.title('Hierarchical Clustering Dendrogram (Ward)')
plt.xlabel('Cluster label')
plt.ylabel('Distance')
plt.show()



import matplotlib.pyplot as plt
import seaborn as sns

# Assuming new_adata_subset is your AnnData object
# Extract the relevant columns
samples = new_adata_subset.obs['type']
cluster_labels = new_adata_subset.obs['cluster_labels_annotated_iPT']

# Create a DataFrame from the relevant columns
data = pd.DataFrame({'type': samples, 'cluster_labels_annotated_iPT': cluster_labels})

# Group by 'sample' and 'cluster_labels_annotated' and count the occurrences
grouped_data = data.groupby(['type', 'cluster_labels_annotated_iPT']).size().unstack(fill_value=0)
#desired_order = [
 #   "Pediatric1", "HK2753", "HK3039", "HK3106", "HK3531", "HK3531_2",
  #  "HK3063", "HK3542", "HK2874", "HK2695", "HK3035", "HK3035_2",
   #  "HK2841", "HK2873","HK2844", "HK2844_2"
#]

#grouped_data=grouped_data.reindex(desired_order)
sample_totals = data['type'].value_counts()
#sample_totals=sample_totals.reindex(desired_order)
normalized_count_df = grouped_data.div(sample_totals, axis='index')
ax = normalized_count_df.plot(kind='bar', stacked=True, figsize=(8, 8), colormap='tab20')

# Set plot labels and title
plt.xlabel('')
plt.ylabel('Relative Amount')
plt.title('Sample Niche Composition')
plt.legend(title='Niche Labels', bbox_to_anchor=(1.05, 1), loc='upper left', frameon=False)
plt.tick_params(axis='x', which='both', length=0)
ax.spines['top'].set_visible(False)
ax.spines['right'].set_visible(False)
plt.tight_layout()
#plt.savefig('iPT relative niche amount.png', dpi=450, bbox_inches='tight')
plt.show()

random_seed = 42

# Sample 1000 rows from each cluster
sampled_df = filtered_df.groupby('cluster_labels').apply(lambda x: x.sample(min(len(x), 1000), random_state=random_seed)).reset_index(drop=True)

# Aggregate the sampled dataframe by cluster_labels
aggregated_df = sampled_df.groupby('cluster_labels').mean()

# Scale the aggregated data
scaler = StandardScaler()
aggregated_normalized = scaler.fit_transform(aggregated_df)

# Compute the linkage matrix using the Ward method
linkage_matrix = linkage(aggregated_normalized, method='ward')

# Plot the dendrogram
plt.figure(figsize=(15, 10))
dendrogram(linkage_matrix, labels=aggregated_df.index.astype(str).tolist(), leaf_rotation=90, leaf_font_size=10)
plt.title('Hierarchical Clustering Dendrogram (Ward)')
plt.xlabel('Cluster label')
plt.ylabel('Distance')
plt.show()

iPT

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Extract the relevant columns from the AnnData object
mean_DAPI = iPT.obs['Mean.DAPI']
iPT_ME = iPT.obs['iPT_subcluster_ME']

# Create a DataFrame from the extracted data
data = pd.DataFrame({'mean_DAPI': mean_DAPI, 'iPT_ME': iPT_ME})

# Create a box plot
plt.figure(figsize=(10, 6))
sns.boxplot(data=data, x='iPT_ME', y='mean_DAPI', palette='Blues',fliersize=0)

# Add labels and title
plt.xlabel('iPT_ME')
plt.ylabel('Mean DAPI')
plt.title('Mean DAPI by iPT_ME')
plt.ylim(0,4500)
# Adjust layout
plt.tight_layout()

# Show the plot
plt.show()

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Extract the relevant columns from the AnnData object
mean_DAPI = iPT.obs['Mean.DAPI']
iPT_ME = iPT.obs['iPT_subcluster_ME']

# Create a DataFrame from the extracted data
data = pd.DataFrame({'mean_DAPI': mean_DAPI, 'iPT_ME': iPT_ME})

# Create a violin plot with inner set to None to turn off the fliers
plt.figure(figsize=(5, 6))
sns.violinplot(data=data, x='iPT_ME', y='mean_DAPI', palette='tab10', inner=None)

# Add labels and title
plt.xlabel('iPT_ME')
plt.ylabel('Mean DAPI')
plt.title('Mean DAPI by iPT_ME')

# Adjust layout
plt.tight_layout()

# Show the plot
plt.show()

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Extract the relevant columns from the AnnData object
mean_DAPI = iPT.obs['Mean.CK8.18']
iPT_ME = iPT.obs['iPT_subcluster_ME']

# Create a DataFrame from the extracted data
data = pd.DataFrame({'mean_DAPI': mean_DAPI, 'iPT_ME': iPT_ME})

# Create a violin plot with inner set to None to turn off the fliers
plt.figure(figsize=(5, 6))
sns.violinplot(data=data, x='iPT_ME', y='mean_DAPI', palette='tab10', inner=None)

# Add labels and title
plt.xlabel('iPT_ME')
plt.ylabel('Mean DAPI')
plt.title('Mean DAPI by iPT_ME')

# Adjust layout
plt.tight_layout()

# Show the plot
plt.show()

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Extract the relevant columns from the AnnData object
mean_DAPI = iPT.obs['Mean.CK8.18']
iPT_ME = iPT.obs['iPT_subcluster_ME']

# Create a DataFrame from the extracted data
data = pd.DataFrame({'mean_DAPI': mean_DAPI, 'iPT_ME': iPT_ME})

# Create a box plot
plt.figure(figsize=(10, 6))
sns.boxplot(data=data, x='iPT_ME', y='mean_DAPI', palette='Blues',fliersize=0)

# Add labels and title
plt.xlabel('iPT_ME')
plt.ylabel('Mean DAPI')
plt.title('Mean DAPI by iPT_ME')
#plt.ylim(0,4500)
# Adjust layout
plt.tight_layout()

# Show the plot
plt.show()

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Extract the relevant columns from the AnnData object
mean_DAPI = iPT.obs['Area']
iPT_ME = iPT.obs['iPT_subcluster_ME']

# Create a DataFrame from the extracted data
data = pd.DataFrame({'mean_DAPI': mean_DAPI, 'iPT_ME': iPT_ME})

# Create a box plot
plt.figure(figsize=(10, 6))
sns.boxplot(data=data, x='iPT_ME', y='mean_DAPI', palette='Blues',fliersize=0)

# Add labels and title
plt.xlabel('iPT_ME')
plt.ylabel('Mean DAPI')
plt.title('Mean DAPI by iPT_ME')
plt.ylim(0,20000)
# Adjust layout
plt.tight_layout()

# Show the plot
plt.show()

"""lets do GSEA with imputed expression"""

!pip --quiet install gseapy

import gseapy
from gseapy import barplot, dotplot

human = gseapy.get_library_name(organism='Human')

v_library='WikiPathway_2023_Human' ## 'WikiPathway_2023_Human' ###
gset = gseapy.parser.get_library(v_library, min_size=20)

iTAL1 = pd.read_csv("/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/iTAL_subclustering/iTAL_ME/iTAL ME 1_DEG.csv")
iTAL2 = pd.read_csv("/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/iTAL_subclustering/iTAL_ME/iTAL ME 2_DEG.csv")
iTAL3 = pd.read_csv("/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/iTAL_subclustering/iTAL_ME/iTAL ME 3_DEG.csv")
iTAL4 = pd.read_csv("/content/drive/MyDrive/Bernhard/Cosmx/After_Cleaning/iTAL_subclustering/iTAL_ME/iTAL ME 4_DEG.csv")

populations_to_test = [iTAL1,iTAL2,iTAL3,iTAL4]
very_small_value = 1e-310

for i in populations_to_test:
    i['pValue_adj'] = i['pValue_adj'].replace(0, very_small_value)
    i["names"] = i['Gene']
    i["pi_score"] = -1 * np.log10(i["pValue_adj"]) * i["LogFoldChange"] # pvalue
    i.dropna(inplace=True)

iTAL1.shape

iPT1=iPT1[iPT1['pValue_adj']<0.01]
iPT2=iPT2[iPT2['pValue_adj']<0.01]
iPT3=iPT3[iPT3['pValue_adj']<0.01]
iPT4=iPT4[iPT4['pValue_adj']<0.01]
iPT1.shape

iPT1=iPT1[iPT1['LogFoldChange']>0.3]
iPT2=iPT2[iPT2['LogFoldChange']>0.3]
iPT3=iPT3[iPT3['LogFoldChange']>0.3]
iPT4=iPT4[iPT4['LogFoldChange']>0.3]
iPT1.shape

iPT1

iPT1.to_csv('iPT1.csv')
iPT2.to_csv('iPT2.csv')
iPT3.to_csv('iPT3.csv')
iPT4.to_csv('iPT4.csv')

gene_rank

gene_rank = iTAL1[['names','pi_score']]
gene_rank.sort_values(by=['pi_score'], inplace=True, ascending=False)
gene_rank = gene_rank.reset_index(drop=True)

res = gseapy.prerank(rnk=gene_rank, gene_sets=gset)

res.res2d.head(20)
res.res2d_plot = res.res2d[res.res2d["FDR q-val"] < 0.25]
res.res2d_plot

df=res.res2d.copy()
df['Term'] = df['Term'].str.replace(r' WP.*$', '', regex=True)

from gseapy import dotplot
# to save your figure, make sure that ``ofname`` is not None
ax = dotplot(df,
             column="FDR q-val",
             title='iTAL ME 1',
             cmap=plt.cm.coolwarm,
             size=6, # adjust dot size
             figsize=(4,5), cutoff=0.25, show_ring=False)
ax.tick_params(axis='y', which='both', left=False, right=False)
ax.grid(False)
# Remove the grey grid background
ax.set_facecolor('white')
ax.set_xlabel('')

from gseapy import dotplot
import matplotlib.pyplot as plt

# Create the dot plot
ax = dotplot(df,
             column="FDR q-val",
             title='iTAL ME 1',
             cmap=plt.cm.coolwarm,
             size=8, # adjust dot size
             figsize=(4,5), cutoff=0.25, show_ring=False)

# Move the y-axis labels to the right
ax.yaxis.tick_right()

# Set the label size and remove the ticks
ax.tick_params(axis='y',labelsize=25, which='both', left=False, right=False)
ax.grid(False)

# Remove the grey grid background
ax.set_facecolor('white')
ax.set_xlabel('')

# Save the plot
plt.savefig('iTAL1_ME_GSEA.png', dpi=450, bbox_inches='tight')

# Show the plot
plt.show()

from gseapy import dotplot
import matplotlib.pyplot as plt

# Create the dot plot
ax = dotplot(df,
             column="FDR q-val",
             title='iTAL ME 1',
             cmap=plt.cm.coolwarm,
             size=7, # adjust dot size
             figsize=(4,5), cutoff=0.25, show_ring=False)

# Move the y-axis labels and ticks to the right
ax.yaxis.tick_right()

# Remove the color bar
if ax.collections and ax.collections[-1].colorbar:
    ax.collections[-1].colorbar.remove()

# Remove the dot size legend
if ax.legend_:
    ax.legend_.remove()

# Adjust the tick parameters and remove the grey grid background
ax.tick_params(axis='y', labelsize=25, which='both', left=False, right=False)
ax.grid(False)
ax.set_facecolor('white')
ax.set_xlabel('')

# Save the plot
plt.savefig('iTAL1_ME_GSEA.png', dpi=450, bbox_inches='tight')

# Show the plot
plt.show()

gene_rank = iTAL2[['names','pi_score']]
gene_rank.sort_values(by=['pi_score'], inplace=True, ascending=False)
gene_rank = gene_rank.reset_index(drop=True)

res = gseapy.prerank(rnk=gene_rank, gene_sets=gset)

res.res2d.head(20)
res.res2d_plot = res.res2d[res.res2d["FDR q-val"] < 0.25]
res.res2d_plot

gene_rank = iTAL3[['names','pi_score']]
gene_rank.sort_values(by=['pi_score'], inplace=True, ascending=False)
gene_rank = gene_rank.reset_index(drop=True)

res = gseapy.prerank(rnk=gene_rank, gene_sets=gset)

res.res2d.head(20)
res.res2d_plot = res.res2d[res.res2d["FDR q-val"] < 0.25]
res.res2d_plot

gene_rank = iTAL4[['names','pi_score']]
gene_rank.sort_values(by=['pi_score'], inplace=True, ascending=False)
gene_rank = gene_rank.reset_index(drop=True)

res = gseapy.prerank(rnk=gene_rank, gene_sets=gset)

res.res2d.head(20)
res.res2d_plot = res.res2d[res.res2d["FDR q-val"] < 0.25]
res.res2d_plot



"""lets try distance and ECM score"""

adata

hk2695=adata[adata.obs['sample']=='HK2695']
hk2695

outliers = hk2695[hk2695.obs['iPT_subcluster_ME'] == 'outlier']
ipt_me1 = hk2695[hk2695.obs['iPT_subcluster_ME'] == 'iPT ME 4']

# Get the coordinates of the outlier cells
outliers_coords = outliers.obs[['CenterX_global_px', 'CenterY_global_px']].values

# Get the coordinates of the iPT ME 1 cells
ipt_me1_coords = ipt_me1.obs[['CenterX_global_px', 'CenterY_global_px']].values

# Initialize an array to store distances
distances = np.zeros((len(outliers_coords), len(ipt_me1_coords)))

# Calculate the Euclidean distance between each outlier cell and each iPT ME 1 cell
for i, outlier in enumerate(outliers_coords):
    for j, ipt_me1 in enumerate(ipt_me1_coords):
        distances[i, j] = np.sqrt((outlier[0] - ipt_me1[0])**2 + (outlier[1] - ipt_me1[1])**2)

# Convert distances to a DataFrame for better visualization (optional)
distances_df = pd.DataFrame(distances, index=outliers.obs_names)
min_distances = distances_df.min(axis=1)
min_distances_df = min_distances.to_frame(name='Min_Distance_iPT_ME_4')
print(min_distances_df)

pops=['iTAL', 'Fibroblast']
mask=adata_all.obs['annotation_post_scanvi70_broad'].isin(pops)
subset_iTAL_Fibro=adata_all[mask]
subset_iTAL_Fibro

subset_iTAL_Fibro.obs['sample'].value_counts()

print(subset_iTAL_Fibro[subset_iTAL_Fibro.obs['sample']=='HK2844'].obs['CenterY_global_px'])

all_min_distances = []

# Loop through each sample
for sample in subset_iTAL_Fibro.obs['sample'].unique():
    subset = subset_iTAL_Fibro[subset_iTAL_Fibro.obs['sample'] == sample]
    print(sample)
    outliers = subset[subset.obs['iTAL_subcluster_ME'] == 'outlier']
    ipt_me4 = subset[subset.obs['iTAL_subcluster_ME'] == 'iTAL ME 1']

    # Get the coordinates of the outlier cells
    outliers_coords = outliers.obs[['CenterX_global_px', 'CenterY_global_px']].values

    # Get the coordinates of the iPT ME 4 cells
    ipt_me4_coords = ipt_me4.obs[['CenterX_global_px', 'CenterY_global_px']].values

    # Initialize an array to store distances
    distances = np.zeros((len(outliers_coords), len(ipt_me4_coords)))

    # Calculate the Euclidean distance between each outlier cell and each iPT ME 4 cell
    for i, outlier in enumerate(outliers_coords):
        for j, ipt_me4 in enumerate(ipt_me4_coords):
            distances[i, j] = np.sqrt((outlier[0] - ipt_me4[0])**2 + (outlier[1] - ipt_me4[1])**2)

    # Convert distances to a DataFrame for better visualization (optional)
    distances_df = pd.DataFrame(distances, index=outliers.obs_names)
    min_distances = distances_df.min(axis=1)
    min_distances_df = min_distances.to_frame(name=f'Min_Distance_iTAL_ME_1')

    # Append the DataFrame to the list
    all_min_distances.append(min_distances_df)

# Concatenate all DataFrames in the list
result_df = pd.concat(all_min_distances)

# Print the final concatenated DataFrame
print(result_df)

adata.obs['Min_Distance_iTAL_ME_1'] = adata.obs_names.map(result_df['Min_Distance_iTAL_ME_1'])
subset_iTAL_Fibro.obs['Min_Distance_iTAL_ME_1'] = subset_iTAL_Fibro.obs_names.map(result_df['Min_Distance_iTAL_ME_1'])
# Verify the mapping by printing a sample of the new column
print(adata.obs[['Min_Distance_iTAL_ME_1']].head())

all_min_distances = []

# Loop through each sample
for sample in adata.obs['sample'].unique():
    subset = adata[adata.obs['sample'] == sample]
    print(sample)
    outliers = subset[subset.obs['iPT_subcluster_ME'] == 'outlier']
    ipt_me4 = subset[subset.obs['iPT_subcluster_ME'] == 'iPT ME 3']

    # Get the coordinates of the outlier cells
    outliers_coords = outliers.obs[['CenterX_global_px', 'CenterY_global_px']].values

    # Get the coordinates of the iPT ME 4 cells
    ipt_me4_coords = ipt_me4.obs[['CenterX_global_px', 'CenterY_global_px']].values

    # Initialize an array to store distances
    distances = np.zeros((len(outliers_coords), len(ipt_me4_coords)))

    # Calculate the Euclidean distance between each outlier cell and each iPT ME 4 cell
    for i, outlier in enumerate(outliers_coords):
        for j, ipt_me4 in enumerate(ipt_me4_coords):
            distances[i, j] = np.sqrt((outlier[0] - ipt_me4[0])**2 + (outlier[1] - ipt_me4[1])**2)

    # Convert distances to a DataFrame for better visualization (optional)
    distances_df = pd.DataFrame(distances, index=outliers.obs_names)
    min_distances = distances_df.min(axis=1)
    min_distances_df = min_distances.to_frame(name=f'Min_Distance_iPT_ME_3_{sample}')

    # Append the DataFrame to the list
    all_min_distances.append(min_distances_df)

# Concatenate all DataFrames in the list
result_df_3 = pd.concat(all_min_distances)

# Print the final concatenated DataFrame
print(result_df_3)

all_min_distances = []

# Loop through each sample
for sample in adata.obs['sample'].unique():
    subset = adata[adata.obs['sample'] == sample]
    print(sample)
    outliers = subset[subset.obs['iPT_subcluster_ME'] == 'outlier']
    ipt_me4 = subset[subset.obs['iPT_subcluster_ME'] == 'iPT ME 2']

    # Get the coordinates of the outlier cells
    outliers_coords = outliers.obs[['CenterX_global_px', 'CenterY_global_px']].values

    # Get the coordinates of the iPT ME 4 cells
    ipt_me4_coords = ipt_me4.obs[['CenterX_global_px', 'CenterY_global_px']].values

    # Initialize an array to store distances
    distances = np.zeros((len(outliers_coords), len(ipt_me4_coords)))

    # Calculate the Euclidean distance between each outlier cell and each iPT ME 4 cell
    for i, outlier in enumerate(outliers_coords):
        for j, ipt_me4 in enumerate(ipt_me4_coords):
            distances[i, j] = np.sqrt((outlier[0] - ipt_me4[0])**2 + (outlier[1] - ipt_me4[1])**2)

    # Convert distances to a DataFrame for better visualization (optional)
    distances_df = pd.DataFrame(distances, index=outliers.obs_names)
    min_distances = distances_df.min(axis=1)
    min_distances_df = min_distances.to_frame(name=f'Min_Distance_iPT_ME_2_{sample}')

    # Append the DataFrame to the list
    all_min_distances.append(min_distances_df)

# Concatenate all DataFrames in the list
result_df_2 = pd.concat(all_min_distances)

# Print the final concatenated DataFrame
print(result_df_2)

all_min_distances = []

# Loop through each sample
for sample in adata.obs['sample'].unique():
    subset = adata[adata.obs['sample'] == sample]
    print(sample)
    outliers = subset[subset.obs['iPT_subcluster_ME'] == 'outlier']
    ipt_me4 = subset[subset.obs['iPT_subcluster_ME'] == 'iPT ME 1']

    # Get the coordinates of the outlier cells
    outliers_coords = outliers.obs[['CenterX_global_px', 'CenterY_global_px']].values

    # Get the coordinates of the iPT ME 4 cells
    ipt_me4_coords = ipt_me4.obs[['CenterX_global_px', 'CenterY_global_px']].values

    # Initialize an array to store distances
    distances = np.zeros((len(outliers_coords), len(ipt_me4_coords)))

    # Calculate the Euclidean distance between each outlier cell and each iPT ME 4 cell
    for i, outlier in enumerate(outliers_coords):
        for j, ipt_me4 in enumerate(ipt_me4_coords):
            distances[i, j] = np.sqrt((outlier[0] - ipt_me4[0])**2 + (outlier[1] - ipt_me4[1])**2)

    # Convert distances to a DataFrame for better visualization (optional)
    distances_df = pd.DataFrame(distances, index=outliers.obs_names)
    min_distances = distances_df.min(axis=1)
    min_distances_df = min_distances.to_frame(name=f'Min_Distance_iPT_ME_1_{sample}')

    # Append the DataFrame to the list
    all_min_distances.append(min_distances_df)

# Concatenate all DataFrames in the list
result_df_1 = pd.concat(all_min_distances)

# Print the final concatenated DataFrame
print(result_df_1)

min_distances_df

adata

distances

ipt_me1

pip install --quiet pygam

fibro=subset_iTAL_Fibro[subset_iTAL_Fibro.obs['annotation_post_scanvi70_broad']=='Fibroblast']

fibro

fibro.X=fibro.layers['counts'].copy()
sc.pp.normalize_total(fibro, inplace=True)
sc.pp.log1p(fibro)

fibro.obs['Min_Distance_iTAL_ME_1']=fibro.obs['Min_Distance_iTAL_ME_1']*0.12

dense_matrix = fibro.X.toarray()

# Create a DataFrame from the dense matrix
df = pd.DataFrame(dense_matrix, index=fibro.obs.index, columns=fibro.var.index)
df

df['distance']=fibro.obs['Min_Distance_iTAL_ME_1']

df.shape

df = df.dropna(subset=['distance'])
df.shape

df

from pygam import LinearGAM, s
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Assuming df is your DataFrame and 'distance' is the column with distance values

# Iterate over all columns except 'distance'
for column in df.drop('distance', axis=1).columns:
    # Identify non-zero and zero expression rows
    non_zeros = df[df[column] > 0]
    zeros = df[df[column] == 0]

    # Number of samples to match
    n_samples = min(len(non_zeros), len(zeros))

    # Randomly sample from zeros to match the number of non-zero samples
    sampled_zeros = zeros.sample(n=n_samples, random_state=42)
    sampled_non_zeros = non_zeros.sample(n=n_samples, random_state=42)

    # Concatenate the sampled data
    balanced_df = pd.concat([sampled_zeros, sampled_non_zeros])

    # Shuffle the rows to mix zero and non-zero samples
    balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)

    # Prepare the data
    X = balanced_df[['distance']].values  # Independent variable (2D)
    y = balanced_df[column].values  # Continuous response variable (expression level)

    # Fit a Linear GAM with a spline term for the 'distance' feature
    gam = LinearGAM(s(0, n_splines=10)).fit(X, y)

    # Generate predictions for plotting
    X_pred = np.linspace(X.min(), X.max(), 100).reshape(-1, 1)
    y_pred = gam.predict(X_pred)

    # Plotting
    fig, ax = plt.subplots()
    plt.scatter(X.flatten(), y, alpha=0.5, color='gray', label='Balanced Data')
    plt.plot(X_pred.flatten(), y_pred, color='blue', label=f'Fit for {column}')

    # Customizing the plot
    ax.spines['right'].set_visible(False)
    ax.spines['top'].set_visible(False)

    plt.xlabel('Distance')
    plt.ylabel(f'Expression of {column}')
    plt.title(f'Linear GAM Fit for {column}')
    plt.legend()
    plt.show()

df['distance']=df['distance']*0.12
df

df_close=df[df['distance']<80]

df_close.shape

from pygam import LinearGAM, s
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Assuming df is your DataFrame and 'distance' is the column with distance values

# Iterate over all columns except 'distance'
for column in df_close.drop('distance', axis=1).columns:
    # Identify non-zero and zero expression rows
    non_zeros = df_close[df_close[column] > 0]
    zeros = df_close[df_close[column] == 0]

    # Number of samples to match
    n_samples = min(len(non_zeros), len(zeros))

    # Randomly sample from zeros to match the number of non-zero samples
    sampled_zeros = zeros.sample(n=n_samples, random_state=42)
    sampled_non_zeros = non_zeros.sample(n=n_samples, random_state=42)

    # Concatenate the sampled data
    balanced_df = pd.concat([sampled_zeros, sampled_non_zeros])

    # Shuffle the rows to mix zero and non-zero samples
    balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)

    # Prepare the data
    X = balanced_df[['distance']].values  # Independent variable (2D)
    y = balanced_df[column].values  # Continuous response variable (expression level)

    # Fit a Linear GAM with a spline term for the 'distance' feature
    gam = LinearGAM(s(0, n_splines=10)).fit(X, y)

    # Generate predictions for plotting
    X_pred = np.linspace(X.min(), X.max(), 100).reshape(-1, 1)
    y_pred = gam.predict(X_pred)

    # Plotting
    fig, ax = plt.subplots()
    plt.scatter(X.flatten(), y, alpha=0.5, color='gray', label='Balanced Data')
    plt.plot(X_pred.flatten(), y_pred, color='blue', label=f'Fit for {column}')

    # Customizing the plot
    ax.spines['right'].set_visible(False)
    ax.spines['top'].set_visible(False)

    plt.xlabel('Distance')
    plt.ylabel(f'Expression of {column}')
    plt.title(f'Linear GAM Fit for {column}')
    plt.legend()
    plt.show()

df_close=df[df['distance']<40]

df_close.shape

from pygam import LinearGAM, s
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Assuming df is your DataFrame and 'distance' is the column with distance values

# Iterate over all columns except 'distance'
for column in df_close.drop('distance', axis=1).columns:
    # Identify non-zero and zero expression rows
    non_zeros = df_close[df_close[column] > 0]
    zeros = df_close[df_close[column] == 0]

    # Number of samples to match
    n_samples = min(len(non_zeros), len(zeros))

    # Randomly sample from zeros to match the number of non-zero samples
    sampled_zeros = zeros.sample(n=n_samples, random_state=42)
    sampled_non_zeros = non_zeros.sample(n=n_samples, random_state=42)

    # Concatenate the sampled data
    balanced_df = pd.concat([sampled_zeros, sampled_non_zeros])

    # Shuffle the rows to mix zero and non-zero samples
    balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)

    # Prepare the data
    X = balanced_df[['distance']].values  # Independent variable (2D)
    y = balanced_df[column].values  # Continuous response variable (expression level)

    # Fit a Linear GAM with a spline term for the 'distance' feature
    gam = LinearGAM(s(0, n_splines=10)).fit(X, y)

    # Generate predictions for plotting
    X_pred = np.linspace(X.min(), X.max(), 100).reshape(-1, 1)
    y_pred = gam.predict(X_pred)

    # Plotting
    fig, ax = plt.subplots()
    plt.scatter(X.flatten(), y, alpha=0.5, color='gray', label='Balanced Data')
    plt.plot(X_pred.flatten(), y_pred, color='blue', label=f'Fit for {column}')

    # Customizing the plot
    ax.spines['right'].set_visible(False)
    ax.spines['top'].set_visible(False)

    plt.xlabel('Distance')
    plt.ylabel(f'Expression of {column}')
    plt.title(f'Linear GAM Fit for {column}')
    plt.legend()
    plt.show()

#fibro.obs['Min_Distance_iPT_ME_4']=fibro.obs['Min_Distance_iPT_ME_4']*0.12
# Create a new column in fibro.obs to categorize the Min_Distance_iPT_ME_4 values
fibro.obs['Distance_Category'] = pd.cut(fibro.obs['Min_Distance_iTAL_ME_1'],
                                        bins=[-float('inf'), 30, float('inf')],
                                        labels=['<30', '>30'])

# Verify the new column by printing a sample of it
print(fibro.obs[['Distance_Category']].head())
fibro.obs['Distance_Category']=fibro.obs['Distance_Category'].astype('category')
sc.tl.rank_genes_groups(fibro, groupby='Distance_Category', method='wilcoxon', pts = True)
sc.pl.rank_genes_groups(fibro, n_genes=25, sharey=False)

markers = ["TIMP1","CCL2", 'JUNB', 'JUN', 'COL12A1']

sc.pl.dotplot(fibro, markers, groupby='Distance_Category',cmap='Blues', log = False)

de_results = fibro.uns['rank_genes_groups']
#folder_name = '/home/bcd/cosmx/All_Runs/Imputation/Neighborhood_objects/iPT_ME'

# Iterate through each group to save DEG lists
for group in de_results['names'].dtype.names:
    gene_names = de_results['names'][group]
    gene_scores = de_results['scores'][group]
    gene_logfoldchanges = de_results['logfoldchanges'][group]
    gene_pvals = de_results['pvals'][group]
    gene_pvals_adj = de_results['pvals_adj'][group]

    # Create a DataFrame
    df = pd.DataFrame({
        'Gene': gene_names,
        'Score': gene_scores,
        'LogFoldChange': gene_logfoldchanges,
        'pValue': gene_pvals,
        'pValue_adj': gene_pvals_adj
    })
    print(df.head(30)),

fibro.obs['Distance_Category'].value_counts()